{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting utilization of medical services: achieving equalized coverage\n",
    "\n",
    "This tutorial demonstrates how to use the equalized coverage software package [1]. We also show that usual (marginal) conformal methods do not attain equal coverage across the two groups under study: non-white and white individuals. Then, we compare the performance of joint vs. groupwise model fitting and show that, in this example, the former yields shorter predictive intervals. Follow the instructions in `cqr/get_meps_data/` folder to download the data set.\n",
    "\n",
    "Please refer to `prediction_bias_example.ipynb` for more details about this data set. Also, please check the tutorials `cqr_real_data_example.ipynb` and `cqr_synthetic_data_example.ipynb` that illustrate how to construct marginal prediction intervals using CQR and conformal prediction.\n",
    "\n",
    "[1] Y. Romano, R. F. Barber, C. Sabbatti and E. J. Candès. \"With malice towards none: Assessing uncertainty via equalized coverage.\" 2019.\n",
    "\n",
    "## MEPS data set\n",
    "\n",
    "The Medical Expenditure Panel Survey (MEPS) 2016 data set, provided by the Agency for Healthcare Research and Quality, contains information on individuals and their utilization of medical services. The features used for modeling include age, marital status, race, poverty status, functional limitations, health status, health insurance type, and more. The goal is to predict the health care system utilization of each individual; a score that reflects the number of visits to a doctor’s office, hospital visits, etc. After removing observations with missing entries, there are $n=15656$ observations on $p=139$ features. We set the sensitive attribute $A$ to \\textbf{race}, with $A=0$ for non-white and $A=1$ for white individuals, resulting in $n_0=9640$ samples for the first group and $n_1=6016$ for the second.\n",
    "\n",
    "## Equalized Coverage\n",
    "\n",
    "Let $\\{(X_i, A_i, Y_i)\\}$, $i = 1, \\ldots, n$, be some training data where the vector $X_i \\in \\mathbb{R}^p$ may contain the sensitive attribute $A_i \\in \\{0,1,2,\\dots\\}$ as one of the features. Imagine we hold a test point with known $X_{n+1}$ and $A_{n+1}$ and aim to construct a prediction interval $C(X_{n+1},A_{n+1}) \\subseteq \\mathbb{R}$ which contains the unknown response $Y_{n+1} \\in \\mathbb{R}$ with probability at least $1-\\alpha$ on average within each group; here, $0 < 1-\\alpha < 1$ is a desired coverage level. Formally, we assume that the training and test samples $ \\{(X_i,A_i,Y_i)\\}_{i=1}^{n+1} $ are drawn exchangeably from some arbitrary and unknown distribution $P_{XAY}$, and we wish that our  prediction interval obeys the following property\n",
    "$$\\mathbb{P}\\{Y_{n+1} \\in C(X_{n+1},A_{n+1}) \\mid A_{n+1}=a\\} \\geq 1-\\alpha$$\n",
    "for all $a$. The probability is taken over the $n$ training samples and the test case. \n",
    "The probability statement above must hold for any distribution $P_{XAY}$, sample size $n$, and regardless of the group identifier $A_{n+1}$. (While this only ensures that coverage is at least $1-\\alpha$ for each group---and, therefore, the groups may have unequal coverage level---we show in our paper that under mild conditions the coverage can also be upper bounded to lie very close to the target level $1-\\alpha$.)\n",
    "\n",
    "## Constructing prediction intervals\n",
    "\n",
    "In this experiment, we set the miscoverage rate to 10%.\n",
    "\n",
    "First, let's randomly split the data into train (80%) and test (20%) sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data set name: meps_21\n",
      "Data set dimensions: (n=15656, p=139)\n",
      "A=0 : non-white\n",
      "A=1 : white\n",
      "Samples per group: A=0 (n=9640) ; A=1 (n=6016)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "np.warnings.filterwarnings('ignore')\n",
    "\n",
    "from cqr import helper\n",
    "from datasets import datasets\n",
    "from matplotlib import pyplot\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from nonconformist.nc import RegressorNc\n",
    "from nonconformist.nc import SignErrorErrFunc\n",
    "from nonconformist.nc import QuantileRegAsymmetricErrFunc\n",
    "\n",
    "\n",
    "# set some arbitrary seed for reproducability \n",
    "seed = 30\n",
    "\n",
    "random_state_train_test = seed\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    \n",
    "dataset_base_path = '/Users/romano/mydata/regression_data/'\n",
    "\n",
    "dataset_name = \"meps_21\"\n",
    "\n",
    "# used to determine the size of test set\n",
    "test_ratio = 0.2\n",
    "\n",
    "# load the dataset\n",
    "X, y = datasets.GetDataset(dataset_name, dataset_base_path)\n",
    "\n",
    "# display basic information\n",
    "print(\"Data set name: %s\" % (dataset_name))\n",
    "print(\"Data set dimensions: (n=%d, p=%d)\" % \n",
    "      (X.shape[0], X.shape[1]))\n",
    "\n",
    "print(\"A=0 : non-white\")\n",
    "print(\"A=1 : white\")\n",
    "print(\"Samples per group: A=0 (n=%d) ; A=1 (n=%d)\" % \n",
    "      (X[X[:,-1]==0].shape[0], X[X[:,-1]==1].shape[0]))\n",
    "\n",
    "dataset_name_group_0 = dataset_name + \"_non_white\"\n",
    "dataset_name_group_1 = dataset_name + \"_white\"\n",
    "\n",
    "# divide the dataset into test and train based on the test_ratio parameter\n",
    "x_train_orig, x_test_orig, y_train_orig, y_test_orig = train_test_split(X,\n",
    "                                                                        y,\n",
    "                                                                        test_size=test_ratio,\n",
    "                                                                        random_state=random_state_train_test)\n",
    "\n",
    "# compute input dimensions\n",
    "n_train = x_train_orig.shape[0]\n",
    "in_shape = x_train_orig.shape[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split and standardize\n",
    "\n",
    "Our method starts by randomly splitting the $n$ training points into two disjoint subsets; a proper training set $ \\left\\lbrace (X_i,A_i,Y_i): i \\in {I}_1 \\right\\rbrace  $ and a calibration set $ \\left\\lbrace (X_i,A_i,Y_i): i \\in {I}_2 \\right\\rbrace  $. We also standardize the features to have zero mean and unit variance; the means and variances are computed using the proper training examples. We transform the response variable by $Y = \\log(1 + (\\text{utilization score}))$ as the raw score is highly skewed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# divide the data into proper training set and calibration set\n",
    "idx = np.random.permutation(n_train)\n",
    "n_half = int(np.floor(n_train/2))\n",
    "idx_train, idx_cal = idx[:n_half], idx[n_half:2*n_half]\n",
    "\n",
    "# zero mean and unit variance scaling \n",
    "scalerX = StandardScaler()\n",
    "scalerX = scalerX.fit(x_train_orig[idx_train])\n",
    "x_train = scalerX.transform(x_train_orig)\n",
    "x_test = scalerX.transform(x_test_orig)\n",
    "\n",
    "# scale the response as it is highly skewed\n",
    "y_train = np.log(1.0 + y_train_orig)\n",
    "y_test = np.log(1.0 + y_test_orig)\n",
    "# reshape the response\n",
    "y_train = np.squeeze(np.asarray(y_train))\n",
    "y_test = np.squeeze(np.asarray(y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conformal Neural Network\n",
    "\n",
    "Below, we define the parameters of a classic regression algorithm, minimizing the mean-squared-error loss. The function is formulated as a network network. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# desired miscoverage level\n",
    "alpha = 0.1\n",
    "\n",
    "# desired quantile levels, used when fitting a quantile neural network model\n",
    "quantiles_net = [alpha/2, 1-alpha/2]\n",
    "\n",
    "# pytorch's optimizer object\n",
    "nn_learn_func = torch.optim.Adam\n",
    "\n",
    "# number of epochs\n",
    "epochs = 1000\n",
    "\n",
    "# learning rate\n",
    "lr = 0.0005\n",
    "\n",
    "# mini-batch size\n",
    "batch_size = 64\n",
    "\n",
    "# hidden dimension of the network\n",
    "hidden_size = 64\n",
    "\n",
    "# dropout regularization rate\n",
    "dropout = 0.1\n",
    "\n",
    "# weight decay regularization\n",
    "wd = 1e-6\n",
    "\n",
    "# seed for splitting the data in cross-validation.\n",
    "cv_test_ratio = 0.1\n",
    "\n",
    "# ratio of held-out data, used in cross-validation\n",
    "cv_random_state = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below, we define the function `condition` that extracts the group identifier from a given feature vector $X$. Here, the race attribute is the last feature in $X$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function that extracts the group identifier\n",
    "def condition(x, y=None):\n",
    "    return int(x[0][-1]>0)\n",
    "\n",
    "# extract groups\n",
    "category_map = np.array([condition((x_test[i, :], None)) for i in range(x_test.shape[0])])\n",
    "categories = np.unique(category_map)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Marginal Conformal Neural Network\n",
    "\n",
    "Below, we run marginal split conformal prediction (which is not guarenteed to attain valid group-conditional coverage). First, we fit a neural network to the proper training examples. Then, we use conformal inference to construct marginal prediction intervals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Marginal Conformal Neural Network: Group 0 : Percentage in the range (expecting 90.00): 92.546584\n",
      "Marginal Conformal Neural Network: Group 0 : Average length: 2.928768\n",
      "Marginal Conformal Neural Network: Group 1 : Percentage in the range (expecting 90.00): 88.083333\n",
      "Marginal Conformal Neural Network: Group 1 : Average length: 2.928768\n"
     ]
    }
   ],
   "source": [
    "model = helper.MSENet_RegressorAdapter(model=None,\n",
    "                                       fit_params=None,\n",
    "                                       in_shape = in_shape,\n",
    "                                       hidden_size = hidden_size,\n",
    "                                       learn_func = nn_learn_func,\n",
    "                                       epochs = epochs,\n",
    "                                       batch_size=batch_size,\n",
    "                                       dropout=dropout,\n",
    "                                       lr=lr,\n",
    "                                       wd=wd,\n",
    "                                       test_ratio=cv_test_ratio,\n",
    "                                       random_state=cv_random_state)\n",
    "        \n",
    "nc = RegressorNc(model, SignErrorErrFunc())\n",
    "\n",
    "y_lower, y_upper = helper.run_icp(nc, x_train, y_train, x_test, idx_train, idx_cal, alpha)\n",
    "\n",
    "method_name = \"Marginal Conformal Neural Network\"\n",
    "\n",
    "# compute and print average coverage and average length\n",
    "coverage_sample, length_sample = helper.compute_coverage_per_sample(y_test,\n",
    "                                                                    y_lower,\n",
    "                                                                    y_upper,\n",
    "                                                                    alpha,\n",
    "                                                                    method_name,\n",
    "                                                                    x_test,\n",
    "                                                                    condition)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As can be seen, in this example, the marginal approach undercovers group 1 (white individuals) and overcovers group 0 (non-white individuals).\n",
    "\n",
    "### Group-Conditional Conformal Neural Network ( joint )\n",
    "\n",
    "We now turn to run the group-conditional variant. This method trains one neural netowok model for both group."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conditional Conformal Neural Network (joint): Group 0 : Percentage in the range (expecting 90.00): 91.407867\n",
      "Conditional Conformal Neural Network (joint): Group 0 : Average length: 2.799576\n",
      "Conditional Conformal Neural Network (joint): Group 1 : Percentage in the range (expecting 90.00): 90.166667\n",
      "Conditional Conformal Neural Network (joint): Group 1 : Average length: 3.130488\n"
     ]
    }
   ],
   "source": [
    "model = helper.MSENet_RegressorAdapter(model=None,\n",
    "                                       fit_params=None,\n",
    "                                       in_shape = in_shape,\n",
    "                                       hidden_size = hidden_size,\n",
    "                                       learn_func = nn_learn_func,\n",
    "                                       epochs = epochs,\n",
    "                                       batch_size=batch_size,\n",
    "                                       dropout=dropout,\n",
    "                                       lr=lr,\n",
    "                                       wd=wd,\n",
    "                                       test_ratio=cv_test_ratio,\n",
    "                                       random_state=cv_random_state)\n",
    "nc = RegressorNc(model, SignErrorErrFunc())\n",
    "\n",
    "y_lower, y_upper = helper.run_icp(nc, x_train, y_train, x_test, idx_train, idx_cal, alpha, condition)\n",
    "\n",
    "method_name = \"Conditional Conformal Neural Network (joint)\"\n",
    "\n",
    "# compute and print average coverage and average length\n",
    "coverage_sample, length_sample = helper.compute_coverage_per_sample(y_test,\n",
    "                                                                    y_lower,\n",
    "                                                                    y_upper,\n",
    "                                                                    alpha,\n",
    "                                                                    method_name,\n",
    "                                                                    x_test,\n",
    "                                                                    condition)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this realization, the coverage is valid for both groups.\n",
    "\n",
    "### Group-Conditional Conformal Neural Network ( groupwise )\n",
    "\n",
    "It is intersting to compare the performace of the method above---joint training---that fits one model to the whole proper training set, to another approach---groupwise training---that fits a separate model per group (one for non-white and the second for white individuals)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conditional Conformal Neural Network (groupwise): Group 0 : Percentage in the range (expecting 90.00): 91.666667\n",
      "Conditional Conformal Neural Network (groupwise): Group 0 : Average length: 2.813798\n",
      "Conditional Conformal Neural Network (groupwise): Group 1 : Percentage in the range (expecting 90.00): 90.833333\n",
      "Conditional Conformal Neural Network (groupwise): Group 1 : Average length: 3.172685\n"
     ]
    }
   ],
   "source": [
    "estimator_list = []\n",
    "nc_list = []\n",
    "for i in range(len(categories)):\n",
    "\n",
    "    # define a NN model per group\n",
    "    estimator_list.append(helper.MSENet_RegressorAdapter(model=None,\n",
    "                                                         fit_params=None,\n",
    "                                                         in_shape = in_shape,\n",
    "                                                         hidden_size = hidden_size,\n",
    "                                                         learn_func = nn_learn_func,\n",
    "                                                         epochs = epochs,\n",
    "                                                         batch_size=batch_size,\n",
    "                                                         dropout=dropout,\n",
    "                                                         lr=lr,\n",
    "                                                         wd=wd,\n",
    "                                                         test_ratio=cv_test_ratio,\n",
    "                                                         random_state=cv_random_state))\n",
    "\n",
    "    # append the model to the list\n",
    "    nc_list.append(RegressorNc(estimator_list[i], SignErrorErrFunc()))\n",
    "\n",
    "# run groupswise learning procedure\n",
    "y_lower, y_upper = helper.run_icp_sep(nc_list, x_train, y_train, x_test, idx_train, idx_cal, alpha, condition)\n",
    "\n",
    "method_name = \"Conditional Conformal Neural Network (groupwise)\"\n",
    "\n",
    "# compute and print average coverage and average length\n",
    "coverage_sample, length_sample = helper.compute_coverage_per_sample(y_test,\n",
    "                                                                    y_lower,\n",
    "                                                                    y_upper,\n",
    "                                                                    alpha,\n",
    "                                                                    method_name,\n",
    "                                                                    x_test,\n",
    "                                                                    condition)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Marginal CQR Neural Network\n",
    "\n",
    "We now turn to demonstrate the performace of conformalized quantile regression. Starting with the marginal approach, we fit a quantile neural network model and set the low and high quantile to 5% and 95%. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Marginal CQR Neural Network: Group 0 : Percentage in the range (expecting 90.00): 91.356108\n",
      "Marginal CQR Neural Network: Group 0 : Average length: 2.490780\n",
      "Marginal CQR Neural Network: Group 1 : Percentage in the range (expecting 90.00): 90.750000\n",
      "Marginal CQR Neural Network: Group 1 : Average length: 3.172656\n"
     ]
    }
   ],
   "source": [
    "# define quantile neural network model\n",
    "quantile_estimator = helper.AllQNet_RegressorAdapter(model=None,\n",
    "                                                     fit_params=None,\n",
    "                                                     in_shape=in_shape,\n",
    "                                                     hidden_size=hidden_size,\n",
    "                                                     quantiles=quantiles_net,\n",
    "                                                     learn_func=nn_learn_func,\n",
    "                                                     epochs=epochs,\n",
    "                                                     batch_size=batch_size,\n",
    "                                                     dropout=dropout,\n",
    "                                                     lr=lr,\n",
    "                                                     wd=wd,\n",
    "                                                     test_ratio=cv_test_ratio,\n",
    "                                                     random_state=cv_random_state,\n",
    "                                                     use_rearrangement=False)\n",
    "# define the CQR object\n",
    "nc = RegressorNc(quantile_estimator, QuantileRegAsymmetricErrFunc())\n",
    "\n",
    "# run CQR procedure\n",
    "y_lower, y_upper = helper.run_icp(nc, x_train, y_train, x_test, idx_train, idx_cal, alpha)\n",
    "\n",
    "method_name = \"Marginal CQR Neural Network\"\n",
    "\n",
    "# compute and print average coverage and average length\n",
    "coverage_sample, length_sample = helper.compute_coverage_per_sample(y_test,\n",
    "                                                                    y_lower,\n",
    "                                                                    y_upper,\n",
    "                                                                    alpha,\n",
    "                                                                    method_name,\n",
    "                                                                    x_test,\n",
    "                                                                    condition)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that, for this realization, we obtain valid coverage for both groups, although the marginal method is not designed to give such a guarentee.\n",
    "\n",
    "### Group-Conditional CQR Neural Network ( joint )\n",
    "\n",
    "Moving to the group-conditional variant of CQR, we use the join training approach and fit one model for both groups.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conditional CQR Neural Network (joint): Group 0 : Percentage in the range (expecting 90.00): 91.200828\n",
      "Conditional CQR Neural Network (joint): Group 0 : Average length: 2.619198\n",
      "Conditional CQR Neural Network (joint): Group 1 : Percentage in the range (expecting 90.00): 90.083333\n",
      "Conditional CQR Neural Network (joint): Group 1 : Average length: 3.064932\n"
     ]
    }
   ],
   "source": [
    "quantile_estimator = helper.AllQNet_RegressorAdapter(model=None,\n",
    "                                                             fit_params=None,\n",
    "                                                             in_shape=in_shape,\n",
    "                                                             hidden_size=hidden_size,\n",
    "                                                             quantiles=quantiles_net,\n",
    "                                                             learn_func=nn_learn_func,\n",
    "                                                             epochs=epochs,\n",
    "                                                             batch_size=batch_size,\n",
    "                                                             dropout=dropout,\n",
    "                                                             lr=lr,\n",
    "                                                             wd=wd,\n",
    "                                                             test_ratio=cv_test_ratio,\n",
    "                                                             random_state=cv_random_state,\n",
    "                                                             use_rearrangement=False)\n",
    "                \n",
    "# define the CQR object\n",
    "nc = RegressorNc(quantile_estimator, QuantileRegAsymmetricErrFunc())\n",
    "\n",
    "# run CQR procedure\n",
    "y_lower, y_upper = helper.run_icp(nc, x_train, y_train, x_test, idx_train, idx_cal, alpha, condition)\n",
    "\n",
    "method_name = \"Conditional CQR Neural Network (joint)\"\n",
    "\n",
    "# compute and print average coverage and average length\n",
    "coverage_sample, length_sample = helper.compute_coverage_per_sample(y_test,\n",
    "                                                                    y_lower,\n",
    "                                                                    y_upper,\n",
    "                                                                    alpha,\n",
    "                                                                    method_name,\n",
    "                                                                    x_test,\n",
    "                                                                    condition)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The coverage is around the desired level. \n",
    "\n",
    "### Group-Conditional CQR Neural Network ( groupwise )\n",
    "\n",
    "Now, let's check the groupwise traning alternative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conditional CQR Neural Network (groupwise): Group 0 : Percentage in the range (expecting 90.00): 91.614907\n",
      "Conditional CQR Neural Network (groupwise): Group 0 : Average length: 2.648615\n",
      "Conditional CQR Neural Network (groupwise): Group 1 : Percentage in the range (expecting 90.00): 89.666667\n",
      "Conditional CQR Neural Network (groupwise): Group 1 : Average length: 3.086279\n"
     ]
    }
   ],
   "source": [
    "quantile_estimator_list = []\n",
    "nc_list = []\n",
    "for i in range(len(categories)):\n",
    "    quantile_estimator_list.append(helper.AllQNet_RegressorAdapter(model=None,\n",
    "                                                     fit_params=None,\n",
    "                                                     in_shape=in_shape,\n",
    "                                                     hidden_size=hidden_size,\n",
    "                                                     quantiles=quantiles_net,\n",
    "                                                     learn_func=nn_learn_func,\n",
    "                                                     epochs=epochs,\n",
    "                                                     batch_size=batch_size,\n",
    "                                                     dropout=dropout,\n",
    "                                                     lr=lr,\n",
    "                                                     wd=wd,\n",
    "                                                     test_ratio=cv_test_ratio,\n",
    "                                                     random_state=cv_random_state,\n",
    "                                                     use_rearrangement=False))      \n",
    "\n",
    "    # append CQR object\n",
    "    nc_list.append(RegressorNc(quantile_estimator_list[i], QuantileRegAsymmetricErrFunc()))\n",
    "\n",
    "# run CQR procedure\n",
    "y_lower, y_upper = helper.run_icp_sep(nc_list, x_train, y_train, x_test, idx_train, idx_cal, alpha, condition)\n",
    "\n",
    "method_name = \"Conditional CQR Neural Network (groupwise)\"\n",
    "\n",
    "# compute and print average coverage and average length\n",
    "coverage_sample, length_sample = helper.compute_coverage_per_sample(y_test,\n",
    "                                                                    y_lower,\n",
    "                                                                    y_upper,\n",
    "                                                                    alpha,\n",
    "                                                                    method_name,\n",
    "                                                                    x_test,\n",
    "                                                                    condition)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The average coverage (of this realization) is around the nominal level."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this experiment we illustrate how the coverage rates of the conditional methods are close to the target level. In our paper, we repeat this experiment for 40 different train-test splits and average the results. Then, the obtained coverage per group is exactly 90%. Moreover, the difference between joint and groupwise training becomes apparent, suggesting that for this data set the joint approach is more effective."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
