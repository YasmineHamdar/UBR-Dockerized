{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'MultiOutputMixin'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-60859d7fa682>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtree\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDecisionTreeRegressor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mngboost\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mNGBRegressor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_selection\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mngboost\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mNGBRegressor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/ngboost/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mngboost\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mNGBoost\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mapi\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mNGBClassifier\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNGBRegressor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNGBSurvival\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/ngboost/ngboost.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mngboost\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistns\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mNormal\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk_categorical\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mngboost\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmanifold\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmanifold\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mngboost\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearners\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdefault_tree_learner\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdefault_linear_learner\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcheck_random_state\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcheck_X_y\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcheck_array\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/ngboost/learners.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtree\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDecisionTreeRegressor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear_model\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mRidge\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m default_tree_learner = DecisionTreeRegressor(\n\u001b[1;32m      5\u001b[0m     \u001b[0mcriterion\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"friedman_mse\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# complete documentation.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_base\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mLinearRegression\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_bayes\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBayesianRidge\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mARDRegression\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m from ._least_angle import (Lars, LassoLars, lars_path, lars_path_gram, LarsCV,\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_base.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mjoblib\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mParallel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdelayed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m from ..base import (BaseEstimator, ClassifierMixin, RegressorMixin,\n\u001b[0m\u001b[1;32m     28\u001b[0m                     MultiOutputMixin)\n\u001b[1;32m     29\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'MultiOutputMixin'"
     ]
    }
   ],
   "source": [
    "#############################################################################\n",
    "                            #Imports\n",
    "#############################################################################\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "from scipy.stats.stats import pearsonr, spearmanr\n",
    "from scipy.spatial import distance\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "import os\n",
    "import time\n",
    "import itertools as it\n",
    "from sklearn.model_selection import KFold, RepeatedKFold\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import learning_curve\n",
    "import pickle\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from scipy.stats.stats import pearsonr, spearmanr\n",
    "import scipy.special\n",
    "import scipy.stats\n",
    "import mc_dropout\n",
    "from sklearn.model_selection import train_test_split\n",
    "import math\n",
    "import tensorflow as tf\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from ngboost import NGBRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from ngboost import NGBRegressor\n",
    "import mc_dropout\n",
    "from ngboost.distns import Exponential, Normal\n",
    "import pandas as pd\n",
    "pd.set_option('use_inf_as_na', True)\n",
    "import numpy as np\n",
    "from scipy.spatial import distance\n",
    "import rpy2.robjects as robjects\n",
    "from rpy2.robjects import pandas2ri\n",
    "import rpy2\n",
    "import rpy2.robjects.packages as rpackages\n",
    "import rpy2.robjects as ro\n",
    "pandas2ri.activate()\n",
    "import rpy2.robjects.numpy2ri\n",
    "rpy2.robjects.numpy2ri.activate()\n",
    "from rpy2.robjects.conversion import localconverter\n",
    "import warnings\n",
    "from rpy2.rinterface import RRuntimeWarning\n",
    "warnings.filterwarnings(\"ignore\", category=RRuntimeWarning)\n",
    "pandas2ri.activate()\n",
    "from matplotlib import pyplot\n",
    "from collections import Counter\n",
    "from sklearn.model_selection import StratifiedKFold, KFold\n",
    "from numpy import mean\n",
    "from numpy import std\n",
    "from pandas import read_csv\n",
    "from sklearn.preprocessing import RobustScaler, OneHotEncoder, MinMaxScaler, PowerTransformer, StandardScaler\n",
    "from scipy.stats import normaltest\n",
    "from sklearn.model_selection import ParameterSampler\n",
    "from numpy.random import randn\n",
    "from scipy.stats import shapiro\n",
    "from sklearn.model_selection import train_test_split\n",
    "from numpy import *\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "from scipy.stats.stats import pearsonr, spearmanr\n",
    "from scipy import stats\n",
    "import tensorflow as tf\n",
    "import multiprocessing as mp\n",
    "import time\n",
    "from sklearn.metrics.cluster import normalized_mutual_info_score\n",
    "import os\n",
    "import collections\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import OrderedDict\n",
    "import category_encoders as ce\n",
    "import itertools as it\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)\n",
    "import re\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "#############################################################################\n",
    "                            #Helper Methods\n",
    "#############################################################################\n",
    "\n",
    "class ProbabilisticForecastsAnalyzer:\n",
    "\n",
    "    def __init__(self, df, target_variable, split_ratio: float,\n",
    "                    output_folder,\n",
    "                    cols_drop=None, scale=True, scale_input=True, scale_output=False,\n",
    "                    output_zscore=False, output_minmax=False, output_box=False, output_log=False,\n",
    "                    input_zscore=None, input_minmax=None, input_box=None, input_log=None,\n",
    "                    testing_data=None,\n",
    "                    grid=True, random_grid=False,\n",
    "                    nb_folds_grid=None, nb_repeats_grid=None,\n",
    "                    save_errors_xlsx=True,\n",
    "                    save_validation=False):\n",
    "\n",
    "        # data frames. If df_test is None, df will be split into training and testing according to split_ratio\n",
    "        # Otherwise, df will be training, df_test will be testing\n",
    "        self.df = df\n",
    "        self.testing_data = testing_data\n",
    "\n",
    "        # drop NaN values\n",
    "        self.df = self.df.dropna()\n",
    "        if self.testing_data is not None:\n",
    "            self.testing_data = self.testing_data.dropna()\n",
    "\n",
    "        if self.testing_data is None:\n",
    "            nb_rows_test = int(round(len(self.df) * split_ratio))\n",
    "            nb_rows_train = len(self.df) - nb_rows_test\n",
    "\n",
    "            self.df_train = self.df[0: nb_rows_train]\n",
    "            self.df_test = self.df[nb_rows_train:]\n",
    "            print('first %d rows for training, last %d rows for testing' % (nb_rows_train, nb_rows_test))\n",
    "        else:\n",
    "            self.df_train = self.df\n",
    "            self.df_test = self.testing_data\n",
    "            print('param df is the training, param df_test is the testing ...')\n",
    "\n",
    "        # original testing data (the testing data before dropping columns from it)\n",
    "        # needed when attaching the 'predicted' column\n",
    "        self.df_test_orig = self.df_test\n",
    "\n",
    "        # output folder to save plots and data\n",
    "        self.output_folder = output_folder\n",
    "\n",
    "        # save the training and testing datasets before doing anything\n",
    "        self.save_train_test_before_modeling()\n",
    "\n",
    "        self.target_variable = target_variable\n",
    "\n",
    "        # list of columns to drop\n",
    "        self.cols_drop = cols_drop\n",
    "        if self.cols_drop is not None:\n",
    "            self.df_train = self.df_train.drop(self.cols_drop, axis=1)\n",
    "            self.df_test = self.df_test.drop(self.cols_drop, axis=1)\n",
    "            print('list of columns used in modeling')\n",
    "            print(list(self.df_test.columns.values))\n",
    "\n",
    "        # print('shuffling the 80% training before cv ...')\n",
    "        # self.df_train = self.df_train.sample(frac=1, random_state=42)\n",
    "\n",
    "        # output folder\n",
    "        self.output_folder = output_folder\n",
    "\n",
    "        # scaling input & output\n",
    "        self.scale = scale\n",
    "        self.scale_input = scale_input\n",
    "        self.scale_output = scale_output\n",
    "\n",
    "        # specify scaling method for output\n",
    "        self.output_zscore = output_zscore\n",
    "        self.output_minmax = output_minmax\n",
    "        self.output_box = output_box\n",
    "        self.output_log = output_log\n",
    "\n",
    "        # specify scaling method for input\n",
    "        self.input_zscore = input_zscore\n",
    "        self.input_minmax = input_minmax\n",
    "        self.input_box = input_box\n",
    "        self.input_log = input_log\n",
    "\n",
    "        # related to cross validation\n",
    "        self.grid = grid\n",
    "        self.random_grid = random_grid\n",
    "        self.nb_folds_grid = nb_folds_grid\n",
    "        self.nb_repeats_grid = nb_repeats_grid\n",
    "        self.split_ratio = split_ratio\n",
    "\n",
    "        # save error metrics to xlsx sheet\n",
    "        self.save_errors_xlsx = save_errors_xlsx\n",
    "        self.save_validation = save_validation\n",
    "        if self.save_errors_xlsx:\n",
    "            # data frame in case of probabilistic forecasts\n",
    "            self.results = pd.DataFrame(columns=[\n",
    "                'r2', 'adj_r2', 'rmse_point', 'mse', 'mae', 'mape',\n",
    "                'avg_%s' % self.target_variable,\n",
    "                'rmse_prob', 'nll', 'nb_splits',\n",
    "                'pearson', 'spearman', 'distance',\n",
    "                'winning_hyperparams', 'training_time_min', 'training_time_sec'\n",
    "            ])\n",
    "        else:\n",
    "            # self.results = None\n",
    "            self.results = None\n",
    "\n",
    "        df_without_target = self.df_train\n",
    "        df_without_target = df_without_target.drop([self.target_variable], axis=1)\n",
    "        self.feature_names = list(df_without_target.columns.values)\n",
    "        print(self.feature_names)\n",
    "\n",
    "        # numpy arrays X_train, y_train, X_test, y_test\n",
    "        # print('self in dftrain')\n",
    "        # print(self.df_train.loc[:, self.df_train.columns != self.target_variable].columns)\n",
    "        self.X_train = np.array(self.df_train.loc[:, self.df_train.columns != self.target_variable])\n",
    "        self.y_train = np.array(self.df_train.loc[:, self.target_variable])\n",
    "\n",
    "        self.X_test = np.array(self.df_test.loc[:, self.df_test.columns != self.target_variable])\n",
    "        self.y_test = np.array(self.df_test.loc[:, self.target_variable])\n",
    "\n",
    "        # get the list of indices of columns for each scaling type\n",
    "        self.idx_zscore, self.idx_minmax, self.idx_box, self.idx_log = None, None, None, None\n",
    "\n",
    "        if self.input_zscore is not None:\n",
    "            self.idx_zscore = list(range(self.input_zscore[0], self.input_zscore[1]))\n",
    "\n",
    "        if self.input_minmax is not None:\n",
    "            self.idx_minmax = list(range(self.input_minmax[0], self.input_minmax[1]))\n",
    "\n",
    "        if self.input_box is not None:\n",
    "            self.idx_box = list(range(self.input_box[0], self.input_box[1]))\n",
    "\n",
    "        if self.input_log is not None:\n",
    "            self.idx_log = list(range(self.input_log[0], self.input_log[1]))\n",
    "\n",
    "    def save_train_test_before_modeling(self):\n",
    "        ''' save the training and testing data frames before any processing happens to them '''\n",
    "        path = self.output_folder + 'train_test_before_modeling/'\n",
    "        if not os.path.exists(path):\n",
    "            os.makedirs(path)\n",
    "        self.df_train.to_csv(path + 'training.csv', index=False)\n",
    "        self.df_test.to_csv(path + 'testing.csv', index=False)\n",
    "\n",
    "    def generate_splits(self, n_splits, perc_train):\n",
    "        '''\n",
    "        generate permutations of train-test splits\n",
    "        :param n_splits: number of desired splits\n",
    "        :param perc_train: percentage of training data, the rest will be for testing\n",
    "        :return:\n",
    "        '''\n",
    "        folds = []\n",
    "        n = len(self.df_train) + len(self.df_test)\n",
    "        df_combined = pd.concat([self.df_train, self.df_test])\n",
    "        # df_combined = df_combined[:100]\n",
    "        # n = len(df_combined)\n",
    "\n",
    "        for i in range(n_splits):\n",
    "            permutation = np.random.choice(range(n), n, replace=False)\n",
    "            end_train = round(n * perc_train)\n",
    "\n",
    "            train_index = permutation[0:end_train]\n",
    "            test_index = permutation[end_train:n]\n",
    "            folds.append((train_index, test_index))\n",
    "\n",
    "        return df_combined, folds\n",
    "\n",
    "    def check_create_dir(self, dir):\n",
    "        if not os.path.exists(dir):\n",
    "            os.makedirs(dir)\n",
    "\n",
    "    def gen_xspan(self, mean, std, n=2, num=100):\n",
    "        # Generate span for x axis to plot the probability distribution\n",
    "        return np.linspace(mean - n * std, mean + n * std, num)\n",
    "\n",
    "    def normalize_ticks(self, ax):\n",
    "        cln_ticks = lambda t: float(t.replace(\"-\", \"-\"))\n",
    "        get_ticks = lambda ax: list(\n",
    "            map(cln_ticks, [item.get_text() for item in ax.get_yticklabels()])\n",
    "        )\n",
    "        labels = get_ticks(ax)\n",
    "        labels = [f\"{l / np.max(labels):.3f}\" for l in labels]\n",
    "        ax.set_yticklabels(labels)\n",
    "        return ax\n",
    "\n",
    "    def heavyside(self, thresholds, actual):\n",
    "        # Given a deterministic observation, make a CDF out of it\n",
    "        result = [1 if t >= actual else 0 for t in thresholds]\n",
    "        return result\n",
    "\n",
    "    def is_cdf_valid(self, case):\n",
    "        if case[0] < 0 or case[0] > 1:\n",
    "            return False\n",
    "        for i in range(1, len(case)):\n",
    "            if case[i] > 1 or case[i] < case[i - 1]:\n",
    "                return False\n",
    "        return True\n",
    "\n",
    "    def calc_crps(self, thresholds, predictions, actuals):\n",
    "        nthresh = len(thresholds)  # 70 in example\n",
    "        ncases = len(predictions)\n",
    "        crps = 0\n",
    "        for case, actual in zip(predictions, actuals):\n",
    "            if (len(case) == nthresh) and self.is_cdf_valid(case):\n",
    "                obscdf = self.heavyside(thresholds, actual)\n",
    "                for fprob, oprob in zip(case, obscdf):\n",
    "                    crps = crps + (fprob - oprob) * (fprob - oprob)\n",
    "            else:\n",
    "                crps = crps + nthresh  # treat delta at each threshold as 1\n",
    "        crps = crps / float(ncases * nthresh)\n",
    "\n",
    "    def boxplot_random_predictions(self, X_test, trained_model, output_folder, fig_name):\n",
    "        '''\n",
    "        :param X_test: testing data to use for predcition\n",
    "        :param trained_model: trained probabilistic model\n",
    "        :param output_folder: directory to store the resultant plot\n",
    "        :param fig_name: the name the resultant plot will be saved as\n",
    "        :return:\n",
    "        '''\n",
    "        num_plots = 10\n",
    "        cands = np.sort(np.random.choice(np.arange(0, X_test.shape[0]), num_plots))\n",
    "        dist_values = []\n",
    "        colors = [\"olive\", \"navy\", \"tomato\", \"turquoise\", \"red\"]\n",
    "        for cand, c in zip(cands, colors):\n",
    "            y_dists = trained_model.pred_dist(np.array(X_test[cand]).reshape(1, -1))\n",
    "            x_span = self.gen_xspan(y_dists.scale, y_dists.scale, num=100)\n",
    "            dist_values.append(y_dists.pdf(x_span))\n",
    "        x = np.array(dist_values).reshape(50, 10)\n",
    "        plt.boxplot(x, labels=cands)\n",
    "\n",
    "        self.check_create_dir(output_folder)\n",
    "\n",
    "        plt.savefig(os.path.join(output_folder, fig_name))\n",
    "        plt.close()\n",
    "\n",
    "    def cdf_random_predictions(self, trained_model, X_test, cands, output_folder, fig_name):\n",
    "        '''\n",
    "        :param trained_model: trained probabilistic model\n",
    "        :param X_test: testing input data to predict upon\n",
    "        :param cands: candidates (indices) to predict\n",
    "        :param output_folder: directory to store the resultant plot\n",
    "        :param fig_name: the name the resultant plot will be saved as\n",
    "        :return:\n",
    "        '''\n",
    "        colors = [\"olive\", \"navy\", \"tomato\", \"turquoise\", \"red\"]\n",
    "        fig, ax = plt.subplots(1, 1)\n",
    "        print(cands)\n",
    "        for cand, c in zip(cands, colors):\n",
    "            y_dists = trained_model.pred_dist(np.array(X_test[cand]).reshape(1, -1))\n",
    "            x_span = self.gen_xspan(y_dists.scale, y_dists.scale, num=100)\n",
    "            dist_values = y_dists.cdf(x_span)\n",
    "            ax.plot(x_span, dist_values, color=c, label=f\"{cand}\")\n",
    "            ax.legend(loc=\"upper right\")\n",
    "            del y_dists, x_span, dist_values\n",
    "        fig.canvas.draw()\n",
    "        ax = self.normalize_ticks(ax)\n",
    "        fig.suptitle(\"Cumulative Distribution plots for 5 Random Predictions\")\n",
    "\n",
    "        self.check_create_dir(output_folder)\n",
    "\n",
    "        plt.savefig(os.path.join(output_folder, fig_name))\n",
    "        plt.close()\n",
    "\n",
    "    def pdf_random_predictions(self, trained_model, X_test, cands, output_folder,\n",
    "                               fig_name):\n",
    "        '''\n",
    "        :param trained_model: trained probabilistic model\n",
    "        :param X_test: testing input data\n",
    "        :param cands: candidates (indices) to show\n",
    "        :param output_folder: directory to store the resultant plot\n",
    "        :param fig_name: the name the resultant plot will be saved as\n",
    "        '''\n",
    "        colors = [\"olive\", \"navy\", \"tomato\", \"turquoise\", \"red\"]\n",
    "        fig, ax = plt.subplots(1, 1)\n",
    "        for cand, c in zip(cands, colors):\n",
    "            # if it is ngboost\n",
    "            y_dists = trained_model.pred_dist(np.array(X_test[cand]).reshape(1, -1))\n",
    "            x_span = self.gen_xspan(y_dists.scale, y_dists.scale, num=100)\n",
    "            dist_values = y_dists.pdf(x_span)\n",
    "            ax.plot(x_span, dist_values, color=c, label=f\"{cand}\")\n",
    "            ax.legend(loc=\"upper right\")\n",
    "            # del y_dists, x_span, dist_values\n",
    "\n",
    "        fig.canvas.draw()\n",
    "        ax = self.normalize_ticks(ax)\n",
    "        fig.suptitle(\"Probability Density Functions for {} Random Predictions\".format(len(cands)))\n",
    "\n",
    "        self.check_create_dir(output_folder)\n",
    "\n",
    "        plt.savefig(os.path.join(output_folder, fig_name))\n",
    "        plt.close()\n",
    "\n",
    "    def plot_errors_ngboost(self, Yt_hat, X, y, output_folder, fig_name):\n",
    "        means = Yt_hat.mean() # save mean for each predicted point\n",
    "        std = Yt_hat.std() # save standard dev for each predicted\n",
    "        plt.figure(figsize=(16, 16))  # make the size of the plot a bit bigger\n",
    "        plt.errorbar(x=list(range(X.shape[0])), y=means, yerr=std, fmt='x', label='errors')\n",
    "        plt.scatter(list(range(X.shape[0])), y, c='r',\n",
    "                    label='real')  # add the real values on top with red color\n",
    "        plt.legend(loc='best')\n",
    "\n",
    "        self.check_create_dir(output_folder)\n",
    "\n",
    "        plt.savefig(os.path.join(output_folder, fig_name))\n",
    "        plt.close()\n",
    "\n",
    "    def plot_errors_mc_dropout(self, Yt_hat, X, y, T, output_folder, fig_name):\n",
    "        pred = np.zeros((X.shape[0], T))  # empty array to be populated\n",
    "        means = []  # save mean for each predicted point\n",
    "        std = []  # save standard dev for each predicted\n",
    "        for j in range(X.shape[0]):\n",
    "            for i in range(T):\n",
    "                pred[j][i] = Yt_hat[i][j]\n",
    "            means.append(pred[j].mean())  # get the mean for each prediction\n",
    "            std.append(pred[j].std())  # get the standard deviation\n",
    "\n",
    "        plt.figure(figsize=(16, 16))  # make the size of the plot a bit bigger\n",
    "        plt.errorbar(x=list(range(X.shape[0])), y=means, yerr=std, fmt='x', label='errors')\n",
    "        plt.scatter(list(range(X.shape[0])), y, c='r',\n",
    "                    label='real')  # add the real values on top with red color\n",
    "        plt.legend(loc='best')\n",
    "\n",
    "        self.check_create_dir(output_folder)\n",
    "\n",
    "        plt.savefig(os.path.join(output_folder, fig_name))\n",
    "        plt.close()\n",
    "\n",
    "    \n",
    "\n",
    "    def cross_validation_grid_ngboost(self, X_train_orig, y_train_orig, rtrain, possible_hyperparams, sort_by='rmse'):\n",
    "        \n",
    "        if 'LE_bowen_corr_mm' in X_train_orig.columns:\n",
    "            X_train_orig = X_train_orig.drop(['LE_bowen_corr_mm'], axis = 1)\n",
    "\n",
    "        print('X_train cols final before training')\n",
    "        train_cols = X_train_orig.columns\n",
    "        print(X_train_orig.columns)\n",
    "        print(X_train_orig.shape)\n",
    "        #X_train_orig.drop(['LE_bowen_corr_mm'], axis=1)\n",
    "        output_folder = os.path.join(self.output_folder, 'ng_boost/')\n",
    "\n",
    "        def get_param_grid(dicts):\n",
    "            return [dict(zip(dicts.keys(), p)) for p in it.product(*dicts.values())]\n",
    "\n",
    "        df_train = X_train_orig\n",
    "        df_train[y_test_name] = y_train_orig\n",
    "        print('Created df_train, its columns are:')\n",
    "        print(df_train.columns)\n",
    "        \n",
    "\n",
    "        # training and testing data if we have single target variable\n",
    "        X_train, X_test, y_train, y_test = self.X_train, self.X_test, self.y_train, self.y_test\n",
    "\n",
    "        tempModels = []\n",
    "        tempModels_2 = []\n",
    "\n",
    "        # specify the type of cv (kfold vs. repeated kfold)\n",
    "        if self.nb_repeats_grid is None:\n",
    "            print('running %d-fold cross validation' % self.nb_folds_grid)\n",
    "            folds = get_fold_indices(X_train,y_train,self.nb_folds_grid,rtrain)\n",
    "            #kf = KFold(n_splits=self.nb_folds_grid, random_state=2652124)\n",
    "        else:\n",
    "            print('running %d-fold-%d-repeats cross validation' % (self.nb_folds_grid, self.nb_repeats_grid))\n",
    "            kf = RepeatedKFold(n_splits=self.nb_folds_grid, n_repeats=self.nb_repeats_grid, random_state=2652124)\n",
    "\n",
    "        t1 = time.time()\n",
    "\n",
    "        parameters = possible_hyperparams\n",
    "        print('defined folds and X, Y' , file=open(output_path +\"logs.txt\", \"a\"))\n",
    "        # hyper parameters loop\n",
    "        print('Total nb of hyper params: %d' % len(get_param_grid(parameters)))\n",
    "        \n",
    "        for parameter in get_param_grid(parameters):\n",
    "            print('Started Parameter Looping' , file=open(output_path +\"logs.txt\", \"a\"))\n",
    "            model = NGBRegressor(**parameter)\n",
    "\n",
    "            # additional lists specific to ng boost\n",
    "            # references: https://github.com/stanfordmlgroup/ngboost/blob/master/examples/experiments/regression_exp.py\n",
    "            ngb_rmse, ngb_nll = [], []\n",
    "            accuracy_scores, aic_scores, bic_scores, nmi_scores, mape_scores,distance_scores,spearman_scores,pearson_scores,mae_scores,mse_scores,rmse_scores,adj_r2_scores,r2_scores,f1_scores,f2_scores,f5_scores,prec_scores,recall_scores = [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], []\n",
    "\n",
    "            # for train_index, test_index in kf.split(X_train):\n",
    "            for train_index, test_index in folds:\n",
    "                print('Looping over folds' , file=open(output_path +\"logs.txt\", \"a\"))\n",
    "                X_train_inner, X_val = X_train[train_index], X_train[test_index]\n",
    "                y_train_inner, y_val = y_train[train_index], y_train[test_index]\n",
    "\n",
    "                if self.scale:\n",
    "                    X_train_inner, X_val, y_train_inner, y_val, scaler_out_final = self.scale_cols(X_train_inner, X_val,\n",
    "                                                                                                   y_train_inner, y_val)\n",
    "                print(X_train_inner.shape)\n",
    "                print('started fit' , file=open(output_path +\"logs.txt\", \"a\"))\n",
    "                model.fit(X_train_inner, y_train_inner)\n",
    "                print('ended fit' , file=open(output_path +\"logs.txt\", \"a\"))\n",
    "                print(X_val.shape)\n",
    "                y_pred = model.predict(X_val)\n",
    "                y_forecast = model.pred_dist(X_val)\n",
    "\n",
    "                # AFTER PREDICTION, reverse the scaled output\n",
    "                # (if self.scale_output is on). The idea is to reverse scaling\n",
    "                # JUST BEFORE printing out the error metrics\n",
    "                if self.scale_output:\n",
    "                    if self.output_zscore or self.output_minmax:\n",
    "                        y_pred = scaler_out_final.inverse_transform(y_pred.reshape(-1, 1)).reshape(-1)\n",
    "                        y_val = scaler_out_final.inverse_transform(y_val.reshape(-1, 1)).reshape(-1)\n",
    "                    elif self.output_log:\n",
    "                        y_pred = np.exp(y_pred.reshape(-1, 1))\n",
    "                        y_val = np.exp(y_val)\n",
    "                    else:\n",
    "                        y_pred = self.inverse_boxcox(y_pred, self.y_train_lambda_)\n",
    "                        y_val = self.inverse_boxcox(y_val, self.y_test_lambda_)\n",
    "\n",
    "                #error metric calc\n",
    "                df_test = pd.DataFrame(data=X_val,columns= train_cols)\n",
    "                df_test[y_test_name] = y_val\n",
    "                df_test[y_test_pred_name] = y_pred\n",
    "\n",
    "                accuracy, aic, bic, nmi, mape,distance,spearman,pearson,mae,mse,rmse,adj_r2,r2,f1,f2,f5,prec,recall = evaluate(df_test, actual=y_test_name, predicted=y_test_pred_name,thresh=0.8, rel_method='extremes', extr_type='high',coef=1.5, relevance_pts=None)\n",
    "          \n",
    "                # add the point prediction error metrics\n",
    "                r2_scores.append(r2)\n",
    "                adj_r2_scores.append(adj_r2)\n",
    "                rmse_scores.append(rmse)\n",
    "                mse_scores.append(mse)\n",
    "                mae_scores.append(mae)\n",
    "                mape_scores.append(mape)\n",
    "                accuracy_scores.append(accuracy)\n",
    "                aic_scores.append(aic)\n",
    "                bic_scores.append(bic)\n",
    "                nmi_scores.append(nmi)\n",
    "                f1_scores.append(f1)\n",
    "                f2_scores.append(f2)\n",
    "                f5_scores.append(f1)\n",
    "                prec_scores.append(prec)\n",
    "                recall_scores.append(recall)\n",
    "                pearson_scores.append(pearson)\n",
    "                spearman_scores.append(spearman)\n",
    "                distance_scores.append(distance)\n",
    "                print('appended all scores' , file=open(output_path +\"logs.txt\", \"a\"))\n",
    "                # add the ngb probabilistic predictions\n",
    "                ngb_rmse.append(np.sqrt(mean_squared_error(y_forecast.mean(), y_val)))\n",
    "                ngb_nll.append(-y_forecast.logpdf(y_val.flatten()).mean())\n",
    "                print('appended nll scores' , file=open(output_path +\"logs.txt\", \"a\"))\n",
    "                \n",
    "            print('before temp models appending' , file=open(output_path +\"logs.txt\", \"a\"))\n",
    "            print(np.mean(r2_scores) , file=open(output_path +\"logs.txt\", \"a\"))\n",
    "            \n",
    "            tempModels.append(\n",
    "                [parameter, np.mean(r2_scores), np.mean(adj_r2_scores), np.mean(rmse_scores), np.mean(mse_scores),\n",
    "                 np.mean(mae_scores), np.mean(mape_scores), np.mean(accuracy_scores), np.mean(aic_scores), np.mean(bic_scores), np.mean(nmi_scores), np.mean(f1_scores), np.mean(f2_scores),\n",
    "                 np.mean(f5_scores), np.mean(prec_scores), np.mean(recall_scores), np.mean(pearson_scores), np.mean(spearman_scores), np.mean(distance_scores),\n",
    "                 np.mean(ngb_rmse), np.mean(ngb_nll)])\n",
    "            \n",
    "            tempModels_2.append(\n",
    "                [parameter, np.std(r2_scores), np.std(adj_r2_scores), np.std(rmse_scores), np.std(mse_scores),\n",
    "                 np.std(mae_scores), np.std(mape_scores), np.std(accuracy_scores), np.std(aic_scores), np.std(bic_scores), np.std(nmi_scores), np.std(f1_scores), np.std(f2_scores),\n",
    "                 np.std(f5_scores), np.std(prec_scores), np.std(recall_scores), np.std(pearson_scores), np.std(spearman_scores), np.std(distance_scores),\n",
    "                 np.std(ngb_rmse), np.std(ngb_nll)])\n",
    "                 \n",
    "            print('appended big temp models score' , file=open(output_path +\"logs.txt\", \"a\"))\n",
    "\n",
    "        print('started sorting' , file=open(output_path +\"logs.txt\", \"a\"))\n",
    "        # the best by RMSE\n",
    "        if sort_by == 'rmse':\n",
    "            tempModels = sorted(tempModels, key=lambda k: k[3])\n",
    "            winning_hyperparameters = tempModels[0][0]\n",
    "        else:\n",
    "            # sort by the best NGB RMSE\n",
    "            tempModels = sorted(tempModels, key=lambda k: k[19])\n",
    "            winning_hyperparameters = tempModels[0][0]\n",
    "\n",
    "        print('finished sorting' , file=open(output_path +\"logs.txt\", \"a\"))\n",
    "        print('winning hyper parameters: ', str(winning_hyperparameters))\n",
    "\n",
    "        scores_dict = OrderedDict()\n",
    "        scores_dict[\"mean target\"] = average_target_variable  \n",
    "        scores_dict[\"F1\"] = tempModels[0][11]\n",
    "        scores_dict[\"F2\"] =  tempModels[0][12]\n",
    "        scores_dict[\"F05\"] = tempModels[0][13]\n",
    "        scores_dict[\"Precision\"] = tempModels[0][14]\n",
    "        scores_dict[\"Recall\"] = tempModels[0][15]\n",
    "        scores_dict[\"R2\"] = tempModels[0][1]\n",
    "        scores_dict[\"Adjusted R2\"] = tempModels[0][2]\n",
    "        scores_dict[\"RMSE\"] = tempModels[0][3]\n",
    "        scores_dict[\"MSE\"] = tempModels[0][4]\n",
    "        scores_dict[\"MAE\"] = tempModels[0][5]\n",
    "        scores_dict[\"MAPE\"] = tempModels[0][6]\n",
    "        scores_dict[\"Accuracy\"] = tempModels[0][7]\n",
    "        scores_dict[\"Pearson C.C.\"] = tempModels[0][16]\n",
    "        scores_dict[\"Spearman C.C.\"] = tempModels[0][17]\n",
    "        scores_dict[\"Spatial Distance\"] =  tempModels[0][18]\n",
    "        scores_dict[\"NMI\"] = tempModels[0][10]\n",
    "        scores_dict[\"AIC\"] = tempModels[0][8]\n",
    "        scores_dict[\"BIC\"] = tempModels[0][9]\n",
    "        scores_dict[\"Probabilistic RMSE\"] = tempModels[0][19]\n",
    "        scores_dict[\"Probabilistic NLL\"] = tempModels[0][20]\n",
    "        scores_dict[\"Data Size train\"] = '-'\n",
    "        scores_dict[\"Data Size test\"] = '-'\n",
    "        scores_dict[\"Training Time (seconds)\"] = '-'\n",
    "        scores_dict[\"Testing Time (seconds)\"] = '-'\n",
    "        export_scores(scores_dict, \"NGBoost Validation Scores\", \"ng_boost\")\n",
    "        \n",
    "        \n",
    "        scores_dict[\"mean target\"] = average_target_variable  \n",
    "        scores_dict[\"F1\"] = tempModels_2[0][11]\n",
    "        scores_dict[\"F2\"] =  tempModels_2[0][12]\n",
    "        scores_dict[\"F05\"] = tempModels_2[0][13]\n",
    "        scores_dict[\"Precision\"] = tempModels_2[0][14]\n",
    "        scores_dict[\"Recall\"] = tempModels_2[0][15]\n",
    "        scores_dict[\"R2\"] = tempModels_2[0][1]\n",
    "        scores_dict[\"Adjusted R2\"] = tempModels_2[0][2]\n",
    "        scores_dict[\"RMSE\"] = tempModels_2[0][3]\n",
    "        scores_dict[\"MSE\"] = tempModels_2[0][4]\n",
    "        scores_dict[\"MAE\"] = tempModels_2[0][5]\n",
    "        scores_dict[\"MAPE\"] = tempModels_2[0][6]\n",
    "        scores_dict[\"Accuracy\"] = tempModels_2[0][7]\n",
    "        scores_dict[\"Pearson C.C.\"] = tempModels_2[0][16]\n",
    "        scores_dict[\"Spearman C.C.\"] = tempModels_2[0][17]\n",
    "        scores_dict[\"Spatial Distance\"] =  tempModels_2[0][18]\n",
    "        scores_dict[\"NMI\"] = tempModels_2[0][10]\n",
    "        scores_dict[\"AIC\"] = tempModels_2[0][8]\n",
    "        scores_dict[\"BIC\"] = tempModels_2[0][9]\n",
    "        scores_dict[\"Probabilistic RMSE\"] = tempModels_2[0][19]\n",
    "        scores_dict[\"Probabilistic NLL\"] = tempModels_2[0][20]\n",
    "        scores_dict[\"Data Size train\"] = '-'\n",
    "        scores_dict[\"Data Size test\"] = '-'\n",
    "        scores_dict[\"Training Time (seconds)\"] = '-'\n",
    "        scores_dict[\"Testing Time (seconds)\"] = '-'\n",
    "        export_scores(scores_dict, \"NGBoost Validation Scores std\", \"ng_boost\")\n",
    "      \n",
    "        write_to_txt('winning-hyperparams.txt', str(winning_hyperparameters))\n",
    "\n",
    "        # if self.save_errors_xlsx:\n",
    "        #     if self.save_validation:\n",
    "        #         self.results.loc['ngboost_val'] = pd.Series({'r2': tempModels[0][1],\n",
    "        #                                                              'adj-r2': tempModels[0][2],\n",
    "        #                                                              'rmse_point': tempModels[0][3],\n",
    "        #                                                              'mse': tempModels[0][4],\n",
    "        #                                                              'mae': tempModels[0][5],\n",
    "        #                                                              'mape': tempModels[0][6],\n",
    "        #                                                              'rmse_prob': tempModels[0][7],\n",
    "        #                                                              'ngb_nll': tempModels[0][8]})\n",
    "\n",
    "        # re-initialize model with the best set of hyper parameters\n",
    "        model = NGBRegressor(**winning_hyperparameters)\n",
    "\n",
    "        if smogn:\n",
    "            #redefine my train data\n",
    "        \n",
    "            if 'LE_bowen_corr_mm' in X_train_orig.columns:\n",
    "                X_train_orig = X_train_orig.drop(['LE_bowen_corr_mm'], axis = 1)\n",
    "            df_train = X_train_orig\n",
    "            df_train[target_variable] = y_train\n",
    "            # df_train.to_csv(output_path + \"df_train_before_smogn_1.csv\")\n",
    "            #confirm if rel method matches rel points\n",
    "            if rel_method == 'range' and relevance_pts is None:\n",
    "                raise ValueError('You have set rel_method = range. You must provide relevance_pts as a matrix. Currently, it is None')\n",
    "            #get phi loss params\n",
    "            y_train_a = np.array(df_train[target_variable])\n",
    "            phi_params, loss_params, relevance_values = get_phi_loss_params(y_train_a, rel_method, extr_type, coef,relevance_pts)\n",
    "            # df_train.to_csv(output_path + \"df_train_before_smogn_after_phi.csv\")\n",
    "            #apply smogn on the df_train data \n",
    "            print(df_train.columns)\n",
    "            print(target_variable)\n",
    "            \n",
    "            X_train,y_train = apply_smogn(df_train, smogn, target_variable, phi_params, thr_rel, Cperc, k, repl, dist, p, pert, plotdensity=False)\n",
    "    \n",
    "                \n",
    "            #define X_train as df \n",
    "            X_train = pd.DataFrame(X_train, columns= cols)\n",
    "        \n",
    "            print(\"cols in X train after smogn are :\" )\n",
    "            print(X_train.columns)\n",
    "            \n",
    "            #define a smogned df_train\n",
    "            df_train_smogned = X_train\n",
    "            df_train_smogned[output_column] = y_train\n",
    "            self.check_create_dir(output_path + 'ng_boost/')\n",
    "            df_train_smogned.to_csv(output_path + 'ng_boost/' + 'df_train_smogned.csv')\n",
    "        \n",
    "        if smogn:\n",
    "          if one_hot_encoded:\n",
    "              count_abnormal(X_train)\n",
    "              print(\"fixing one hot encoded cols\")\n",
    "              X_train = round_oversampled_one_hot_encoded(X_train) \n",
    "          \n",
    "          else:\n",
    "              print(\"there are no onehot encoded cols to be accounted for\")\n",
    "              \n",
    "          if 'LE_bowen_corr_mm' in X_train.columns:\n",
    "              X_train = X_train.drop(['LE_bowen_corr_mm'], axis =1 )\n",
    "\n",
    "        if self.scale:\n",
    "            X_train, X_test, y_train, y_test, scaler_out_final = self.scale_cols(X_train, X_test, y_train, y_test)\n",
    "            \n",
    "  \n",
    "            \n",
    "        print('start with test scores' , file=open(output_path +\"logs.txt\", \"a\"))\n",
    "        # train the model\n",
    "        print(\"cols for X_train final training after cross validation\")\n",
    "        print(train_cols)\n",
    "        print(X_train.shape)\n",
    "        time1 = time.time()\n",
    "        print('fit test model' , file=open(output_path +\"logs.txt\", \"a\"))\n",
    "        model.fit(X_train, y_train)\n",
    "        time2 = time.time()\n",
    "        time_tr = float(time2 - time1)\n",
    "        # save the model\n",
    "        models_folder = self.output_folder + 'trained_models/'\n",
    "        if not os.path.exists(models_folder):\n",
    "            os.makedirs(models_folder)\n",
    "        # pkl_filename = \"%ngboost.pkl\"\n",
    "        # with open(models_folder + pkl_filename, 'wb') as file:\n",
    "        #     pickle.dump(model, file)\n",
    "        print('saved model to {} as ngboost.pkl'.format(models_folder))\n",
    "\n",
    "        # the point and probabilistic predictions\n",
    "        time1 = time.time()\n",
    "        print('columns used for X_test final')\n",
    "        y_pred = model.predict(X_test)\n",
    "        time2 = time.time()\n",
    "        time_test = float(time2 - time1)\n",
    "        forecast = model.pred_dist(X_test)\n",
    "        print('predict test model' , file=open(output_path +\"logs.txt\", \"a\"))\n",
    "        # the NGB NLL & RMSE\n",
    "        ngb_rmse = np.sqrt(mean_squared_error(forecast.mean(), y_test))\n",
    "        ngb_nll = -forecast.logpdf(y_test.flatten()).mean()\n",
    "\n",
    "        if self.scale_output:\n",
    "            if self.output_log:\n",
    "                y_pred_reverse = np.exp(y_pred.reshape(-1, 1))\n",
    "                y_test = np.exp(y_test)\n",
    "            elif self.output_box:\n",
    "                y_pred_reverse = self.inverse_boxcox(y_pred, self.y_train_lambda_)\n",
    "                y_test = self.inverse_boxcox(y_test, self.y_test_lambda_)\n",
    "            else:\n",
    "                y_pred_reverse = scaler_out_final.inverse_transform(y_pred.reshape(-1, 1)).reshape(-1)\n",
    "                y_test = scaler_out_final.inverse_transform(y_test.reshape(-1, 1)).reshape(-1)\n",
    "\n",
    "    \n",
    "\n",
    "        if self.scale_output:\n",
    "            output_dataset = self.create_output_dataset(y_pred_reverse, 'ngboost',\n",
    "                                                        output_folder + 'output_vector_datasets/')\n",
    "            print('created output dataset')\n",
    "        else:\n",
    "            output_dataset = self.create_output_dataset(y_pred, 'ngboost',\n",
    "                                                        output_folder + 'output_vector_datasets/')\n",
    "            print('created output dataset')\n",
    "\n",
    "        print('creating plots ...')\n",
    "\n",
    "        self.plot_actual_vs_predicted(y_test, y_pred,\n",
    "                                      output_folder + 'train_test_forecasts_lineplot/',\n",
    "                                      'ngboost')\n",
    "\n",
    "        self.plot_actual_vs_predicted_scatter_bisector(y_test, y_pred,\n",
    "                                      output_folder + 'train_test_forecasts_scatterplot_bisector/',\n",
    "                                      'ngboost')\n",
    "\n",
    "        self.plot_errors_ngboost(Yt_hat=forecast, X=X_test, y=y_pred,\n",
    "                                 output_folder=os.path.join(output_folder, 'probabilistic_forecasts/'),\n",
    "                                 fig_name='errors')\n",
    "\n",
    "        print('This is X_test shape')\n",
    "        print(X_test.shape)\n",
    "        df_test = pd.DataFrame(data=X_test,columns= train_cols)\n",
    "        X_test_df = df_test\n",
    "        df_test[y_test_name] = y_test\n",
    "        df_test[y_test_pred_name] = y_pred\n",
    "        print(\"this is df test after creation columns\")\n",
    "        print(df_test.columns)\n",
    "        df_test.to_csv(output_path + 'ng_boost/' + 'df_test_pred.csv')\n",
    "        \n",
    "\n",
    "        print('calculate errors' , file=open(output_path +\"logs.txt\", \"a\"))\n",
    "        accuracy, aic, bic, nmi, mape,distance,spearman,pearson,mae,mse,rmse,adj_r2,r2,f1,f2,f5,prec,recall = evaluate(df_test, actual=y_test_name, predicted=y_test_pred_name,\n",
    "                    thresh=0.8, rel_method='extremes', extr_type='high',\n",
    "                    coef=1.5, relevance_pts=None)\n",
    "\n",
    "\n",
    "        print('create dict' , file=open(output_path +\"logs.txt\", \"a\"))\n",
    "        scores_dict = OrderedDict()\n",
    "        scores_dict[\"mean target\"] = average_target_variable  \n",
    "        scores_dict[\"F1\"] = f1\n",
    "        scores_dict[\"F2\"] =  f2\n",
    "        scores_dict[\"F05\"] = f5\n",
    "        scores_dict[\"Precision\"] =prec\n",
    "        scores_dict[\"Recall\"] = recall\n",
    "        scores_dict[\"R2\"] = r2\n",
    "        scores_dict[\"Adjusted R2\"] = adj_r2\n",
    "        scores_dict[\"RMSE\"] = rmse\n",
    "        scores_dict[\"MSE\"] = mse\n",
    "        scores_dict[\"MAE\"] = mae\n",
    "        scores_dict[\"MAPE\"] = mape\n",
    "        scores_dict[\"Accuracy\"] = accuracy\n",
    "        scores_dict[\"Pearson C.C.\"] = pearson\n",
    "        scores_dict[\"Spearman C.C.\"] = spearman\n",
    "        scores_dict[\"Spatial Distance\"] =  distance\n",
    "        scores_dict[\"NMI\"] = nmi\n",
    "        scores_dict[\"AIC\"] = aic\n",
    "        scores_dict[\"BIC\"] = bic\n",
    "        scores_dict[\"Probabilistic RMSE\"] = ngb_rmse\n",
    "        scores_dict[\"Probabilistic NLL\"] = ngb_nll\n",
    "        scores_dict[\"Data Size train\"] = df_train.shape\n",
    "        scores_dict[\"Data Size test\"] = df_test.shape\n",
    "        scores_dict[\"Training Time (seconds)\"] = time_tr\n",
    "        scores_dict[\"Testing Time (seconds)\"] = time_test\n",
    "        export_scores(scores_dict, \" NGBoost Test Scores\", \"ng_boost\")\n",
    "\n",
    "\n",
    "        #plotting target variable is smogn is used\n",
    "        if smogn:\n",
    "            print(df_train.columns, df_train_smogned.columns)\n",
    "            print(df_train.columns)\n",
    "            print(\"ERRORR HERE\")\n",
    "            plot_target_variable(df_train, df_train_smogned, output_column, output_path, 'ng_boost/target_variable')\n",
    "            print(X_test_df.shape)\n",
    "            print(X_test_df.columns)\n",
    "            print(df_test[y_test_name].shape)\n",
    "            X_test_df = X_test_df.drop(['LE_bowen_corr_mm', 'LE_bowen_corr_pred'], axis = 1)\n",
    "            \n",
    "        pdf_plot(X_test_df, df_test[y_test_name], model)\n",
    "        cdf_plot(X_test_df, df_test[y_test_name],  model)\n",
    "        \n",
    "        #dotted_box_cox(X_test_df, df_test[y_test_name],  model)\n",
    "\n",
    "        # print('Testing Scores:\\nR^2: %.5f\\nAdj R^2: %.5f\\nRMSE: %.5f\\nMSE: %.5f\\nMAE: %.5f\\nMAPE: %.5f\\n' %\n",
    "        #       (r2, adj_r2, rmse, mse, mae, mape))\n",
    "\n",
    "        # print('NGB-RMSE: %.5f' % ngb_rmse)\n",
    "        # print('NGB-NLL: %.5f' % ngb_nll)\n",
    "\n",
    "        # avg_target = np.mean(y_test)\n",
    "        # print('Average %s: %.5f' % (self.target_variable, avg_target))\n",
    "        # print('Pearson Correlation: %.5f' % pearson)\n",
    "        # print('Spearman Correlation: %.5f' % spearman)\n",
    "        # print('Distance Correlation: %.5f\\n' % distance)\n",
    "\n",
    "        # print('function took %.5f mins\\nfunction took %.5f secs\\n' % (time_taken_min, time_taken_sec))\n",
    "\n",
    "        # if self.save_errors_xlsx:\n",
    "        #     if self.save_validation:\n",
    "        #         row_name = '%ngboost_grid_test'\n",
    "        #     else:\n",
    "        #         row_name = 'ngboost_grid'\n",
    "        #     print('Saving results to csv file ...')\n",
    "\n",
    "        #     self.results.loc[row_name] = pd.Series({'r2': r2, 'adj-r2': adj_r2,\n",
    "        #                                                 'rmse_point': rmse, 'mse': mse,\n",
    "        #                                                 'mae': mae, 'mape': mape,\n",
    "        #                                                 'rmse_prob': ngb_rmse,\n",
    "        #                                                 'nll': ngb_nll,\n",
    "        #                                                 'avg_%s' % self.target_variable: avg_target,\n",
    "        #                                                 'pearson': pearson, 'spearman': spearman,\n",
    "        #                                                 'distance': distance,\n",
    "        #                                                 'winning_hyperparams': str(winning_hyperparameters),\n",
    "        #                                                 'training_time_min': time_taken_min,\n",
    "        #                                                 'training_time_sec': time_taken_sec\n",
    "        #                                             })\n",
    "\n",
    "        # if not os.path.exists(output_folder + '/winning_hyperparams/'):\n",
    "        #     os.makedirs(output_folder + '/winning_hyperparams/')\n",
    "        # # with open(output_folder + 'winning_hyperparams/ngboost_hyperparams.pickle', 'wb') as handle:\n",
    "        # #     pickle.dump(winning_hyperparameters, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "    def cross_validation_grid_mc_dropout(self, X_train_orig, y_train_orig, rtrain, possible_hyperparams, sort_by='rmse'):\n",
    "\n",
    "        if 'LE_bowen_corr_mm' in X_train_orig.columns:\n",
    "            X_train_orig = X_train_orig.drop(['LE_bowen_corr_mm'], axis = 1)\n",
    "\n",
    "        print('X_train cols final before training')\n",
    "        train_cols = X_train_orig.columns\n",
    "        print(X_train_orig.columns)\n",
    "        print(X_train_orig.shape)\n",
    "\n",
    "        output_folder = os.path.join(self.output_folder, 'mc_dropout/')\n",
    "\n",
    "        def get_param_grid(dicts):\n",
    "            return [dict(zip(dicts.keys(), p)) for p in it.product(*dicts.values())]\n",
    "\n",
    "        df_train = X_train_orig\n",
    "        df_train[y_test_name] = y_train_orig\n",
    "        print('Created df_train, its columns are:')\n",
    "        print(df_train.columns)\n",
    "\n",
    "            # training and testing data if we have single target variable\n",
    "\n",
    "        X_train, X_test, y_train, y_test = self.X_train, self.X_test, self.y_train, self.y_test\n",
    "\n",
    "        tempModels = []\n",
    "        tempModels_2 = []\n",
    "\n",
    "        # specify the type of cv (kfold vs. repeated kfold)\n",
    "        if self.nb_repeats_grid is None:\n",
    "            print('running %d-fold cross validation' % self.nb_folds_grid)\n",
    "            folds = get_fold_indices(X_train,y_train,self.nb_folds_grid,rtrain)\n",
    "            # kf = KFold(n_splits=self.nb_folds_grid, random_state=2652124)\n",
    "        else:\n",
    "            print('running %d-fold-%d-repeats cross validation' % (self.nb_folds_grid, self.nb_repeats_grid))\n",
    "            kf = RepeatedKFold(n_splits=self.nb_folds_grid, n_repeats=self.nb_repeats_grid, random_state=2652124)\n",
    "\n",
    "        t1 = time.time()\n",
    "\n",
    "        parameters = possible_hyperparams\n",
    "        print('defined folds and X, Y' , file=open(output_path +\"logs.txt\", \"a\"))\n",
    "        # hyper parameters loop\n",
    "        print('Total nb of hyper params: %d' % len(get_param_grid(parameters)))\n",
    "\n",
    "        for parameter in get_param_grid(parameters):\n",
    "            print('Started Parameter Looping' , file=open(output_path +\"logs.txt\", \"a\"))\n",
    "            # additional lists specific to ng boost\n",
    "            # references: https://github.com/stanfordmlgroup/ngboost/blob/master/examples/experiments/regression_exp.py\n",
    "            mc_rmse_scores, mc_nll_scores = [], []\n",
    "            accuracy_scores, aic_scores, bic_scores, nmi_scores, mape_scores,distance_scores,spearman_scores,pearson_scores,mae_scores,mse_scores,rmse_scores,adj_r2_scores,r2_scores,f1_scores,f2_scores,f5_scores,prec_scores,recall_scores = [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], []\n",
    "\n",
    "\n",
    "            for train_index, test_index in folds:\n",
    "                print('Looping over folds' , file=open(output_path +\"logs.txt\", \"a\"))\n",
    "                X_train_inner, X_val = X_train[train_index], X_train[test_index]\n",
    "                y_train_inner, y_val = y_train[train_index], y_train[test_index]\n",
    "\n",
    "                # get the hyper parameters\n",
    "                n_hidden = parameter['n_hidden']\n",
    "                num_hidden_layers = parameter['num_hidden_layers']\n",
    "                n_epochs = parameter['n_epochs']\n",
    "                epochs_multiplier = parameter['epochx']\n",
    "                tau = parameter['tau']\n",
    "                dropout_rate = parameter['dropout_rate']\n",
    "                norm = parameter['normalize']\n",
    "\n",
    "                print('started fit' , file=open(output_path +\"logs.txt\", \"a\"))\n",
    "                model = mc_dropout.net(X_train_inner, y_train_inner, ([int(n_hidden)] * num_hidden_layers),\n",
    "                                       normalize=norm, n_epochs=int(n_epochs * epochs_multiplier), tau=tau,\n",
    "                                       dropout=dropout_rate)\n",
    "\n",
    "                print('ended fit' , file=open(output_path +\"logs.txt\", \"a\"))\n",
    "                y_pred, yt_hat, mc_rmse, mc_nll = model.predict(X_val, y_val)\n",
    "\n",
    "                # error metrics for the regular point predictions\n",
    "                df_test = pd.DataFrame(data=X_val,columns= train_cols)\n",
    "                df_test[y_test_name] = y_val\n",
    "                df_test[y_test_pred_name] = y_pred\n",
    "\n",
    "                accuracy, aic, bic, nmi, mape,distance,spearman,pearson,mae,mse,rmse,adj_r2,r2,f1,f2,f5,prec,recall = evaluate(df_test, actual=y_test_name, predicted=y_test_pred_name,thresh=0.8, rel_method='extremes', extr_type='high',coef=1.5, relevance_pts=None)\n",
    "          \n",
    "\n",
    "                # add the point prediction error metrics\n",
    "                r2_scores.append(r2)\n",
    "                adj_r2_scores.append(adj_r2)\n",
    "                rmse_scores.append(rmse)\n",
    "                mse_scores.append(mse)\n",
    "                mae_scores.append(mae)\n",
    "                mape_scores.append(mape)\n",
    "                accuracy_scores.append(accuracy)\n",
    "                aic_scores.append(aic)\n",
    "                bic_scores.append(bic)\n",
    "                nmi_scores.append(nmi)\n",
    "                f1_scores.append(f1)\n",
    "                f2_scores.append(f2)\n",
    "                f5_scores.append(f1)\n",
    "                prec_scores.append(prec)\n",
    "                recall_scores.append(recall)\n",
    "                pearson_scores.append(pearson)\n",
    "                spearman_scores.append(spearman)\n",
    "                distance_scores.append(distance)\n",
    "                print('appended all scores' , file=open(output_path +\"logs.txt\", \"a\"))\n",
    "                # add the ngb probabilistic predictions\n",
    "                mc_rmse_scores.append(mc_rmse)\n",
    "                mc_nll_scores.append(mc_nll)\n",
    "                print('appended nll scores' , file=open(output_path +\"logs.txt\", \"a\"))\n",
    "\n",
    "            print('before temp models appending' , file=open(output_path +\"logs.txt\", \"a\"))\n",
    "\n",
    "            tempModels.append(\n",
    "                [parameter, np.mean(r2_scores), np.mean(adj_r2_scores), np.mean(rmse_scores), np.mean(mse_scores),\n",
    "                 np.mean(mae_scores), np.mean(mape_scores), np.mean(accuracy_scores), np.mean(aic_scores), np.mean(bic_scores), np.mean(nmi_scores), np.mean(f1_scores), np.mean(f2_scores),\n",
    "                 np.mean(f5_scores), np.mean(prec_scores), np.mean(recall_scores), np.mean(pearson_scores), np.mean(spearman_scores), np.mean(distance_scores),\n",
    "                 np.mean(mc_rmse_scores), np.mean(mc_nll_scores)])\n",
    "            \n",
    "            tempModels_2.append(\n",
    "                [parameter, np.std(r2_scores), np.std(adj_r2_scores), np.std(rmse_scores), np.std(mse_scores),\n",
    "                 np.std(mae_scores), np.std(mape_scores), np.std(accuracy_scores), np.std(aic_scores), np.std(bic_scores), np.std(nmi_scores), np.std(f1_scores), np.std(f2_scores),\n",
    "                 np.std(f5_scores), np.std(prec_scores), np.std(recall_scores), np.std(pearson_scores), np.std(spearman_scores), np.std(distance_scores),\n",
    "                 np.std(mc_rmse), np.std(mc_nll)])\n",
    "\n",
    "            print('appended big temp models score' , file=open(output_path +\"logs.txt\", \"a\"))\n",
    "\n",
    "        # the best by RMSE\n",
    "        if sort_by == 'rmse':\n",
    "            tempModels = sorted(tempModels, key=lambda k: k[3])\n",
    "            winning_hyperparameters = tempModels[0][0]\n",
    "        else:\n",
    "            # sort by the best NGB RMSE\n",
    "            tempModels = sorted(tempModels, key=lambda k: k[19])\n",
    "            winning_hyperparameters = tempModels[0][0]\n",
    "\n",
    "        print('finished sorting' , file=open(output_path +\"logs.txt\", \"a\"))\n",
    "        print('winning hyper parameters: ', str(winning_hyperparameters))\n",
    "\n",
    "        scores_dict = OrderedDict()\n",
    "        scores_dict[\"mean target\"] = average_target_variable  \n",
    "        scores_dict[\"F1\"] = tempModels[0][11]\n",
    "        scores_dict[\"F2\"] =  tempModels[0][12]\n",
    "        scores_dict[\"F05\"] = tempModels[0][13]\n",
    "        scores_dict[\"Precision\"] = tempModels[0][14]\n",
    "        scores_dict[\"Recall\"] = tempModels[0][15]\n",
    "        scores_dict[\"R2\"] = tempModels[0][1]\n",
    "        scores_dict[\"Adjusted R2\"] = tempModels[0][2]\n",
    "        scores_dict[\"RMSE\"] = tempModels[0][3]\n",
    "        scores_dict[\"MSE\"] = tempModels[0][4]\n",
    "        scores_dict[\"MAE\"] = tempModels[0][5]\n",
    "        scores_dict[\"MAPE\"] = tempModels[0][6]\n",
    "        scores_dict[\"Accuracy\"] = tempModels[0][7]\n",
    "        scores_dict[\"Pearson C.C.\"] = tempModels[0][16]\n",
    "        scores_dict[\"Spearman C.C.\"] = tempModels[0][17]\n",
    "        scores_dict[\"Spatial Distance\"] =  tempModels[0][18]\n",
    "        scores_dict[\"NMI\"] = tempModels[0][10]\n",
    "        scores_dict[\"AIC\"] = tempModels[0][8]\n",
    "        scores_dict[\"BIC\"] = tempModels[0][9]\n",
    "        scores_dict[\"Probabilistic RMSE\"] = tempModels[0][19]\n",
    "        scores_dict[\"Probabilistic NLL\"] = tempModels[0][20]\n",
    "        scores_dict[\"Data Size train\"] = '-'\n",
    "        scores_dict[\"Data Size test\"] = '-'\n",
    "        scores_dict[\"Training Time (seconds)\"] = '-'\n",
    "        scores_dict[\"Testing Time (seconds)\"] = '-'\n",
    "        export_scores(scores_dict, \"MC Validation Scores\", \"mc_dropout\")\n",
    "        \n",
    "        scores_dict[\"mean target\"] = average_target_variable  \n",
    "        scores_dict[\"F1\"] = tempModels_2[0][11]\n",
    "        scores_dict[\"F2\"] =  tempModels_2[0][12]\n",
    "        scores_dict[\"F05\"] = tempModels_2[0][13]\n",
    "        scores_dict[\"Precision\"] = tempModels_2[0][14]\n",
    "        scores_dict[\"Recall\"] = tempModels_2[0][15]\n",
    "        scores_dict[\"R2\"] = tempModels_2[0][1]\n",
    "        scores_dict[\"Adjusted R2\"] = tempModels_2[0][2]\n",
    "        scores_dict[\"RMSE\"] = tempModels_2[0][3]\n",
    "        scores_dict[\"MSE\"] = tempModels_2[0][4]\n",
    "        scores_dict[\"MAE\"] = tempModels_2[0][5]\n",
    "        scores_dict[\"MAPE\"] = tempModels_2[0][6]\n",
    "        scores_dict[\"Accuracy\"] = tempModels_2[0][7]\n",
    "        scores_dict[\"Pearson C.C.\"] = tempModels_2[0][16]\n",
    "        scores_dict[\"Spearman C.C.\"] = tempModels_2[0][17]\n",
    "        scores_dict[\"Spatial Distance\"] =  tempModels_2[0][18]\n",
    "        scores_dict[\"NMI\"] = tempModels_2[0][10]\n",
    "        scores_dict[\"AIC\"] = tempModels_2[0][8]\n",
    "        scores_dict[\"BIC\"] = tempModels_2[0][9]\n",
    "        scores_dict[\"Probabilistic RMSE\"] = tempModels_2[0][19]\n",
    "        scores_dict[\"Probabilistic NLL\"] = tempModels_2[0][20]\n",
    "        scores_dict[\"Data Size train\"] = '-'\n",
    "        scores_dict[\"Data Size test\"] = '-'\n",
    "        scores_dict[\"Training Time (seconds)\"] = '-'\n",
    "        scores_dict[\"Testing Time (seconds)\"] = '-'\n",
    "        export_scores(scores_dict, \"MC Validation Scores std\", \"mc_dropout\")\n",
    "      \n",
    "        write_to_txt('winning-hyperparams.txt', str(winning_hyperparameters))\n",
    "\n",
    "        print('winning hyper parameters: {}'.format(str(winning_hyperparameters)))\n",
    "        # print(\n",
    "        #     'Best Validation Scores:\\nR^2: %.5f\\nAdj R^2: %.5f\\nRMSE: %.5f\\nMSE: %.5f\\nMAE: %.5f\\nMAPE: %.5f\\nMC-RMSE: %.5f\\nMC-NLL: %.5f\\n' %\n",
    "        #     (tempModels[0][1], tempModels[0][2], tempModels[0][3], tempModels[0][4], tempModels[0][5],\n",
    "        #      tempModels[0][6], tempModels[0][7], tempModels[0][8]))\n",
    "\n",
    "\n",
    "\n",
    "        if smogn:\n",
    "            #redefine my train data\n",
    "            if 'LE_bowen_corr_mm' in X_train_orig.columns:\n",
    "              X_train_orig = X_train_orig.drop(['LE_bowen_corr_mm'], axis = 1)\n",
    "            \n",
    "            f_train = X_train_orig\n",
    "            df_train[target_variable] = y_train\n",
    "            # df_train.to_csv(output_path + \"df_train_before_smogn_1.csv\")\n",
    "            \n",
    "            #confirm if rel method matches rel points\n",
    "            if rel_method == 'range' and relevance_pts is None:\n",
    "                raise ValueError('You have set rel_method = range. You must provide relevance_pts as a matrix. Currently, it is None')\n",
    "\n",
    "            #get phi loss params\n",
    "            y_train_a = np.array(df_train[target_variable])\n",
    "            phi_params, loss_params, relevance_values = get_phi_loss_params(y_train_a, rel_method, extr_type, coef,relevance_pts)\n",
    "            # df_train.to_csv(output_path + \"df_train_before_smogn_after_phi.csv\")\n",
    "\n",
    "            #apply smogn on the df_train data \n",
    "            print(df_train.columns)\n",
    "            print(target_variable)\n",
    "            X_train,y_train = apply_smogn(df_train, smogn, target_variable, phi_params, thr_rel, Cperc, k, repl, dist, p, pert, plotdensity=False)\n",
    "        \n",
    "            #define X_train as df \n",
    "            X_train = pd.DataFrame(X_train, columns= cols)\n",
    "        \n",
    "            print(\"cols in X train after smogn are :\" )\n",
    "            print(X_train.columns)\n",
    "            \n",
    "            #define a smogned df_train\n",
    "            df_train_smogned = X_train\n",
    "            df_train_smogned[output_column] = y_train\n",
    "            self.check_create_dir(output_path + 'ngboost/')\n",
    "            df_train_smogned.to_csv(output_path + 'mc_dropout/' + 'df_train_smogned.csv')\n",
    "\n",
    "        if smogn:\n",
    "          if one_hot_encoded:\n",
    "              count_abnormal(X_train)\n",
    "              print(\"fixing one hot encoded cols\")\n",
    "              X_train = round_oversampled_one_hot_encoded(X_train) \n",
    "          \n",
    "          else:\n",
    "              print(\"there are no onehot encoded cols to be accounted for\")\n",
    "              \n",
    "          if 'LE_bowen_corr_mm' in X_train.columns:\n",
    "              X_train = X_train.drop(['LE_bowen_corr_mm'], axis =1 )\n",
    "\n",
    "\n",
    "        # get the winning hyper parameters\n",
    "        n_hidden = winning_hyperparameters['n_hidden']\n",
    "        num_hidden_layers = winning_hyperparameters['num_hidden_layers']\n",
    "        n_epochs = winning_hyperparameters['n_epochs']\n",
    "        epochs_multiplier = winning_hyperparameters['epochx']\n",
    "        tau = winning_hyperparameters['tau']\n",
    "        dropout_rate = winning_hyperparameters['dropout_rate']\n",
    "\n",
    "\n",
    "\n",
    "            \n",
    "        print('start with test scores' , file=open(output_path +\"logs.txt\", \"a\"))\n",
    "        # train the model\n",
    "        print(\"cols for X_train final training after cross validation\")\n",
    "        print(train_cols)\n",
    "        print(X_train.shape)\n",
    "        time1 = time.time()\n",
    "        print('fit test model' , file=open(output_path +\"logs.txt\", \"a\"))\n",
    "        model = mc_dropout.net(X_train, y_train, ([int(n_hidden)] * num_hidden_layers),\n",
    "                               normalize=True, n_epochs=int(n_epochs * epochs_multiplier), tau=tau,\n",
    "                               dropout=dropout_rate)\n",
    "\n",
    "        time2 = time.time()\n",
    "        time_tr = float(time2 - time1)\n",
    "        time1 = time.time()\n",
    "        print('columns used for X_test final')\n",
    "        y_pred, yt_hat, MC_RMSE, MC_nll = model.predict(X_test, y_test)\n",
    "        time2 = time.time()\n",
    "        time_test = float(time2 - time1)\n",
    "        print('predict test model' , file=open(output_path +\"logs.txt\", \"a\"))\n",
    "\n",
    "        # average of the `actual` target variable\n",
    "        avg_target = np.mean(y_test)\n",
    "\n",
    "        # point prediction errors\n",
    "        print('This is X_test shape')\n",
    "        print(X_test.shape)\n",
    "        df_test = pd.DataFrame(data=X_test,columns= train_cols)\n",
    "        df_test[y_test_name] = y_test\n",
    "        df_test[y_test_pred_name] = y_pred\n",
    "        print(\"this is df test after creation columns\")\n",
    "        print(df_test.columns)\n",
    "        df_test.to_csv(output_path + 'mc_dropout/' + 'df_test_pred.csv')\n",
    "        \n",
    "\n",
    "        print('calculate errors' , file=open(output_path +\"logs.txt\", \"a\"))\n",
    "        accuracy, aic, bic, nmi, mape,distance,spearman,pearson,mae,mse,rmse,adj_r2,r2,f1,f2,f5,prec,recall = evaluate(df_test, actual=y_test_name, predicted=y_test_pred_name,\n",
    "                    thresh=0.8, rel_method='extremes', extr_type='high',\n",
    "                    coef=1.5, relevance_pts=None)\n",
    "\n",
    "\n",
    "        print('create dict' , file=open(output_path +\"logs.txt\", \"a\"))\n",
    "        scores_dict = OrderedDict()\n",
    "        scores_dict[\"mean target\"] = average_target_variable  \n",
    "        scores_dict[\"F1\"] = f1\n",
    "        scores_dict[\"F2\"] =  f2\n",
    "        scores_dict[\"F05\"] = f5\n",
    "        scores_dict[\"Precision\"] =prec\n",
    "        scores_dict[\"Recall\"] = recall\n",
    "        scores_dict[\"R2\"] = r2\n",
    "        scores_dict[\"Adjusted R2\"] = adj_r2\n",
    "        scores_dict[\"RMSE\"] = rmse\n",
    "        scores_dict[\"MSE\"] = mse\n",
    "        scores_dict[\"MAE\"] = mae\n",
    "        scores_dict[\"MAPE\"] = mape\n",
    "        scores_dict[\"Accuracy\"] = accuracy\n",
    "        scores_dict[\"Pearson C.C.\"] = pearson\n",
    "        scores_dict[\"Spearman C.C.\"] = spearman\n",
    "        scores_dict[\"Spatial Distance\"] =  distance\n",
    "        scores_dict[\"NMI\"] = nmi\n",
    "        scores_dict[\"AIC\"] = aic\n",
    "        scores_dict[\"BIC\"] = bic\n",
    "        scores_dict[\"Probabilistic RMSE\"] = MC_RMSE\n",
    "        scores_dict[\"Probabilistic NLL\"] = MC_nll\n",
    "        scores_dict[\"Data Size train\"] = df_train.shape\n",
    "        scores_dict[\"Data Size test\"] = df_test.shape\n",
    "        scores_dict[\"Training Time (seconds)\"] = time_tr\n",
    "        scores_dict[\"Testing Time (seconds)\"] = time_test\n",
    "        export_scores(scores_dict, \"MC Test Scores\", \"mc_dropout\")\n",
    "\n",
    "        #plotting target variable is smogn is used\n",
    "        if smogn:\n",
    "            print(df_train.columns, df_train_smogned.columns)\n",
    "            plot_target_variable(df_train, df_train_smogned, output_column, output_path, 'mc_dropout/target_variable')\n",
    "\n",
    "        # # display error metrics\n",
    "        # print('Testing Scores:\\nR^2: %.5f\\nAdj R^2: %.5f\\nRMSE: %.5f\\nMSE: %.5f\\nMAE: %.5f\\nMAPE: %.5f\\n' %\n",
    "        #       (r2, adj_r2, rmse, mse, mae, mape))\n",
    "\n",
    "        # print('MC-RMSE: %.5f' % MC_RMSE)\n",
    "        # print('MC-NLL: %.5f' % MC_nll)\n",
    "\n",
    "        # avg_target = np.mean(y_test)\n",
    "        # print('Average %s: %.5f' % (self.target_variable, avg_target))\n",
    "        # print('Pearson Correlation: %.5f' % pearson)\n",
    "        # print('Spearman Correlation: %.5f' % spearman)\n",
    "        # print('Distance Correlation: %.5f\\n' % distance)\n",
    "\n",
    "        # print('function took %.5f mins\\nfunction took %.5f secs\\n' % (time_taken_min, time_taken_sec))\n",
    "\n",
    "        # save the model\n",
    "        models_folder = output_folder + 'trained_models/'\n",
    "        if not os.path.exists(models_folder):\n",
    "            os.makedirs(models_folder)\n",
    "        pkl_filename = \"mc_dropout.pkl\"\n",
    "        # with open(models_folder + pkl_filename, 'wb') as file:\n",
    "        #     pickle.dump(model, file)\n",
    "        print('saved model to {} as ngboost.pkl'.format(models_folder))\n",
    "\n",
    "        # Plotting\n",
    "        self.plot_actual_vs_predicted(y_test, y_pred, output_folder,\n",
    "                                      'mc_dropout')\n",
    "        self.plot_actual_vs_predicted_scatter_bisector(y_test, y_pred,\n",
    "                                                       output_folder + 'train_test_forecasts_scatterplot_bisector/',\n",
    "                                                       'mc_dropout')\n",
    "\n",
    "        self.plot_errors_mc_dropout(Yt_hat=yt_hat, X=X_test, y=y_pred, T=winning_hyperparameters['T'],\n",
    "                         output_folder=os.path.join(output_folder, 'probabilistic_forecasts/'),\n",
    "                         fig_name='errors')\n",
    "\n",
    "    def create_output_dataset(self, y_pred, model_name, output_folder):\n",
    "        if not os.path.exists(output_folder):\n",
    "            os.makedirs(output_folder)\n",
    "\n",
    "        # JUST TO AVOID THE CURRENT BUG\n",
    "        df_test_curr = self.df_test_orig\n",
    "        if 'predicted' in list(df_test_curr.columns.values):\n",
    "            df_test_curr = df_test_curr.drop('predicted', axis=1)\n",
    "\n",
    "        # add the predicted value to the df\n",
    "        target_loc = df_test_curr.columns.get_loc(self.target_variable)\n",
    "        df_test_curr.insert(target_loc + 1, 'predicted', list(y_pred))\n",
    "\n",
    "        # df_train.to_csv('train_df.csv')\n",
    "        df_test_curr.to_csv(output_folder + 'test_df_%s.csv' % model_name, index=False)\n",
    "\n",
    "        return df_test_curr\n",
    "\n",
    "    def scale_cols(self, X_train, X_test, y_train, y_test):\n",
    "\n",
    "        # z-score scaling\n",
    "        if self.scale_input:\n",
    "            if self.input_zscore is not None:\n",
    "                # apply Standard scaling to the specified columns.\n",
    "                scaler = StandardScaler()\n",
    "                X_train = X_train.astype('float64')\n",
    "                X_test = X_test.astype('float64')\n",
    "\n",
    "                X_train_zscaled = scaler.fit_transform(X_train[:, self.idx_zscore])\n",
    "                X_test_zscaled = scaler.transform(X_test[:, self.idx_zscore])\n",
    "\n",
    "                for i in range(len(self.idx_zscore)):\n",
    "                    X_train[:, self.idx_zscore[i]] = X_train_zscaled[:, i]\n",
    "                    X_test[:, self.idx_zscore[i]] = X_test_zscaled[:, i]\n",
    "\n",
    "            if self.input_minmax is not None:\n",
    "                # apply MinMax scaling to the specified columns.\n",
    "                scaler = MinMaxScaler()\n",
    "                if X_train.dtype != 'float64':\n",
    "                    X_train = X_train.astype('float64')\n",
    "                    X_test = X_test.astype('float64')\n",
    "\n",
    "                X_train_minmaxscaled = scaler.fit_transform(X_train[:, self.idx_minmax])\n",
    "                X_test_minmaxscaled = scaler.transform(X_test[:, self.idx_minmax])\n",
    "\n",
    "                for i in range(len(self.idx_minmax)):\n",
    "                    X_train[:, self.idx_minmax[i]] = X_train_minmaxscaled[:, i]\n",
    "                    X_test[:, self.idx_minmax[i]] = X_test_minmaxscaled[:, i]\n",
    "\n",
    "            if self.input_box is not None:\n",
    "                # apply BoxCox transform to the specified columns.\n",
    "                if X_train.dtype != 'float64':\n",
    "                    X_train = X_train.astype('float64')\n",
    "                    X_test = X_test.astype('float64')\n",
    "\n",
    "                X_train_boxscaled = np.array([list(scipy.stats.boxcox(X_train[:, self.idx_box[i]])[0]) for i in range(len(self.idx_box))]).T\n",
    "                X_test_boxscaled = np.array([list(scipy.stats.boxcox(X_test[:, self.idx_box[i]])[0]) for i in range(len(self.idx_box))]).T\n",
    "\n",
    "                for i in range(len(self.idx_box)):\n",
    "                    X_train[:, self.idx_box[i]] = X_train_boxscaled[:, i]\n",
    "                    X_test[:, self.idx_box[i]] = X_test_boxscaled[:, i]\n",
    "\n",
    "            if self.input_log is not None:\n",
    "                # apply Log transform to the specified columns.\n",
    "\n",
    "                if X_train.dtype != 'float64':\n",
    "                    X_train = X_train.astype('float64')\n",
    "                    X_test = X_test.astype('float64')\n",
    "\n",
    "                X_train_logscaled = np.log(X_train[:, self.idx_log])\n",
    "                X_test_logscaled = np.log(X_test[:, self.idx_log])\n",
    "\n",
    "                for i in range(len(self.idx_log)):\n",
    "                    X_train[:, self.idx_log[i]] = X_train_logscaled[:, i]\n",
    "                    X_test[:, self.idx_log[i]] = X_test_logscaled[:, i]\n",
    "\n",
    "        scaler_out_final = None\n",
    "\n",
    "        if self.scale_output:\n",
    "            if self.output_zscore:\n",
    "                scaler_out = StandardScaler()\n",
    "                y_train = y_train.reshape(-1, 1)\n",
    "                y_test = y_test.reshape(-1, 1)\n",
    "\n",
    "                y_train = scaler_out.fit_transform(y_train)\n",
    "                y_test = scaler_out.transform(y_test)\n",
    "\n",
    "                y_train = y_train.reshape(-1)\n",
    "                y_test = y_test.reshape(-1)\n",
    "\n",
    "                scaler_out_final = scaler_out\n",
    "\n",
    "            elif self.output_minmax:\n",
    "                scaler_out = MinMaxScaler()\n",
    "                y_train = y_train.reshape(-1, 1)\n",
    "                y_test = y_test.reshape(-1, 1)\n",
    "\n",
    "                y_train = scaler_out.fit_transform(y_train)\n",
    "                y_test = scaler_out.transform(y_test)\n",
    "\n",
    "                y_train = y_train.reshape(-1)\n",
    "                y_test = y_test.reshape(-1)\n",
    "\n",
    "                scaler_out_final = scaler_out\n",
    "\n",
    "            elif self.output_box:\n",
    "                # first I get the best lambda from scipy.stats, use it scipy.special to use it in scipy.spcial.inverse_box\n",
    "                y_train, self.y_train_lambda_ = scipy.stats.boxcox(y_train)\n",
    "                y_test, self.y_test_lambda_ = scipy.stats.boxcox(y_test)\n",
    "\n",
    "            else:\n",
    "                if self.output_log:\n",
    "                    y_train = np.log(y_train)\n",
    "                    y_test = np.log(y_test)\n",
    "\n",
    "        return X_train, X_test, y_train, y_test, scaler_out_final\n",
    "\n",
    "    def produce_learning_curve(self, model, model_name, nb_splits, output_folder, parameters, nb_repeats=None):\n",
    "\n",
    "        '''\n",
    "        produce learning curve of a certain model, using either KFold or repeated KFold cross validation\n",
    "        :param model: the model\n",
    "        :param model_name: name of the model, string.\n",
    "        :param nb_splits: number of splits in KFold\n",
    "        :param output_folder: path to output folder. If doesn't exist, will be created at runtime\n",
    "        :param nb_repeats: number of repeats in case of RepeatedKFold. By defualt None. If None,\n",
    "        KFold will be used instead\n",
    "        :return: saves the learning curve\n",
    "        '''\n",
    "\n",
    "        X_train, y_train = self.X_train, self.y_train\n",
    "        pipe = None\n",
    "\n",
    "        if self.scale:\n",
    "            if self.scale_output:\n",
    "                if self.output_zscore:\n",
    "                    scaler = StandardScaler()\n",
    "                    y_train = scaler.fit_transform(y_train)\n",
    "                elif self.output_minmax:\n",
    "                    scaler = MinMaxScaler()\n",
    "                    y_train = scaler.fit_transform(y_train)\n",
    "                elif self.output_log:\n",
    "                    y_train = np.log(y_train)\n",
    "                else:\n",
    "                    y_train, _ = scipy.stats.boxcox(y_train)\n",
    "\n",
    "            if self.scale_input:\n",
    "                if self.input_zscore is not None and self.input_minmax is not None:\n",
    "                    # print('1st condition')\n",
    "                    ct = ColumnTransformer([('standard', StandardScaler(), self.idx_zscore),\n",
    "                                            ('minmax', MinMaxScaler(), self.idx_minmax)], remainder='passthrough')\n",
    "                    pipe = Pipeline(steps=[('preprocessor', ct), ('model', model(**parameters))])\n",
    "\n",
    "                elif self.input_zscore is not None and self.input_minmax is None:\n",
    "                    # print('2nd condition')\n",
    "                    ct = ColumnTransformer([('standard', StandardScaler(), self.idx_zscore)], remainder='passthrough')\n",
    "                    pipe = Pipeline(steps=[('preprocessor', ct), ('model', model(**parameters))])\n",
    "\n",
    "                elif self.input_zscore is None and self.input_minmax is not None:\n",
    "                    # print('3rd condition')\n",
    "                    ct = ColumnTransformer([('minmax', MinMaxScaler(), self.idx_minmax)], remainder='passthrough')\n",
    "                    pipe = Pipeline(steps=[('preprocessor', ct), ('model', model(**parameters))])\n",
    "\n",
    "                else:\n",
    "                    # print('4th condition')\n",
    "                    pipe = model(**parameters)\n",
    "\n",
    "        else:\n",
    "            # print('4th condition')\n",
    "            pipe = model(**parameters)\n",
    "\n",
    "        if nb_repeats is None:\n",
    "            cv = KFold(n_splits=nb_splits, random_state=2652124)\n",
    "        else:\n",
    "            cv = RepeatedKFold(n_splits=nb_splits, n_repeats=nb_repeats, random_state=2652124)\n",
    "\n",
    "        # if box or log transform is needed, this must not necessarily be done in a pipeline manner\n",
    "        # because same transformation is done for training and validation, UNLIKE z-score and minmax\n",
    "        # whereby scaling must be done on training THEN taking the parameters and apply them\n",
    "        # to the validation\n",
    "\n",
    "        if self.scale:\n",
    "            if self.scale_input:\n",
    "                if self.input_box is not None:\n",
    "                    # apply BoxCox transform to the specified columns.\n",
    "                    if X_train.dtype != 'float64':\n",
    "                        X_train = X_train.astype('float64')\n",
    "\n",
    "                    X_train_boxscaled = np.array([list(scipy.stats.boxcox(X_train[:, self.idx_box[i]])[0]) for i in range(len(self.idx_box))]).T\n",
    "\n",
    "                    for i in range(len(self.idx_box)):\n",
    "                        X_train[:, self.idx_box[i]] = X_train_boxscaled[:, i]\n",
    "\n",
    "                if self.input_log is not None:\n",
    "                    # apply Log transform to the specified columns.\n",
    "                    if X_train.dtype != 'float64':\n",
    "                        X_train = X_train.astype('float64')\n",
    "\n",
    "                    X_train_logscaled = np.log(X_train[:, self.idx_log])\n",
    "\n",
    "                    for i in range(len(self.idx_log)):\n",
    "                        X_train[:, self.idx_log[i]] = X_train_logscaled[:, i]\n",
    "\n",
    "        train_sizes, train_scores, test_scores = learning_curve(pipe, X_train, y_train, cv=cv, scoring='neg_mean_squared_error')  # calculate learning curve values\n",
    "\n",
    "        train_scores_mean = -train_scores.mean(axis=1)\n",
    "        test_scores_mean = -test_scores.mean(axis=1)\n",
    "\n",
    "        plt.figure()\n",
    "        plt.xlabel(\"Number of Training Samples\")\n",
    "        plt.ylabel(\"MSE\")\n",
    "\n",
    "        plt.plot(train_sizes, train_scores_mean, label=\"training\")\n",
    "        plt.plot(train_sizes, test_scores_mean, label=\"validation\")\n",
    "        plt.legend()\n",
    "\n",
    "        if not os.path.exists(output_folder):\n",
    "            os.makedirs(output_folder)\n",
    "        plt.savefig(output_folder + '%s_learning_curve.png' % model_name)\n",
    "        plt.close()\n",
    "\n",
    "    # def plot_actual_vs_predicted(self, df, model_name, output_folder, predicted_variable):\n",
    "    #\n",
    "    #     plt.plot(list(range(1, len(df) + 1)), df[self.target_variable], color='b', label='actual')\n",
    "    #     plt.plot(list(range(1, len(df) + 1)), df[predicted_variable], color='r', label='predicted')\n",
    "    #     plt.legend(loc='best')\n",
    "    #     plt.suptitle('actual vs. predicted forecasts')\n",
    "    #\n",
    "    #     if not os.path.exists(output_folder):\n",
    "    #         os.makedirs(output_folder)\n",
    "    #     plt.savefig(output_folder + 'forecasts_%s' % model_name)\n",
    "    #     plt.close()\n",
    "\n",
    "    def plot_actual_vs_predicted(self, y_test, y_pred, output_folder, model_name):\n",
    "        plt.plot(list(range(len(y_test))), y_test, color='b', label='actual')\n",
    "        plt.plot(list(range(len(y_pred))), y_pred, color='r', label='predicted')\n",
    "        plt.legend(loc='best')\n",
    "        plt.suptitle('actual vs. predicted forecasts')\n",
    "\n",
    "        self.check_create_dir(output_folder)\n",
    "        plt.savefig(output_folder + 'forecasts_{}'.format(model_name))\n",
    "        plt.close()\n",
    "\n",
    "    def plot_actual_vs_predicted_scatter_bisector(self, y_test, y_pred, output_folder, model_name):\n",
    "        fig, ax = plt.subplots()\n",
    "\n",
    "        ax.scatter(y_test, y_pred, c='black')\n",
    "\n",
    "        lims = [\n",
    "            np.min([ax.get_xlim(), ax.get_ylim()]),  # min of both axes\n",
    "            np.max([ax.get_xlim(), ax.get_ylim()]),  # max of both axes\n",
    "        ]\n",
    "        ax.plot(lims, lims, 'k-', alpha=0.75, zorder=0)\n",
    "        ax.set_aspect('equal')\n",
    "        ax.set_xlim(lims)\n",
    "        ax.set_ylim(lims)\n",
    "\n",
    "        plt.suptitle('actual vs. predicted forecasts')\n",
    "\n",
    "        self.check_create_dir(output_folder)\n",
    "        plt.savefig(os.path.join(output_folder, 'scatter_%s' % model_name))\n",
    "        plt.close()\n",
    "\n",
    "    def errors_to_csv(self):\n",
    "        ''' saves the error metrics (stored in `results`) as a csv file '''\n",
    "        if self.results is not None:\n",
    "            errors_df = self.results\n",
    "            path = self.output_folder + 'error_metrics_csv/'\n",
    "            if not os.path.exists(path):\n",
    "                os.makedirs(path)\n",
    "            errors_df.to_csv(path + 'errors.csv')\n",
    "\n",
    "    def get_stats(y_test, y_pred, nb_columns, thr_rel, phi_params, loss_params):\n",
    "\n",
    "    # Function to compute regression error metrics between actual and predicted values +\n",
    "    # correlation between both using different methods: Pearson, Spearman, and Distance\n",
    "    # param y_test: the actual values. Example df['actual'] (the string inside is the name\n",
    "    # of the actual column. Example: df['LE (mm)'], df['demand'], etc.)\n",
    "    # param y_pred: the predicted vlaues. Example df['predicted']\n",
    "    # param nb_columns: number of columns <<discarding the target variable column>>\n",
    "    # return: R2, Adj-R2, RMSE, MSE, MAE, MAPE\n",
    "\n",
    "        def mean_absolute_percentage_error(y_true, y_pred):\n",
    "            y_true, y_pred = np.array(y_true), np.array(y_pred)\n",
    "            return np.mean(np.abs((y_true - y_pred) / y_true)) * 100\n",
    "\n",
    "        if not isinstance(y_test, list):\n",
    "            y_test = list(y_test)\n",
    "        if not isinstance(y_pred, list):\n",
    "            y_pred = list(y_pred)\n",
    "\n",
    "        n = len(y_test)\n",
    "\n",
    "        r2_Score = r2_score(y_test, y_pred)  # r-squared\n",
    "        adjusted_r2 = 1 - ((1 - r2_Score) * (n - 1)) / (n - nb_columns - 1)  # adjusted r-squared\n",
    "        rmse_score = np.sqrt(mean_squared_error(y_test, y_pred))  # RMSE\n",
    "        mse_score = mean_squared_error(y_test, y_pred)  # MSE\n",
    "        mae_score = mean_absolute_error(y_test, y_pred)  # MAE\n",
    "        #print(np.asarray(np.abs(( np.array(y_test) - np.array(y_pred)) / np.array(y_test)), dtype=np.float64))\n",
    "        mape_score = np.asarray(np.abs(( np.array(y_test) - np.array(y_pred)) / np.array(y_test)), dtype=np.float64).mean() * 100  # MAPE\n",
    "        accuracy = 100 - mape_score\n",
    "        aic = len(y_test) * np.log(mse_score)\n",
    "        bic = len(y_test) * np.log(mse_score)\n",
    "        nmi = normalized_mutual_info_score(y_test, y_pred)\n",
    "\n",
    "        trues = np.array(y_test)\n",
    "        preds = np.array(y_pred)\n",
    "\n",
    "        method = phi_params['method']\n",
    "        npts = phi_params['npts']\n",
    "        controlpts = phi_params['control.pts']\n",
    "        ymin = loss_params['ymin']\n",
    "        ymax = loss_params['ymax']\n",
    "        tloss = loss_params['tloss']\n",
    "        epsilon = loss_params['epsilon']\n",
    "\n",
    "        rmetrics = runit.eval_stats(trues, preds, thr_rel, method, npts, controlpts, ymin, ymax, tloss, epsilon)\n",
    "\n",
    "        # create a dictionary of the r metrics extracted above\n",
    "        rmetrics_dict = dict(zip(rmetrics.names, list(rmetrics)))\n",
    "\n",
    "        if isinstance(y_pred[0], np.ndarray):\n",
    "            y_pred_new = [x[0] for x in y_pred]\n",
    "            y_pred = y_pred_new\n",
    "        \n",
    "        pearson_corr, _ = pearsonr(y_test, y_pred)\n",
    "        spearman_corr, _ = spearmanr(y_test, y_pred)\n",
    "        distance_corr = distance.correlation(y_test, y_pred)\n",
    "\n",
    "        print('\\nUtility Based Metrics')\n",
    "        print('F1: %.5f' % rmetrics_dict['ubaF1'][0])\n",
    "        print('F2: %.5f' % rmetrics_dict['ubaF2'][0])\n",
    "        print('F05: %.5f' % rmetrics_dict['ubaF05'][0])\n",
    "        print('precision: %.5f' % rmetrics_dict['ubaprec'][0])\n",
    "        print('recall: %.5f' % rmetrics_dict['ubarec'][0])\n",
    "\n",
    "        print('\\nRegression Error Metrics')\n",
    "        print('R2: %.5f' % r2_Score)\n",
    "        print('Adj-R2: %.5f' % adjusted_r2)\n",
    "        print('RMSE: %.5f' % rmse_score)\n",
    "        print('MSE: %.5f' % mse_score)\n",
    "        print('MAE: %.5f' % mae_score)\n",
    "        print('MAPE: %.5f' % mape_score)\n",
    "        print('Accuracy: %.5f' % accuracy)\n",
    "        print('aic: %.5f' % aic)\n",
    "        print('bic: %.5f' % bic)\n",
    "        print('nmi: %.5f' % nmi)\n",
    "\n",
    "        print('\\nCorrelations')\n",
    "        print('Pearson: %.5f' % pearson_corr)\n",
    "        print('Spearman: %.5f' % spearman_corr)\n",
    "        print('Distance: %.5f' % distance_corr)\n",
    "        return accuracy, aic, bic, nmi, mape_score,distance_corr,spearman_corr,pearson_corr,mae_score,mse_score,rmse_score,adjusted_r2,r2_Score,rmetrics_dict['ubaF1'][0],rmetrics_dict['ubaF2'][0],rmetrics_dict['ubaF05'][0],rmetrics_dict['ubaprec'][0],rmetrics_dict['ubarec'][0]\n",
    "\n",
    "\n",
    "    def get_stats_old(self, y_test, y_pred, nb_columns):\n",
    "        '''\n",
    "        Function to compute regression and utility based error metrics between actual and predicted values as well\n",
    "        as their correlation\n",
    "        :param y_test: vector of the actual values\n",
    "        :param y_pred: vector of the predicted values\n",
    "        :param nb_columns: number of columns <<discarding the target variable column>>\n",
    "        :return: R2, Adj-R2, RMSE, MSE, MAE, MAPE, pearson, spearman, distance\n",
    "        '''\n",
    "\n",
    "        def mean_absolute_percentage_error(y_true, y_pred):\n",
    "            y_true, y_pred = np.array(y_true), np.array(y_pred)\n",
    "            return np.mean(np.abs((y_true - y_pred) / y_true)) * 100\n",
    "\n",
    "        if not isinstance(y_test, list):\n",
    "            y_test = list(y_test)\n",
    "        if not isinstance(y_pred, list):\n",
    "            y_pred = list(y_pred)\n",
    "\n",
    "        n = len(y_test)\n",
    "\n",
    "        r2_Score = r2_score(y_test, y_pred)  # r-squared\n",
    "        adjusted_r2 = 1 - ((1 - r2_Score) * (n - 1)) / (n - nb_columns - 1)  # adjusted r-squared\n",
    "        rmse_score = np.sqrt(mean_squared_error(y_test, y_pred))  # RMSE\n",
    "        mse_score = mean_squared_error(y_test, y_pred)  # MSE\n",
    "        mae_score = mean_absolute_error(y_test, y_pred)  # MAE\n",
    "        mape_score = mean_absolute_percentage_error(y_test, y_pred)  # MAPE\n",
    "\n",
    "        if isinstance(y_pred[0], np.ndarray):\n",
    "            y_pred_new = [x[0] for x in y_pred]\n",
    "            y_pred = y_pred_new\n",
    "\n",
    "        pearson_corr, _ = pearsonr(y_test, y_pred)\n",
    "        spearman_corr, _ = spearmanr(y_test, y_pred)\n",
    "        distance_corr = distance.correlation(y_test, y_pred)\n",
    "\n",
    "        return r2_Score, adjusted_r2, rmse_score, mse_score, mae_score, mape_score, pearson_corr, spearman_corr, distance_corr\n",
    "\n",
    "    ## Generate lags for all input features, re-generate even if some exist so that order will not be shuffled after nan dropping\n",
    "def generate_lags_for(df, column, lags_count):\n",
    "        for i in range(lags_count):\n",
    "            lag_name = column + \"-\" + str(i + 1)\n",
    "            df[lag_name] = df[column].shift(i + 1)\n",
    "        return df\n",
    "\n",
    "def generate_lags(df, lagsForColumns):\n",
    "    '''This function generates the lags for the list of columns'''\n",
    "    for k in range(len(lagsForColumns)):\n",
    "        col = lagsForColumns[k]\n",
    "        if col in df.columns:\n",
    "            df = generate_lags_for(df, col, 5)\n",
    "    return df\n",
    "\n",
    "def split_train_test_valid(df, TRAIN_RATIO, TEST_RATIO):\n",
    "    X_train = pd.DataFrame()\n",
    "    X_test = pd.DataFrame()\n",
    "    Y_train = pd.DataFrame()\n",
    "    Y_test = pd.DataFrame()\n",
    "    \n",
    "    unique_sites = df[\"Site\"].unique()\n",
    "    print(\"Number of sites:\", len(unique_sites))\n",
    "\n",
    "    for site in unique_sites:\n",
    "        df_site = df[df[\"Site\"] == site]\n",
    "        X = df_site\n",
    "        train_index = int(X.shape[0] * TRAIN_RATIO)\n",
    "        test_index = int(X.shape[0] * (TRAIN_RATIO + TEST_RATIO))\n",
    "\n",
    "        X_train = X_train.append(X[:train_index], ignore_index = True)\n",
    "        X_test = X_test.append(X[train_index:], ignore_index = True)\n",
    "        Y_train = Y_train.append(X[:train_index], ignore_index = True)\n",
    "        Y_test = Y_test.append(X[train_index:], ignore_index = True)\n",
    "   \n",
    "    Y_train = Y_train[[output_column]]\n",
    "    Y_test = Y_test[[output_column]]\n",
    "   \n",
    "    X_train = X_train.drop([output_column], axis = 1)\n",
    "    X_test = X_test.drop([output_column], axis = 1)\n",
    "   \n",
    "    return X_train, X_test, Y_train, Y_test\n",
    "\n",
    "\n",
    "#calculates all error metrics needed\n",
    "def calculate_errors(y_test, y_pred, nb_columns):\n",
    "    n = len(y_test)\n",
    "    r2_Score = r2_score(y_test, y_pred)  # r-squared\n",
    "    adjusted_r2 = 1 - ((1 - r2_Score) * (n - 1)) / (n - nb_columns - 1)  # adjusted r-squared\n",
    "    rmse_score = np.sqrt(mean_squared_error(y_test, y_pred))  # RMSE\n",
    "    mse_score = mean_squared_error(y_test, y_pred)  # MSE\n",
    "    mae_score = mean_absolute_error(y_test, y_pred)  # MAE\n",
    "    mape_score = np.asarray(np.abs(( np.array(y_test) - np.array(y_pred)) / np.array(y_test)), dtype=np.float64).mean() * 100  # MAPE\n",
    "    pearson_corr, _ = pearsonr(np.array(y_test).ravel(), np.array(y_pred).ravel())\n",
    "    spearman_corr, _ = spearmanr(np.array(y_test).ravel(), np.array(y_pred).ravel())\n",
    "    distance_corr = distance.correlation(np.array(y_test).ravel(), np.array(y_pred).ravel())\n",
    "    acc = 100 - mape_score\n",
    "    nmi = normalized_mutual_info_score(np.array(y_test).ravel(), np.array(y_pred).ravel())\n",
    "    aic = n * np.log(mse_score)\n",
    "    bic = n * np.log(mse_score)\n",
    "    print('\\nRegression Error Metrics')\n",
    "    print('R2: %.5f' % r2_Score)\n",
    "    print('Adj-R2: %.5f' % adjusted_r2)\n",
    "    print('RMSE: %.5f' % rmse_score)\n",
    "    print('MSE: %.5f' % mse_score)\n",
    "    print('MAE: %.5f' % mae_score)\n",
    "    print('MAPE: %.5f' % mape_score)\n",
    "    print('Accuracy: %.5f' % acc)\n",
    "    print('\\nCorrelations')\n",
    "    print('Pearson: %.5f' % pearson_corr)\n",
    "    print('Spearman: %.5f' % spearman_corr)\n",
    "    print('Distance: %.5f' % distance_corr)\n",
    "    \n",
    "    print('\\nExtra')\n",
    "    print('NMI: %.5f' % nmi)\n",
    "    print('AIC: %.5f' % aic)\n",
    "    print('BIC: %.5f' % bic)\n",
    "    \n",
    "    \n",
    "#get indices of folds in Stratified KFold CV\n",
    "def get_fold_indices(X,y,n_splits,rare_values):\n",
    "    rare_vec = [1 if i in rare_values else 0 for i in range(len(y))]\n",
    "    y = np.array(rare_vec)\n",
    "    splitter = StratifiedKFold(n_splits=n_splits, shuffle=False, random_state=123)\n",
    "    folds = list(splitter.split(X, y))\n",
    "    return folds\n",
    "\n",
    "def check_create_dir(dir):\n",
    "    if not os.path.exists(dir):\n",
    "        os.makedirs(dir)\n",
    "\n",
    "def export_scores(scores, columnName, name_model):\n",
    "    check_create_dir(output_path + name_model + '/')\n",
    "    file_name = output_path + name_model + \"/score_sheet_final.csv\"\n",
    "    if not os.path.exists(file_name):\n",
    "        print(\"path does not exist\")\n",
    "        df = pd.DataFrame(list())\n",
    "        df.to_csv(file_name, index=False)\n",
    "    else:\n",
    "        print(\"path exists\")\n",
    "        df = pd.read_csv(file_name, delimiter=',')\n",
    "\n",
    "    df[\"Error Metrics\"] = scores.keys()\n",
    "    df[columnName] = scores.values()\n",
    "    df.to_csv(file_name, index=False)\n",
    "    return df\n",
    "\n",
    "def write_to_txt(filename, content):\n",
    "    text_file = open(output_path + filename, \"w\")\n",
    "    text_file.write(content)\n",
    "    text_file.close()\n",
    "        \n",
    "def gen_xspan(mean, std, n=2, num=100):\n",
    "#Generate span for x axis to plot the probabilty distribution\n",
    "    return np.linspace(mean - n * std, mean + n * std, num)\n",
    "\n",
    "def normalize_ticks(ax):\n",
    "    cln_ticks = lambda t: float(t.replace(\"-\", \"-\"))\n",
    "    get_ticks = lambda ax: list(\n",
    "        map(cln_ticks, [item.get_text() for item in ax.get_yticklabels()])\n",
    "    )\n",
    "    labels = get_ticks(ax)\n",
    "    labels = [f\"{l / np.max(labels):.3f}\" for l in labels]\n",
    "    ax.set_yticklabels(labels)\n",
    "    return ax\n",
    "\n",
    "def heavyside(thresholds, actual):\n",
    "    # Given a deterministic observation, make a CDF out of it\n",
    "    result = [1 if t >= actual else 0 for t in thresholds]\n",
    "    return result\n",
    "\n",
    "def is_cdf_valid(case):\n",
    "    if case[0] < 0 or case[0] > 1:\n",
    "        return False\n",
    "    for i in xrange(1, len(case)):\n",
    "        if case[i] > 1 or case[i] < case[i-1]:\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "def calc_crps(thresholds, predictions, actuals):\n",
    "    nthresh = len(thresholds)  # 70 in example\n",
    "    ncases  = len(predictions)\n",
    "    crps = 0\n",
    "    for case, actual in zip(predictions, actuals):\n",
    "        if (len(case) == nthresh) and is_cdf_valid(case):\n",
    "            obscdf = heavyside(thresholds, actual)\n",
    "            for fprob, oprob in zip(case, obscdf):\n",
    "                crps = crps + (fprob - oprob)*(fprob - oprob)\n",
    "        else:\n",
    "            crps = crps + nthresh  # treat delta at each threshold as 1\n",
    "    crps = crps / float(ncases * nthresh)\n",
    "\n",
    "def rarify_data(df, df_train, df_test, target_variable, method, extr_type, thresh, coef, control_pts):\n",
    "\n",
    "    # get df_train and df_test\n",
    "    # param df_train: the training data frame\n",
    "    # param df_test: the testing data frame\n",
    "    # param target_variable: name of the target variable column\n",
    "    # return: df_train and df_test with equal class distribution between classes: rare and not rare\n",
    "\n",
    "    print(\"checking null values in dataset when applying rarify\")\n",
    "    print(df.isnull().values.any())\n",
    "\n",
    "    # get y, reset the index to avoid falsy retrievals by index later on\n",
    "    y = df[target_variable].reset_index(drop=True)\n",
    "    # get the indices of the rare values in the combined data frame\n",
    "    # note that the relevance returned is the relevance of the whole data frame not just the training\n",
    "    rare_values, phi_params, loss_params, yrel = get_rare(y, method, extr_type,thresh, coef, control_pts)\n",
    "\n",
    "    # dictionary mapping each value to its relevance\n",
    "    demandrel = {}\n",
    "    relvals = np.array(yrel)\n",
    "\n",
    "    for i, e in enumerate(y):\n",
    "        if e not in demandrel:\n",
    "            rel = relvals[i]\n",
    "            demandrel[e] = rel\n",
    "\n",
    "    # now we have the indices of the rare values, get their percentage\n",
    "\n",
    "    # percentage of rare values in the whole dataset\n",
    "    prare = len(rare_values)/len(df)\n",
    "    print('percentage of rare values in dataset before smogn: ' + str(prare*100) , file=open(output_path +\"rare_perc_results.txt\", \"a\"))\n",
    "    # number of rare values in the whole dataset\n",
    "    numrare = len(rare_values)\n",
    "    print('number of rare values in dataset before smogn: {}/{}'.format(numrare, len(df)), file=open(output_path +\"rare_perc_results.txt\", \"a\"))\n",
    "\n",
    "    # number of rare values that are be in each of the train and test\n",
    "    numraretrain = int(round(prare * len(df_train)))\n",
    "    numraretest = int(round(prare * len(df_test)))\n",
    "\n",
    "    print('number of rare in train: {}/{}'.format(numraretrain, len(df_train)), file=open(output_path +\"smogn_stats.txt\", \"a\"))\n",
    "    print('==> {}%%'.format((numraretrain/len(df_train))*100), file=open(output_path +\"smogn_stats.txt\", \"a\"))\n",
    "    print('number of rare in test: {}/{}'.format(numraretest, len(df_test)), file=open(output_path +\"smogn_stats.txt\", \"a\"))\n",
    "    print('==> {}%%'.format((numraretest / len(df_test))*100), file=open(output_path +\"smogn_stats.txt\", \"a\"))\n",
    "\n",
    "    rare_values = sorted(rare_values)\n",
    "\n",
    "    # rare indices partitioned for each of the train and test\n",
    "    rtrain = rare_values[:numraretrain]\n",
    "    rtest = rare_values[numraretrain:]\n",
    "\n",
    "    # get the relevance of each of the  dftrain and dftest\n",
    "    yreltrain = [demandrel[d] for d in df_train[target_variable]]\n",
    "    yreltest = [demandrel[d] for d in df_test[target_variable]]\n",
    "\n",
    "    if len(rtrain) != numraretrain:\n",
    "        raise ValueError('Incompatibility between the number of rare values that must be included in the '\n",
    "                         'training data for equal class distribution and the obtained number of rare')\n",
    "\n",
    "    if len(rtest) != numraretest:\n",
    "        raise ValueError('Incompatibility between the number of rare values that must be included in the '\n",
    "                         'testing data for equal class distribution and the obtained number of rare')\n",
    "\n",
    "    return rtrain, rtest, yreltrain, yreltest, phi_params['control.pts'], loss_params, demandrel\n",
    "\n",
    "\n",
    "def get_rare(y, method, extr_type, thresh, coef, control_pts):\n",
    "\n",
    "    # we will be getting the relevance function on all the data not just the training data because\n",
    "    # when we want to apply Lime on the 'rare' testing instances, the relevance function must map all possible demand\n",
    "    # values to a certain relevance. If it happens that some demand values are present only in the testing\n",
    "    # and not in the training data, we cannot detect rare values correctly. The way we compute\n",
    "    # rare values depends on the relevance\n",
    "\n",
    "    # param y: the target variable vector\n",
    "    # param method: 'extremes' or 'range'. Default is 'extremes'\n",
    "    # param extr_type: 'both', 'high', or 'low'\n",
    "    # param thresh: threshold. Default is 0.8\n",
    "    # param coef: parameter needed for method \"extremes\" to specify how far the wiskers extend to the most extreme data point in the boxplot. The default is 1.5.\n",
    "    # param control_pts: if method == 'range', then this is the relevance matrix provided by the user. Default is None\n",
    "\n",
    "    # return the indices of the rare values in the data\n",
    "\n",
    "    yrel = get_relevance_2(y, df=None, target_variable=None, method=method, extr_type=extr_type, control_pts=control_pts)\n",
    "\n",
    "    # get the the phi.control returned parameters that are used as input for computing the relevance function phi\n",
    "    # (function provided by R UBL's package: https://www.rdocumentation.org/packages/UBL/versions/0.0.6/topics/phi)\n",
    "    # (function provided by R UBL's package\n",
    "    # https://www.rdocumentation.org/packages/UBL/versions/0.0.6/topics/phi.control)\n",
    "    # we need those returned parameters for computing rare values\n",
    "\n",
    "    print('relevance method - phi function : {}'.format(method))\n",
    "\n",
    "    if control_pts is None:\n",
    "        # without relevance matrix\n",
    "        print('control.pts - phi function: {}'.format(control_pts))\n",
    "        print('without relevance matrix')\n",
    "        params = runit.get_relevance_params_extremes(y, rel_method=method, extr_type=extr_type, coef=coef)\n",
    "    else:\n",
    "        # with relevance matrix (provided by the user)\n",
    "        print('control.pts - phi function: {}'.format(control_pts))\n",
    "        print('with relevance matrix')\n",
    "        params = runit.get_relevance_params_range(y, rel_method=method, extr_type=extr_type, coef=coef,\n",
    "                                                  relevance_pts=control_pts)\n",
    "\n",
    "    # phi params\n",
    "    phi_params = params[0]\n",
    "    loss_params = params[1]\n",
    "\n",
    "    phi_params = dict(zip(phi_params.names, list(phi_params)))\n",
    "    loss_params = dict(zip(loss_params.names, list(loss_params)))\n",
    "\n",
    "    print('\\nCONTROL PTS')\n",
    "    print(phi_params['control.pts'])\n",
    "    print(\"for the whole dataset\")\n",
    "    rare_indices = get_rare_indices(y=y, y_rel=yrel, thresh=thresh, controlpts=phi_params['control.pts'])\n",
    "    # print('rare indices are: {}'.format(rare_indices))\n",
    "\n",
    "    return rare_indices, phi_params, loss_params, yrel\n",
    "\n",
    "\n",
    "def get_stats(y_test, y_pred, nb_columns, thr_rel, phi_params, loss_params):\n",
    "\n",
    "    # Function to compute regression error metrics between actual and predicted values +\n",
    "    # correlation between both using different methods: Pearson, Spearman, and Distance\n",
    "    # param y_test: the actual values. Example df['actual'] (the string inside is the name\n",
    "    # of the actual column. Example: df['LE (mm)'], df['demand'], etc.)\n",
    "    # param y_pred: the predicted vlaues. Example df['predicted']\n",
    "    # param nb_columns: number of columns <<discarding the target variable column>>\n",
    "    # return: R2, Adj-R2, RMSE, MSE, MAE, MAPE\n",
    "\n",
    "    def mean_absolute_percentage_error(y_true, y_pred):\n",
    "        y_true, y_pred = np.array(y_true), np.array(y_pred)\n",
    "        return np.mean(np.abs((y_true - y_pred) / y_true)) * 100\n",
    "\n",
    "    if not isinstance(y_test, list):\n",
    "        y_test = list(y_test)\n",
    "    if not isinstance(y_pred, list):\n",
    "        y_pred = list(y_pred)\n",
    "\n",
    "    n = len(y_test)\n",
    "\n",
    "    r2_Score = r2_score(y_test, y_pred)  # r-squared\n",
    "    adjusted_r2 = 1 - ((1 - r2_Score) * (n - 1)) / (n - nb_columns - 1)  # adjusted r-squared\n",
    "    rmse_score = np.sqrt(mean_squared_error(y_test, y_pred))  # RMSE\n",
    "    mse_score = mean_squared_error(y_test, y_pred)  # MSE\n",
    "    mae_score = mean_absolute_error(y_test, y_pred)  # MAE\n",
    "    #print(np.asarray(np.abs(( np.array(y_test) - np.array(y_pred)) / np.array(y_test)), dtype=np.float64))\n",
    "    mape_score = mean_absolute_percentage_error(y_test, y_pred)\n",
    "    #mape_score = np.asarray(np.abs(( np.array(y_test) - np.array(y_pred)) / np.array(y_test)), dtype=np.float64).mean() * 100  # MAPE\n",
    "    accuracy = 100 - mape_score\n",
    "    aic = len(y_test) * np.log(mse_score)\n",
    "    bic = len(y_test) * np.log(mse_score)\n",
    "    nmi = normalized_mutual_info_score(y_test, y_pred)\n",
    "\n",
    "    trues = np.array(y_test)\n",
    "    preds = np.array(y_pred)\n",
    "\n",
    "    method = phi_params['method']\n",
    "    npts = phi_params['npts']\n",
    "    controlpts = phi_params['control.pts']\n",
    "    ymin = loss_params['ymin']\n",
    "    ymax = loss_params['ymax']\n",
    "    tloss = loss_params['tloss']\n",
    "    epsilon = loss_params['epsilon']\n",
    "\n",
    "    rmetrics = runit.eval_stats(trues, preds, thr_rel, method, npts, controlpts, ymin, ymax, tloss, epsilon)\n",
    "\n",
    "    # create a dictionary of the r metrics extracted above\n",
    "    rmetrics_dict = dict(zip(rmetrics.names, list(rmetrics)))\n",
    "\n",
    "    if isinstance(y_pred[0], np.ndarray):\n",
    "        y_pred_new = [x[0] for x in y_pred]\n",
    "        y_pred = y_pred_new\n",
    "    \n",
    "    pearson_corr, _ = pearsonr(y_test, y_pred)\n",
    "    spearman_corr, _ = spearmanr(y_test, y_pred)\n",
    "    distance_corr = distance.correlation(y_test, y_pred)\n",
    "\n",
    "    print('\\nUtility Based Metrics')\n",
    "    print('F1: %.5f' % rmetrics_dict['ubaF1'][0])\n",
    "    print('F2: %.5f' % rmetrics_dict['ubaF2'][0])\n",
    "    print('F05: %.5f' % rmetrics_dict['ubaF05'][0])\n",
    "    print('precision: %.5f' % rmetrics_dict['ubaprec'][0])\n",
    "    print('recall: %.5f' % rmetrics_dict['ubarec'][0])\n",
    "\n",
    "    print('\\nRegression Error Metrics')\n",
    "    print('R2: %.5f' % r2_Score)\n",
    "    print('Adj-R2: %.5f' % adjusted_r2)\n",
    "    print('RMSE: %.5f' % rmse_score)\n",
    "    print('MSE: %.5f' % mse_score)\n",
    "    print('MAE: %.5f' % mae_score)\n",
    "    print('MAPE: %.5f' % mape_score)\n",
    "    print('Accuracy: %.5f' % accuracy)\n",
    "    print('aic: %.5f' % aic)\n",
    "    print('bic: %.5f' % bic)\n",
    "    print('nmi: %.5f' % nmi)\n",
    "\n",
    "    print('\\nCorrelations')\n",
    "    print('Pearson: %.5f' % pearson_corr)\n",
    "    print('Spearman: %.5f' % spearman_corr)\n",
    "    print('Distance: %.5f' % distance_corr)\n",
    "    return accuracy, aic, bic, nmi, mape_score,distance_corr,spearman_corr,pearson_corr,mae_score,mse_score,rmse_score,adjusted_r2,r2_Score,rmetrics_dict['ubaF1'][0],rmetrics_dict['ubaF2'][0],rmetrics_dict['ubaF05'][0],rmetrics_dict['ubaprec'][0],rmetrics_dict['ubarec'][0]\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "def get_relevance_2(y, df, target_variable, method, extr_type, control_pts):\n",
    "\n",
    "    # gets the relevance values of the target variable vector\n",
    "    # param y: the target variable vector\n",
    "    # param df: if y in None, this must be passed. It is the data frame of interest\n",
    "    # param target_variable: if y is None, this must be passed. It is the name of the target variable\n",
    "    # param method: 'extremes' or 'range'\n",
    "    # param extr_type: 'both', 'high', or 'low'\n",
    "    # param control_pts: if method == 'range', will be a relevance matrix provided by the user\n",
    "    # return: the relevance values of the associated target variable\n",
    "\n",
    "    # get the target variable vector y\n",
    "    if y is None:\n",
    "        if df is None or target_variable is None:\n",
    "            raise ValueError('if y is None, neither df nor target_variable must be None')\n",
    "        y = df[target_variable]\n",
    "\n",
    "    # check that the passed parameters are in order\n",
    "    if method != 'range' and method != 'extremes':\n",
    "        raise ValueError('method must be \"range\" or \"extremes\", there is no method called \"%s\"' % method)\n",
    "    elif method == 'range' and control_pts is None:\n",
    "        raise ValueError('If method == \"range\", then control_pts must not be None')\n",
    "    elif method == 'extremes' and extr_type not in ['high', 'low', 'both']:\n",
    "        raise ValueError('extr_type must wither be \"high\", \"low\", or \"both\"')\n",
    "    else:\n",
    "        if control_pts is None:\n",
    "            print('getting yrel - Control pts is {}, method is {}'.format(control_pts, method))\n",
    "            y_rel = runit.get_yrel(y=np.array(y), meth=method, extr_type=extr_type)\n",
    "        else:\n",
    "            print('getting yrel - Control pts is not None, method is {}'.format(method))\n",
    "            y_rel = runit.get_yrel(y=np.array(y), meth=method, extr_type=extr_type, control_pts=control_pts)\n",
    "\n",
    "    return y_rel\n",
    "\n",
    "\n",
    "def get_rare_indices(y, y_rel, thresh, controlpts):\n",
    "    # get the indices of the rare values in the data\n",
    "    # param y: the target variable vector\n",
    "    # param y_rel: the target variable (y) relevance vector\n",
    "    # param thresh: the threshold of interest\n",
    "    # param controlpts: the phi.control (function provided by R UBL's package: https://www.rdocumentation.org/packages/UBL/versions/0.0.6/topics/phi.control)\n",
    "    # returned parameters that are used as input for computing the relevance function phi (function provided by R UBL's package: https://www.rdocumentation.org/packages/UBL/versions/0.0.6/topics/phi)\n",
    "    # return: the indices of the rare values in 'y'\n",
    "    \n",
    "\n",
    "    # references\n",
    "    # https://github.com/paobranco/SMOGN-LIDTA17/blob/8964a2327de19f6ca9e6f7055479ca863cd6b8a0/R_Code/ExpsDIBS.R#L41\n",
    "\n",
    "    # transform controlpts returned by R into a python list\n",
    "    controlpts = list(np.array(controlpts))\n",
    "    # print(controlpts)\n",
    "\n",
    "    # boolean variable indicating whether both low and high rare exist\n",
    "    both = [controlpts[i] for i in [1, 7]] == [1, 1]\n",
    "\n",
    "    # initialize rare cases to empty list (in case there are no rare cases at all)\n",
    "    rare_cases = []\n",
    "\n",
    "    if both:\n",
    "        # bothr = True\n",
    "        print('\\nWe have both low and high extremes')\n",
    "        rare_low = [i for i, e in enumerate(y_rel) if e > thresh and y[i] < controlpts[3]]\n",
    "        rare_high = [i for i, e in enumerate(y_rel) if e > thresh and y[i] > controlpts[3]]\n",
    "\n",
    "        # merge two lists (of low rare + high rare) together\n",
    "        rare_cases = rare_low + rare_high\n",
    "\n",
    "    else:\n",
    "        print('\\nWe dont have both', end=' ')\n",
    "        if controlpts[1] == 1:\n",
    "            print('We have only low rare')\n",
    "            # lowr = True\n",
    "            rare_cases = [i for i, e in enumerate(y_rel) if e > thresh and y[i] < controlpts[3]]\n",
    "        else:\n",
    "            print('We have only high rare')\n",
    "            # highr = True\n",
    "            rare_cases = [i for i, e in enumerate(y_rel) if e > thresh and y[i] > controlpts[3]]\n",
    "\n",
    "    total = len(rare_cases)\n",
    "\n",
    "    print('Total Number of rare cases: %d out of %d' % (total, len(y)), file=open(output_path +\"smogn_stats.txt\", \"a\"))\n",
    "    print('Percentage of Rare Cases: %.2f%%\\n' % (total/len(y) * 100), file=open(output_path +\"smogn_stats.txt\", \"a\"))\n",
    "\n",
    "    return rare_cases\n",
    "\n",
    "def round_oversampled_one_hot_encoded(df):\n",
    "\n",
    "    # round one hot encoded vectors of an oversampled dataset. We have fed the SMOGN/SMOTER/GN/RandUnder\n",
    "    # a data frame having one hot encoded values (0s and 1s). However, given that we are using Euclidean/Manhattan\n",
    "    # distances for oversampling, some noise is added to these making them 1.0003, 0.99, etc.\n",
    "    # Having this said, this function will round these values back again so they are\n",
    "    # perfect 0s or 1s. We could have used HEOM distance, but it expects \"nominal\" features\n",
    "    # as opposed to one hot encodings.\n",
    "    # param df: the over-sampled data frame\n",
    "    # return: the over-sampled data frame with one hot encodings rounded\n",
    "\n",
    "    for col in one_hot_encoded:\n",
    "      if col in df.columns:\n",
    "        df.loc[df[col] < 0.5, col] = 0\n",
    "        df.loc[df[col] >=0.5, col] = 1\n",
    "    return df\n",
    "\n",
    "\n",
    "def count_abnormal(df):\n",
    "\n",
    "    # Due to Oversampling, SMOGN is adding noise to the one hot encoded vectors. This function counts how many of these\n",
    "    # are being done\n",
    "    # param df: the oversampled data frame\n",
    "    # return: statistics about the above\n",
    "\n",
    "    count = 0\n",
    "    for col in one_hot_encoded:\n",
    "        if col in df.columns:\n",
    "            for i, row in df.iterrows():\n",
    "                if row[col] not in [0, 1]:\n",
    "                    count += 1\n",
    "                else:\n",
    "                    continue\n",
    "\n",
    "    print('number of noisy one hot encoded: {} out of {}'.format(count, len(df)))\n",
    "    print('percentage of noisy one hot encoded: %.3f' % (count / len(df) * 100))\n",
    "\n",
    "\n",
    "#get indices of folds in Stratified KFold CV\n",
    "def get_rare_idex(X,y,rare_values):\n",
    "    rare_vec = [1 if i in rare_values else 0 for i in range(len(y))]\n",
    "    y = np.array(rare_vec)\n",
    "    return y\n",
    "\n",
    "#evaluate ub error metrics\n",
    "def evaluate(df, actual, predicted, thresh, rel_method='extremes', extr_type='high', coef=1.5, relevance_pts=None):\n",
    "    y = np.array(df[actual])\n",
    "    phi_params, loss_params, _ = get_phi_loss_params(y, rel_method, extr_type, coef, relevance_pts)\n",
    "\n",
    "    nb_columns = len(list(df.columns.values)) - 1\n",
    "\n",
    "    accuracy, aic, bic, nmi, mape_score,distance_corr,spearman_corr,pearson_corr,mae_score,mse_score,rmse_score,adjusted_r2,r2_Score,f1,f2,f5,prec,recall = get_stats(df[actual], df[predicted], nb_columns, thresh, phi_params, loss_params)\n",
    "    return accuracy, aic, bic, nmi, mape_score,distance_corr,spearman_corr,pearson_corr,mae_score,mse_score,rmse_score,adjusted_r2,r2_Score,f1,f2,f5,prec,recall\n",
    "\n",
    "\n",
    "def get_phi_loss_params(y, rel_method, extr_type='high', coef=1.5, relevance_pts=None):\n",
    "\n",
    "    # get the parameters of the relevance function\n",
    "    # param df: dataframe being used\n",
    "    # param target_variable: name of the target variable\n",
    "    # param rel_method: either 'extremes' or 'range'\n",
    "    # param extr_type: either 'high', 'low', or 'both' (defualt)\n",
    "    # param coef: default: 1.5\n",
    "    # param relevance_pts: the relevance matrix in case rel_method = 'range'\n",
    "    # return: phi parameters and loss parameters\n",
    "\n",
    "\n",
    "    if relevance_pts is None:\n",
    "        print('Will not use relevance matrix')\n",
    "        params = runit.get_relevance_params_extremes(y, rel_method=rel_method, extr_type=extr_type, coef=coef)\n",
    "    else:\n",
    "        print('Using supplied relevance matrix')\n",
    "        params = runit.get_relevance_params_range(y, rel_method=rel_method, extr_type=extr_type, coef=coef,\n",
    "                                                  relevance_pts=relevance_pts)\n",
    "\n",
    "    # phi params and loss params\n",
    "    phi_params = params[0]\n",
    "    loss_params = params[1]\n",
    "    relevance_values = params[2]\n",
    "\n",
    "    phi_params = dict(zip(phi_params.names, list(phi_params)))\n",
    "    loss_params = dict(zip(loss_params.names, list(loss_params)))\n",
    "\n",
    "    return phi_params, loss_params, relevance_values\n",
    "\n",
    "\n",
    "\n",
    "def get_relevance_oversampling(smogned, target_variable, targetrel):\n",
    "\n",
    "    # gets the relevance values of an oversampled data frame\n",
    "    # param smogned: the oversampled data frame\n",
    "    # param target_variable: name of the target variable column\n",
    "    # param targetrel: dictionary mapping each target variable value to a relevance value\n",
    "    # return: the relevance of the oversampled data frame\n",
    "\n",
    "    yrelafter = []\n",
    "    distances = []\n",
    "    for val in smogned[target_variable]:\n",
    "        if val in targetrel:\n",
    "            yrelafter.append(targetrel[val])\n",
    "        else:\n",
    "            nearest = min(sorted(list(targetrel.keys())), key=lambda x: abs(x - val))\n",
    "            distances.append(abs(nearest - val))\n",
    "            yrelafter.append(targetrel[nearest])\n",
    "\n",
    "    return yrelafter, distances\n",
    "  \n",
    "def get_formula(target_variable):\n",
    "\n",
    "    # gets the formula for passing it to R functions. Example: target_variable ~ col1 + col2 ...\n",
    "    # param target_variable: the name of the target variable\n",
    "    # return: R's formula as follows: target_variable ~ other[0] + other[1] + other[2] + other[3] + ...\n",
    "\n",
    "    formula = runit.create_formula(target_variable)\n",
    "    return formula\n",
    "    \n",
    "def apply_smogn(df_train, smogn, target_variable, phi_params, thr_rel, Cperc, k, repl, dist, p, pert, plotdensity=False ):\n",
    "\n",
    "    #method that applies SMOGN Algorithm to the current data frame\n",
    "    # print('getting back values from oversampled R data frame')\n",
    "    # print('before smogn')\n",
    "    #print(pandas2ri.py2ri(df_train).head(), \"this is py2ri\")\n",
    "    if smogn:\n",
    "        smogned = runit.WFDIBS(\n",
    "            fmla=get_formula(target_variable),\n",
    "            dat= pandas2ri.py2ri(df_train),\n",
    "            #dat=df_train,\n",
    "            method=phi_params['method'][0],\n",
    "            npts=phi_params['npts'][0],\n",
    "            controlpts=phi_params['control.pts'],\n",
    "            thrrel=thr_rel,\n",
    "            Cperc=Cperc,\n",
    "            k=k,\n",
    "            repl=repl,\n",
    "            dist=dist,\n",
    "            p=p, \n",
    "            pert=pert)\n",
    "\n",
    "        # print('after smogn')\n",
    "        # print('before pandas2ri')\n",
    "         #convert the oversampled R Data.Frame back to a pandas data frame\n",
    "        smogned = pandas2ri.ri2py_dataframe(smogned)\n",
    "        # print('after pandas2ri')\n",
    "\n",
    "        if plotdensity:\n",
    "            # density plot after smooting\n",
    "            plot_density(smogned,target_variable,output_folder + 'plots/', 'density_after_smogn', 'Density Plot')\n",
    "\n",
    "        X_train = np.array(smogned.loc[:, smogned.columns != target_variable])\n",
    "        y_train = np.array(smogned.loc[:, target_variable])\n",
    "\n",
    "        return X_train, y_train\n",
    "\n",
    "def plot_relevance(y, yrel, target_variable, output_folder, fig_name):\n",
    "    reldict = {}\n",
    "    y = y[target_variable]\n",
    "    for i, e in enumerate(y):\n",
    "        if e not in reldict:\n",
    "            reldict[e] = yrel[i]\n",
    "\n",
    "    reldict = dict(collections.OrderedDict(sorted(reldict.items())))\n",
    "    plt.plot(list(reldict.keys()), list(reldict.values()))\n",
    "    plt.xlabel(target_variable)\n",
    "    plt.ylabel('relevance')\n",
    "\n",
    "    plt.savefig(output_folder + fig_name)\n",
    "    plt.close()\n",
    "\n",
    "def get_relevance():\n",
    "    ctrl = phi_params['control.pts']\n",
    "    if rel_method[0] == 'extremes' and relevance_pts[0] is None:\n",
    "        rell = np.array([\n",
    "            [ctrl[0], ctrl[1], ctrl[2]],\n",
    "            [ctrl[3], ctrl[4], ctrl[5]],\n",
    "            [ctrl[6], ctrl[7], ctrl[8]]\n",
    "        ])\n",
    "    else:\n",
    "        rell = relevance_pts[0]\n",
    "\n",
    "    return rell\n",
    "\n",
    "\n",
    "def plot_target_variable(df, df_resampled, output_column, output_folder, fig_name):\n",
    "    y = df[output_column]\n",
    "    y_resamp = df_resampled[output_column]\n",
    "    plt.plot(list(range(len(y))), sorted(y), label = \"original\")\n",
    "    plt.plot(list(range(len(y_resamp))), sorted(y_resamp), label = \"resampled\")\n",
    "    plt.xlabel('Index')\n",
    "    plt.ylabel(target_variable)\n",
    "    plt.legend()\n",
    "    plt.savefig(output_folder + fig_name)\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "def gen_xspan(mean, std, n=2, num=100):\n",
    "#Generate span for x axis to plot the probabilty distribution\n",
    "    return np.linspace(mean - n * std, mean + n * std, num)\n",
    "\n",
    "def normalize_ticks(ax):\n",
    "    cln_ticks = lambda t: float(t.replace(\"-\", \"-\"))\n",
    "    get_ticks = lambda ax: list(\n",
    "        map(cln_ticks, [item.get_text() for item in ax.get_yticklabels()])\n",
    "    )\n",
    "    labels = get_ticks(ax)\n",
    "    labels = [f\"{l / np.max(labels):.3f}\" for l in labels]\n",
    "    ax.set_yticklabels(labels)\n",
    "    return ax\n",
    "\n",
    "def heavyside(thresholds, actual):\n",
    "    # Given a deterministic observation, make a CDF out of it\n",
    "    result = [1 if t >= actual else 0 for t in thresholds]\n",
    "    return result\n",
    "\n",
    "def is_cdf_valid(case):\n",
    "    if case[0] < 0 or case[0] > 1:\n",
    "        return False\n",
    "    for i in xrange(1, len(case)):\n",
    "        if case[i] > 1 or case[i] < case[i-1]:\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "def calc_crps(thresholds, predictions, actuals):\n",
    "    nthresh = len(thresholds)  # 70 in example\n",
    "    ncases  = len(predictions)\n",
    "    crps = 0\n",
    "    for case, actual in zip(predictions, actuals):\n",
    "        if (len(case) == nthresh) and is_cdf_valid(case):\n",
    "            obscdf = heavyside(thresholds, actual)\n",
    "            for fprob, oprob in zip(case, obscdf):\n",
    "                crps = crps + (fprob - oprob)*(fprob - oprob)\n",
    "        else:\n",
    "            crps = crps + nthresh  # treat delta at each threshold as 1\n",
    "    crps = crps / float(ncases * nthresh)\n",
    "\n",
    "\n",
    "def pdf_plot(X_test, Y_test, ngb_exp):\n",
    "    colors = [\"olive\", \"navy\", \"tomato\", \"turquoise\", \"red\"]\n",
    "    fig, ax = plt.subplots(1, 1)\n",
    "    num_plots = 5\n",
    "    cands = np.sort(np.random.choice(np.arange(0, X_test.shape[0]), num_plots))\n",
    "    for cand, c in zip(cands, colors):\n",
    "        y_dists = ngb_exp.pred_dist(np.array(X_test.iloc[cand]).reshape(1, -1))\n",
    "        x_span = gen_xspan(y_dists.scale, y_dists.scale, num=100)\n",
    "        x_span = x_span[x_span >=0]\n",
    "        dist_values = y_dists.pdf(x_span)\n",
    "        ax.plot(x_span, dist_values, color=c, label=f\"{np.array(Y_test.iloc[cand])}\")\n",
    "        ax.legend(loc=\"upper right\")\n",
    "        del y_dists, x_span, dist_values\n",
    "    fig.canvas.draw()\n",
    "    # ax = normalize_ticks(ax)\n",
    "    fig.suptitle(\"Probability Density Functions for 5 Random Predictions\")\n",
    "    fig.savefig(output_path+ 'ng_boost/' + 'pdf')\n",
    "\n",
    "\n",
    "def cdf_plot(X_test, Y_test, ngb_exp):\n",
    "    colors = [\"olive\", \"navy\", \"tomato\", \"turquoise\", \"red\"]\n",
    "    fig, ax = plt.subplots(1, 1)\n",
    "    num_plots = 5\n",
    "    cands = np.sort(np.random.choice(np.arange(0, X_test.shape[0]), num_plots))\n",
    "    print(cands)\n",
    "    for cand, c in zip(cands, colors):\n",
    "        y_dists = ngb_exp.pred_dist(np.array(X_test.iloc[cand]).reshape(1, -1))\n",
    "        x_span = gen_xspan(y_dists.scale, y_dists.scale, num=100)\n",
    "        x_span = x_span[x_span >=0]\n",
    "        dist_values = y_dists.cdf(x_span)\n",
    "        ax.plot(x_span, dist_values, color=c, label=f\"{np.array(Y_test.iloc[cand])}\")\n",
    "        ax.legend(loc=\"upper right\")\n",
    "        del y_dists, x_span, dist_values\n",
    "    fig.canvas.draw()\n",
    "    # ax = normalize_ticks(ax)\n",
    "    fig.suptitle(\"Cumaltive Distribution plots for 5 Random Predictions\")\n",
    "    fig.savefig(output_path+ 'ng_boost/' + 'cdf')\n",
    "\n",
    "\n",
    "def dotted_box_cox(X_test, Y_test, ngb_exp):\n",
    "    num_plots = 10\n",
    "    cands = np.sort(np.random.choice(np.arange(0, X_test.shape[0]), num_plots))\n",
    "    x_span = []\n",
    "    actual = []\n",
    "\n",
    "    for cand in cands:\n",
    "        y_dists = ngb_exp.pred_dist(np.array(X_test.iloc[cand]).reshape(1, -1))\n",
    "        actual.append(Y_test.iloc[cand])\n",
    "        x_span.append(gen_xspan(y_dists.scale, y_dists.scale, num=100))\n",
    "\n",
    "    x_span = np.array(x_span)\n",
    "    x_span = x_span[x_span >=0]\n",
    "    x_span = np.array(x_span).reshape(75,10)\n",
    "\n",
    "    print(x_span.shape)\n",
    "    print(cands.shape)\n",
    "    pyplot.boxplot(x_span, labels=cands)\n",
    "\n",
    "    count = 0\n",
    "    for i in range(10) :\n",
    "        y = actual[i]\n",
    "        print(y)\n",
    "        x1 = np.random.normal(i+1, 0.02, 1)\n",
    "        plt.plot(x1, y, 'r.')\n",
    "        \n",
    "    print(\"The red dots which fit in range are \" + str(count) + \" out of \" + str(num_plots))\n",
    "        \n",
    "    # pyplot.show()\n",
    "    ppyplot.savefig(output_path+ 'ng_boost/' + 'box_cox')\n",
    "    \n",
    "def binary_encode_column(df, columnToEncode):\n",
    "    encoder = ce.BinaryEncoder(cols=[columnToEncode])\n",
    "    df_encoder = encoder.fit_transform(df[columnToEncode])\n",
    "    df = pd.concat([df, df_encoder], axis=1)\n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "########################################################################################\n",
    "                               #Hyper-params \n",
    "#######################################################################################\n",
    "# random search first to suggest a few hyperparameters followed by grid search to zoom in\n",
    "#Which features to tune\n",
    "#Which starting point and which ending point for the space of each parameters\n",
    "#Which hop per space\n",
    "\n",
    "possible_hyperparams_per_model = {\n",
    "\n",
    "    'ngboost': {\n",
    "         'Dist': [Exponential],\n",
    "         'n_estimators': [1500],\n",
    "         'learning_rate': [0.1],\n",
    "         'minibatch_frac': [1.0],\n",
    "         'verbose': [False],\n",
    "#        'Dist': [Exponential],\n",
    "        'Base': [DecisionTreeRegressor(criterion='friedman_mse', max_depth=5) ]\n",
    "#        'n_estimators': [1000,1500,2000],\n",
    "#        'learning_rate': [0.1, 0.01, 0.2, 0.3],\n",
    "#        'minibatch_frac': [1.0],\n",
    "#        'verbose': [False]\n",
    "    },\n",
    "\n",
    "    'mc_dropout': {\n",
    "        'n_hidden': [100],\n",
    "        'n_epochs': [4],\n",
    "#         'n_epochs': [10, 15, 20],\n",
    "        'num_hidden_layers': [4],\n",
    "\n",
    "        # Multiplier for the number of epochs for training. - not a parameter in the model\n",
    "        'epochx': [5],\n",
    "        'normalize': [True],\n",
    "        'tau': [0.15],\n",
    "#         'tau': [0.1, 0.15, 0.2],\n",
    "        'dropout_rate': [0.01],\n",
    "#         'dropout_rate': [0.005, 0.01, 0.05, 0.1],\n",
    "#         'T': [100, 1000, 1500]\n",
    "        'T': [1000]\n",
    "\n",
    "    },\n",
    "\n",
    "    'deep_ensemble': {\n",
    "        'learning_rate': [0.0001, 0.001, 0.01, 0.1],\n",
    "        'batch_size': [256, 356]\n",
    "    }\n",
    "\n",
    "}\n",
    "\n",
    "########################################################################################################################\n",
    "                                            #Establish a connection to R library\n",
    "########################################################################################################################\n",
    "rpy2.robjects.numpy2ri.activate()\n",
    "runit = robjects.r\n",
    "runit['source']('smogn.R')\n",
    "\n",
    "########################################################################################\n",
    "                               #Read Data\n",
    "#######################################################################################\n",
    "#specify path of dataset\n",
    "input_path = \"/apps/data/Library_Daily_Albedo_NDVI_LST_Cleaned.csv\"\n",
    "#specify saved file directory \n",
    "output_path = \"/apps/output/\"\n",
    "\n",
    "df = pd.read_csv(input_path, delimiter=',')\n",
    "output_column ='LE_bowen_corr_mm'\n",
    "\n",
    "########################################################################################\n",
    "                               #Preprocess\n",
    "#######################################################################################\n",
    "\n",
    "#define columns to drop\n",
    "#columnsToDrop = ['Year','Month','Day',\n",
    "#                 'Climate', 'Vegetation', 'Latitude', 'Longitude',\n",
    "#                 'G','G-1','G-2','G-3','G-4','G-5',\n",
    "#                 'Climate_1', 'Climate_2', 'Climate_3',\n",
    "#                 'Latitude_1','Latitude_2', 'Latitude_3', 'Latitude_4', 'Latitude_5',\n",
    "#                 'Latitude_6','Longitude_1', 'Longitude_2', 'Longitude_3', 'Longitude_4',\n",
    "#                 'Longitude_5', 'Longitude_6',\n",
    "#                 'H', 'H_bowen_corr', 'H_bowen_corr-1', 'H_bowen_corr-2', 'H_bowen_corr-3', 'H_bowen_corr-4',\n",
    "#                 'H_bowen_corr-5', 'C_BOWENS',\n",
    "#                 'NETRAD','NETRAD-1','NETRAD-2','NETRAD-3','NETRAD-4','NETRAD-5',\n",
    "#                 'LE', 'LE_bowen_corr',\n",
    "#                 'Elevation(m)_1','Elevation(m)_2', 'Elevation(m)_3', 'Elevation(m)_4',\n",
    "#                 'Elevation(m)_5', 'Elevation(m)_6',\n",
    "#                 'ETo', 'EToF', 'ETr', 'ETrF', 'SW_IN'] \n",
    "                 \n",
    "                 \n",
    "#Library\n",
    "columnsToDrop = ['Year','Month','Day','Latitude','Longitude',\n",
    "                 'Vegetation', 'G','G-1','G-2','G-3','G-4','G-5',\n",
    "                 'H','H_bowen_corr','H_bowen_corr-1','H_bowen_corr-2','H_bowen_corr-3',\n",
    "                 'H_bowen_corr-4','H_bowen_corr-5', 'H_ebr_corr','H_ebr_corr-1','H_ebr_corr-2',\n",
    "                 'H_ebr_corr-3','H_ebr_corr-4','H_ebr_corr-5','LE_ebr_corr',\n",
    "                 'ET_bowen','ET_bowen_corr','ET_ebr','ET_ebr_corr',\n",
    "                 'ET_ebr_corr(mm)' ,'NETRAD-1','NETRAD-2','NETRAD-3','NETRAD-4',\n",
    "                 'NETRAD-5','LE','LE_bowen_corr','EToF_bowen','EToF_ebr',\n",
    "                 'ETr','ETrF_bowen','ETrF_ebr', 'Climate_1',\n",
    "                 'Climate_2','Climate_3', 'Latitude_1', \"SW_IN\",\n",
    "                 'Latitude_2','Latitude_3','Latitude_4','Latitude_5','Latitude_6', 'Longitude_1',\n",
    "                 'Longitude_2','Longitude_3','Longitude_4','Longitude_5','Longitude_6',\n",
    "                 'Elevation(m)_1','Elevation(m)_2','Elevation(m)_3','Elevation(m)_4',\n",
    "                 'Elevation(m)_5','Elevation(m)_6', 'NETRAD', 'LE_ebr_corr(mm)',\n",
    "                 'ET_bowen_corr_mm', 'ETo', 'Climate', 'Site_1', 'Site_2', 'Site_3', 'Site_4', 'Site_5', 'Site_6'] \n",
    "                 \n",
    "#columnsToDrop = ['Site_1', 'Site_2', 'Site_3', 'Site_4', 'Site_5', 'Cluster', 'ET_ebr_corr(mm)']\n",
    "\n",
    "\n",
    "#specify if you wish to apply over sampling by smogn or utility based\n",
    "utility_based = True\n",
    "smogn = False\n",
    "\n",
    "#smogn relate hyper-params\n",
    "target_variable = \"LE_bowen_corr_mm\"\n",
    "rel_method='range'\n",
    "extr_type='high'\n",
    "coef=1.5\n",
    "\n",
    "rell = np.array([\n",
    "    [1, 0 , 0],\n",
    "    [4, 0 , 0],\n",
    "    [15, 1 , 0],\n",
    "])\n",
    "\n",
    "#rell = None\n",
    "relevance_pts=rell\n",
    "rel=\"auto\"\n",
    "thr_rel=0.2\n",
    "Cperc=np.array([1,1.2])\n",
    "k=5\n",
    "repl=False\n",
    "dist=\"Manhattan\"\n",
    "p=2\n",
    "pert=0.1\n",
    "\n",
    "\n",
    "#specify one-hot encoded vector names \n",
    "one_hot_encoded = ['Site_1', 'Site_2', 'Site_3', 'Site_4', 'Site_5', 'Month_1',\n",
    "                 'Month_2', 'Month_3', 'Month_4', 'Vegetation_1', 'Vegetation_2',\n",
    "                 'Vegetation_3']\n",
    "\n",
    "#specify name of target variable and name of predicted target variable\n",
    "y_test_name = \"LE_bowen_corr_mm\"\n",
    "y_test_pred_name = \"LE_bowen_corr_pred\"\n",
    "\n",
    "#drop na\n",
    "df.dropna(inplace=True)\n",
    "\n",
    "#set output variable between 1 and 15 only\n",
    "df = df[df[output_column].between(1, 15)]\n",
    "\n",
    "#drop desired columns, rename, and drop the nans\n",
    "df = df.drop(columnsToDrop, axis = 1)\n",
    "df.dropna(inplace=True)\n",
    "\n",
    "#generate lags for columns\n",
    "lagsForColumns = [\"WS\", \"RH\", \"TA\", \"Eeflux_LST\", \"Eeflux_Albedo\", \"Eeflux_NDVI\", \"SW_IN\"]\n",
    "df = generate_lags(df, lagsForColumns)\n",
    "\n",
    "df = binary_encode_column(df, \"Site\")\n",
    "df.drop(columns=['Site_0'], inplace=True)\n",
    "\n",
    "#drop nan for the first 5 rows of the generated lags only 5 rows will be removed in here\n",
    "df.isnull().mean() * 10\n",
    "df.dropna(inplace=True)\n",
    "print(\"Whole Dataset original Shape\")\n",
    "print(df.shape)\n",
    "print(\"checking null values in the whole dataset\")\n",
    "print(df.isnull().values.any())\n",
    "print(df.columns)\n",
    "df.dropna(inplace=True)\n",
    "\n",
    "\n",
    "\n",
    "average_target_variable = np.average(df['LE_bowen_corr_mm'])\n",
    "\n",
    "########################################################################################\n",
    "                               #Split Data\n",
    "#######################################################################################\n",
    "\n",
    "#split into train and test according to special split\n",
    "X_train, X_test, y_train, y_test = split_train_test_valid(df, 0.8, 0.2)\n",
    "X_test.to_csv(output_path+\"X_test_original.csv\")\n",
    "X_train = X_train.drop(['Site', 'Date'], axis = 1)\n",
    "X_test = X_test.drop(['Site', 'Date'], axis = 1)\n",
    "\n",
    "df_train = X_train\n",
    "df_train['LE_bowen_corr_mm'] = y_train\n",
    "df_test = X_test\n",
    "df_test['LE_bowen_corr_mm'] = y_test\n",
    "\n",
    "#checking if null values exist here\n",
    "print(\"checking null values in train\")\n",
    "print(df_train.isnull().values.any())\n",
    "print(\"checking null values in test\")\n",
    "print(df_test.isnull().values.any())\n",
    "\n",
    "if utility_based:\n",
    "    #retrieve indexes of rare values to utilize in stratified folds\n",
    "    df.dropna(inplace=True)\n",
    "    rtrain, rtest, yreltrain, yreltest, phi_params, loss_params, targetrel = rarify_data(df, df_train, df_test, output_column, rel_method,extr_type, thr_rel,coef, relevance_pts)\n",
    "    # df_train.to_csv(output_path + \"df_train_after_rarify.csv\")\n",
    "    #plotting relevance values in both train and test target variables\n",
    "    yreltrain = np.array(yreltrain).reshape(len(yreltrain), 1)\n",
    "    plot_relevance(y_train, yreltrain, output_column, output_path, \"relevance_values_train_data\")\n",
    "    plot_relevance(y_test, yreltest, output_column, output_path, \"relevance_values_test_data\")\n",
    "    df_rel = df_train \n",
    "    df_rel[\"y_rel\"] = yreltrain\n",
    "    df_rel.to_csv(output_path+\"yrel_df.csv\")\n",
    "    X_train = X_train.drop([\"y_rel\"], axis=1)\n",
    "\n",
    "output_column = 'LE_bowen_corr_mm'\n",
    "#Ensuring target variable column is dropped\n",
    "X_train = X_train.drop([output_column], axis=1)\n",
    "X_test = X_test.drop([output_column], axis=1)\n",
    "#showing columns used in training and size of X_train\n",
    "cols = X_train.columns\n",
    "print(\"cols in X train Final \")\n",
    "print(X_train.columns)\n",
    "print(\"cols in X test Final \")\n",
    "print(X_test.columns)\n",
    "print(\"size of Xtrain Final is : \")\n",
    "print(X_train.shape)\n",
    "print(\"size of Xtest Final is : \")\n",
    "print(X_test.shape)\n",
    "\n",
    "df_train = X_train\n",
    "df_train['LE_bowen_corr_mm'] = y_train\n",
    "df_test = X_test\n",
    "df_test['LE_bowen_corr_mm'] = y_test\n",
    "\n",
    "X_train_orig = X_train\n",
    "y_train_orig = y_train \n",
    "\n",
    "########################################################################################\n",
    "                               #Run\n",
    "#######################################################################################\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    df_train_collated = df_train\n",
    "    df_test_collated = df_test\n",
    "    print('Columns for Original Dataset which will be used in training')\n",
    "    print(df_train_collated.columns)\n",
    "\n",
    "    #specify saved file directory \n",
    "    output_folder = \"/apps/output/\"\n",
    "\n",
    "\n",
    "    pfa = ProbabilisticForecastsAnalyzer(df_train_collated, target_variable='LE_bowen_corr_mm',\n",
    "                            split_ratio=0.2, output_folder=output_folder,\n",
    "                            scale=False,\n",
    "                            scale_output=False,\n",
    "                            output_zscore=False, output_minmax=False, output_box=False, output_log=False,\n",
    "                            input_zscore=None, input_minmax=None, input_box=None, input_log=None,\n",
    "                            cols_drop=None,\n",
    "                            grid=True, random_grid=False,\n",
    "                            nb_folds_grid=5, nb_repeats_grid=None,\n",
    "                            testing_data=df_test_collated,\n",
    "                            save_errors_xlsx=True,\n",
    "                            save_validation=False)\n",
    "\n",
    "    print('\\n********** Results for NGBoost **********')\n",
    "    pfa.cross_validation_grid_ngboost(X_train_orig, y_train_orig, rtrain, possible_hyperparams=possible_hyperparams_per_model['ngboost'], sort_by='rmse')\n",
    "\n",
    "#     print('\\n********** Results for MCDropout **********')\n",
    "#     pfa.cross_validation_grid_mc_dropout(X_train_orig, y_train_orig, rtrain,  possible_hyperparams=possible_hyperparams_per_model['mc_dropout'], sort_by='rmse')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "keras==2.3.1\n",
    "tensorflow==2.1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting keras==2.3.1\n",
      "  Downloading Keras-2.3.1-py2.py3-none-any.whl (377 kB)\n",
      "\u001b[K     || 377 kB 220 kB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from keras==2.3.1) (1.1.0)\n",
      "Collecting pyyaml\n",
      "  Downloading PyYAML-5.3.1.tar.gz (269 kB)\n",
      "\u001b[K     || 269 kB 488 kB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python3.6/dist-packages (from keras==2.3.1) (1.0.8)\n",
      "Requirement already satisfied: numpy>=1.9.1 in /usr/local/lib/python3.6/dist-packages (from keras==2.3.1) (1.18.1)\n",
      "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.6/dist-packages (from keras==2.3.1) (1.13.0)\n",
      "Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.6/dist-packages (from keras==2.3.1) (1.4.1)\n",
      "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras==2.3.1) (2.10.0)\n",
      "Building wheels for collected packages: pyyaml\n",
      "  Building wheel for pyyaml (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for pyyaml: filename=PyYAML-5.3.1-cp36-cp36m-linux_x86_64.whl size=45919 sha256=78d4951a7b04f7a927dec42e271a3b37a5757b4e46ce3cf36cc0de12cbf3c8b4\n",
      "  Stored in directory: /root/.cache/pip/wheels/e5/9d/ad/2ee53cf262cba1ffd8afe1487eef788ea3f260b7e6232a80fc\n",
      "Successfully built pyyaml\n",
      "Installing collected packages: pyyaml, keras\n",
      "Successfully installed keras-2.3.1 pyyaml-5.3.1\n",
      "\u001b[33mWARNING: You are using pip version 20.2.1; however, version 20.2.2 is available.\n",
      "You should consider upgrading via the '/usr/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install keras==2.3.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting ngboost\n",
      "  Downloading ngboost-0.2.3-py3-none-any.whl (38 kB)\n",
      "Collecting lifelines>=0.22.8\n",
      "  Downloading lifelines-0.25.2-py3-none-any.whl (344 kB)\n",
      "\u001b[K     || 344 kB 53 kB/s eta 0:00:011\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.17.2 in /usr/local/lib/python3.6/dist-packages (from ngboost) (1.18.1)\n",
      "Collecting scikit-learn>=0.21.3\n",
      "  Using cached scikit_learn-0.23.2-cp36-cp36m-manylinux1_x86_64.whl (6.8 MB)\n",
      "Requirement already satisfied: scipy>=1.3.1 in /usr/local/lib/python3.6/dist-packages (from ngboost) (1.4.1)\n",
      "Collecting tqdm>=4.36.1\n",
      "  Downloading tqdm-4.48.2-py2.py3-none-any.whl (68 kB)\n",
      "\u001b[K     || 68 kB 78 kB/s eta 0:00:011\n",
      "\u001b[?25hRequirement already satisfied: pandas>=0.23.0 in /usr/local/lib/python3.6/dist-packages (from lifelines>=0.22.8->ngboost) (0.24.0)\n",
      "Requirement already satisfied: matplotlib>=3.0 in /usr/local/lib/python3.6/dist-packages (from lifelines>=0.22.8->ngboost) (3.1.3)\n",
      "Collecting autograd>=1.3\n",
      "  Downloading autograd-1.3.tar.gz (38 kB)\n",
      "Collecting autograd-gamma>=0.3\n",
      "  Downloading autograd_gamma-0.4.2-py2.py3-none-any.whl (3.8 kB)\n",
      "Requirement already satisfied: patsy>=0.5.0 in /usr/local/lib/python3.6/dist-packages (from lifelines>=0.22.8->ngboost) (0.5.1)\n",
      "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn>=0.21.3->ngboost) (0.16.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from scikit-learn>=0.21.3->ngboost) (2.1.0)\n",
      "Requirement already satisfied: python-dateutil>=2.5.0 in /usr/local/lib/python3.6/dist-packages (from pandas>=0.23.0->lifelines>=0.22.8->ngboost) (2.8.1)\n",
      "Requirement already satisfied: pytz>=2011k in /usr/local/lib/python3.6/dist-packages (from pandas>=0.23.0->lifelines>=0.22.8->ngboost) (2020.1)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=3.0->lifelines>=0.22.8->ngboost) (2.4.6)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=3.0->lifelines>=0.22.8->ngboost) (0.10.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=3.0->lifelines>=0.22.8->ngboost) (1.1.0)\n",
      "Requirement already satisfied: future>=0.15.2 in /usr/local/lib/python3.6/dist-packages (from autograd>=1.3->lifelines>=0.22.8->ngboost) (0.18.2)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from patsy>=0.5.0->lifelines>=0.22.8->ngboost) (1.13.0)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from kiwisolver>=1.0.1->matplotlib>=3.0->lifelines>=0.22.8->ngboost) (44.0.0)\n",
      "Building wheels for collected packages: autograd\n",
      "  Building wheel for autograd (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for autograd: filename=autograd-1.3-py3-none-any.whl size=48959 sha256=4da503cd882111dc0d0d50e60da2fdd69bb5f7eeb58e9cfd47006b2a68b03718\n",
      "  Stored in directory: /root/.cache/pip/wheels/b2/a5/3c/929b91003a3b75a175ccba7cef35200a1890cdb46903f18072\n",
      "Successfully built autograd\n",
      "Installing collected packages: autograd, autograd-gamma, lifelines, scikit-learn, tqdm, ngboost\n",
      "  Attempting uninstall: scikit-learn\n",
      "    Found existing installation: scikit-learn 0.20.3\n",
      "    Uninstalling scikit-learn-0.20.3:\n",
      "      Successfully uninstalled scikit-learn-0.20.3\n",
      "Successfully installed autograd-1.3 autograd-gamma-0.4.2 lifelines-0.25.2 ngboost-0.2.3 scikit-learn-0.23.2 tqdm-4.48.2\n",
      "\u001b[33mWARNING: You are using pip version 20.2.1; however, version 20.2.2 is available.\n",
      "You should consider upgrading via the '/usr/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install ngboost"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
