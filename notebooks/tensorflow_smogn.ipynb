{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "THIS IS THE VERSION OF KERAS\n",
      "2.1.0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "pd.set_option('use_inf_as_na', True)\n",
    "import numpy as np\n",
    "from scipy.spatial import distance\n",
    "import rpy2.robjects as robjects\n",
    "from rpy2.robjects import pandas2ri\n",
    "import rpy2\n",
    "import rpy2.robjects.packages as rpackages\n",
    "import rpy2.robjects as ro\n",
    "pandas2ri.activate()\n",
    "import rpy2.robjects.numpy2ri\n",
    "rpy2.robjects.numpy2ri.activate()\n",
    "from rpy2.robjects.conversion import localconverter\n",
    "import warnings\n",
    "from rpy2.rinterface import RRuntimeWarning\n",
    "from collections import OrderedDict\n",
    "warnings.filterwarnings(\"ignore\", category=RRuntimeWarning)\n",
    "pandas2ri.activate()\n",
    "from matplotlib import pyplot\n",
    "from collections import Counter\n",
    "from sklearn.metrics.cluster import normalized_mutual_info_score\n",
    "from sklearn.model_selection import StratifiedKFold, KFold\n",
    "from numpy import mean\n",
    "from numpy import std\n",
    "from pandas import read_csv\n",
    "from sklearn.preprocessing import RobustScaler, OneHotEncoder, MinMaxScaler, PowerTransformer, StandardScaler\n",
    "from scipy.stats import normaltest\n",
    "from sklearn.model_selection import ParameterSampler\n",
    "from numpy.random import randn\n",
    "from scipy.stats import shapiro\n",
    "from sklearn.model_selection import train_test_split\n",
    "from numpy import *\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "from scipy.stats.stats import pearsonr, spearmanr\n",
    "from scipy import stats\n",
    "import tensorflow as tf\n",
    "import multiprocessing as mp\n",
    "import time\n",
    "import os\n",
    "import collections\n",
    "import matplotlib.pyplot as plt\n",
    "import itertools as it\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)\n",
    "import re\n",
    "import warnings\n",
    "import keras\n",
    "print(\"THIS IS THE VERSION OF KERAS\")\n",
    "print(tf.__version__)\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "########################################################################################################################\n",
    "                                            #Setting Global Variables\n",
    "########################################################################################################################\n",
    "\n",
    "#specify path of dataset\n",
    "input_path = \"/apps/data/Library_Daily_Albedo_NDVI_LST_Cleaned.csv\"\n",
    "\n",
    "#specify saved file directory \n",
    "output_path = \"/apps/output/\"\n",
    "\n",
    "#specify columns to drop\n",
    "\n",
    "#Manual\n",
    "#columnsToDrop = ['Year','Month','Day',\n",
    "#                 'Climate', 'Vegetation', 'Latitude', 'Longitude',\n",
    "#                 'G','G-1','G-2','G-3','G-4','G-5',\n",
    "#                 'Climate_1', 'Climate_2', 'Climate_3',\n",
    "#                 'Latitude_1','Latitude_2', 'Latitude_3', 'Latitude_4', 'Latitude_5',\n",
    "#                 'Latitude_6','Longitude_1', 'Longitude_2', 'Longitude_3', 'Longitude_4',\n",
    "#                 'Longitude_5', 'Longitude_6',\n",
    "#                 'H', 'H_bowen_corr', 'H_bowen_corr-1', 'H_bowen_corr-2', 'H_bowen_corr-3', 'H_bowen_corr-4',\n",
    "#                 'H_bowen_corr-5', 'C_BOWENS',\n",
    "#                 'NETRAD','NETRAD-1','NETRAD-2','NETRAD-3','NETRAD-4','NETRAD-5',\n",
    "#                 'LE', 'LE_bowen_corr',\n",
    "#                 'Elevation(m)_1','Elevation(m)_2', 'Elevation(m)_3', 'Elevation(m)_4',\n",
    "#                 'Elevation(m)_5', 'Elevation(m)_6',\n",
    "#                 'ETo', 'EToF', 'ETr', 'ETrF', 'SW_IN']            \n",
    "                  \n",
    "#                  \n",
    "#Library\n",
    "columnsToDrop = ['Year','Month','Day','Latitude','Longitude',\n",
    "                 'Climate','Vegetation', 'G','G-1','G-2','G-3','G-4','G-5',\n",
    "                 'H','H_bowen_corr','H_bowen_corr-1','H_bowen_corr-2','H_bowen_corr-3',\n",
    "                 'H_bowen_corr-4','H_bowen_corr-5', 'H_ebr_corr','H_ebr_corr-1','H_ebr_corr-2',\n",
    "                 'H_ebr_corr-3','H_ebr_corr-4','H_ebr_corr-5','LE_ebr_corr',\n",
    "                 'ET_bowen','ET_bowen_corr','ET_ebr','ET_ebr_corr',\n",
    "                 'ET_ebr_corr(mm)' ,'NETRAD-1','NETRAD-2','NETRAD-3','NETRAD-4',\n",
    "                 'NETRAD-5','LE','LE_bowen_corr','EToF_bowen','EToF_ebr',\n",
    "                 'ETr','ETrF_bowen','ETrF_ebr', 'Climate_1',\n",
    "                 'Climate_2','Climate_3', 'Latitude_1', \"SW_IN\",\n",
    "                 'Latitude_2','Latitude_3','Latitude_4','Latitude_5','Latitude_6', 'Longitude_1',\n",
    "                 'Longitude_2','Longitude_3','Longitude_4','Longitude_5','Longitude_6',\n",
    "                 'Elevation(m)_1','Elevation(m)_2','Elevation(m)_3','Elevation(m)_4',\n",
    "                 'Elevation(m)_5','Elevation(m)_6', 'NETRAD', 'LE_ebr_corr(mm)', 'ET_bowen_corr_mm', 'ETo'\n",
    "                 ]    \n",
    "\n",
    "#columnsToDrop = ['Date']\n",
    "#specify the output column\n",
    "output_column = \"LE_bowen_corr_mm\"\n",
    "#output_column = \"ratio_output\"\n",
    "\n",
    "#specify name of target variable and name of predicted target variable\n",
    "y_test_name = \"LE_bowen_corr_mm\"\n",
    "y_test_pred_name = \"LE_bowen_corr_pred\"\n",
    "\n",
    "#rename variables with spacing and under score for better proper namings\n",
    "columns_rename={\"Site Id_1\": \"Site_1\", \"Site Id_2\": \"Site_2\",\n",
    "        \"Site Id_3\": \"Site_3\", \"Site Id_4\": \"Site_4\",\n",
    "        \"Site Id_5\": \"Site_5\", \"Site Id_6\": \"Site_6\"}\n",
    "\n",
    "#specify one-hot encoded vector names \n",
    "one_hot_encoded = ['Site_1', 'Site_2', 'Site_3', 'Site_4', 'Site_5', 'Site_6', 'Month_1',\n",
    "                 'Month_2', 'Month_3', 'Month_4', 'Vegetation_1', 'Vegetation_2',\n",
    "                 'Vegetation_3']\n",
    "                  \n",
    "#specify desired split size\n",
    "test_size = 0.2\n",
    "\n",
    "#specify if scaling\n",
    "scaling = True\n",
    "\n",
    "#specify if automatic or manual scaling\n",
    "automatic = True\n",
    "  \n",
    "#if not automatic specify desired column names\n",
    "all_columns = ['WS', 'RH', 'TA', 'LE', 'ET_bowen_corr']\n",
    "#specify the scaling type for each column\n",
    "scaling = ['MinMax', 'Standard', 'Robust', 'MinMax + PowerTransform', 'Standard + PowerTransform']\n",
    "\n",
    "#specify the option of utility based\n",
    "utility_based = True\n",
    "\n",
    "#specify if batch training \n",
    "train_batches = False\n",
    "\n",
    "#specify number of parameters in random search\n",
    "n_params = 100\n",
    "\n",
    "#specify batch size of hyper-parameters\n",
    "batch_size = 4\n",
    "\n",
    "#specify number of batch you'd like to train model over\n",
    "batch_num = 2\n",
    "\n",
    "#specify if random search\n",
    "random_search = False\n",
    "\n",
    "#specify if grid search\n",
    "grid_search = True \n",
    "\n",
    "#spcify repetitions and folds for repeated stratified cross validation \n",
    "repetitions = 1\n",
    "folds = 5\n",
    "\n",
    "#specify if you wish to apply over sampling by smogn\n",
    "smogn = False\n",
    "\n",
    "#smogn relate hyper-params\n",
    "target_variable = \"LE_bowen_corr_mm\"\n",
    "rel_method='range'\n",
    "extr_type='high'\n",
    "coef=1.5\n",
    "\n",
    "rell = np.array([\n",
    "    [1, 0 , 0],\n",
    "    [4, 0 , 0],\n",
    "    [15, 1 , 0],\n",
    "])\n",
    "#rell = None\n",
    "relevance_pts=rell\n",
    "rel=\"auto\"\n",
    "thr_rel=0.1\n",
    "Cperc=np.array([1,1.2])\n",
    "k=5\n",
    "repl=False\n",
    "dist=\"Euclidean\"\n",
    "p=2\n",
    "pert=0.1\n",
    "\n",
    "########################################################################################################################\n",
    "                                            #Helper Methods\n",
    "########################################################################################################################\n",
    "\n",
    "# checks if the input is gaussian by shapiro wilk test\n",
    "def check_distribution_shapiro(col):\n",
    "    stat, p = shapiro(col)\n",
    "    alpha = 0.05\n",
    "    if p > alpha:\n",
    "        gaussian = True\n",
    "    else:\n",
    "        gaussian = False\n",
    "\n",
    "    return gaussian\n",
    "\n",
    "# checks if the input is gaussian by dagostino test\n",
    "def check_distribution_dagostino(col):\n",
    "    stat, p = normaltest(col)\n",
    "    alpha = 0.05\n",
    "    if p > alpha:\n",
    "        gaussian = True\n",
    "    else:\n",
    "        gaussian = False\n",
    "\n",
    "    return gaussian\n",
    "\n",
    "\n",
    "# splits data into train and test\n",
    "def train_test_splitting(X, y, test_size=0.2):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=1234)\n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "\n",
    "# does automatic standardization according to column values\n",
    "def standardization_needed(col, X):\n",
    "    col = col.values.reshape(-1, 1)\n",
    "    X = X.values.reshape(-1, 1)\n",
    "    # checks if column has zero values\n",
    "    if 0 not in col:\n",
    "        col_trans = Standard(col, X)\n",
    "        col_trans_pow = apply_power_trans(col_trans)\n",
    "    else:\n",
    "        # if there are zero values, applies MinMaxScaler\n",
    "        # check range of values\n",
    "        col_trans = Min_Max(col, X)\n",
    "        col_trans_pow = apply_power_trans(col_trans)\n",
    "    return np.ravel(col_trans_pow)\n",
    "\n",
    "# does manual standardization according to user input\n",
    "def standardization_needed_manual(col, X, scaling):\n",
    "    col = col.values.reshape(-1, 1)\n",
    "    X = X.values.reshape(-1, 1)\n",
    "    if scaling == 'None':\n",
    "        col_trans_final = col\n",
    "    if scaling == 'MinMax':\n",
    "        col_trans_final = Min_Max(col, X)\n",
    "    if scaling == 'Standard':\n",
    "        col_trans_final = Standard(col, X)\n",
    "    if scaling == 'Robust':\n",
    "        col_trans_final = Robust(col, X)\n",
    "    if scaling == 'MinMax + PowerTransform':\n",
    "        col_trans = Min_Max(col, X)\n",
    "        col_trans_final = apply_power_trans(col_trans)\n",
    "    if scaling == 'Standard + PowerTransform':\n",
    "        col_trans = Standard(col, X)\n",
    "        col_trans_final = apply_power_trans(col_trans)\n",
    "    return np.ravel(col_trans_final)\n",
    "\n",
    "\n",
    "def Min_Max(col, X):\n",
    "    if any(n < 0 for n in col):\n",
    "        scaler = MinMaxScaler((-1, 1))\n",
    "    else:\n",
    "        scaler = MinMaxScaler((0, 1))\n",
    "    scaler.fit(X)\n",
    "    col_trans = scaler.transform(col)\n",
    "    return col_trans\n",
    "\n",
    "def Min_Max_auto(X_train, X_test, cols):\n",
    "  for col in X_train.columns:\n",
    "    if any(n < 0 for n in cols):\n",
    "      scaler = MinMaxScaler((-1, 1))\n",
    "    else:\n",
    "      scaler = MinMaxScaler((0, 1))\n",
    "    X_train[col] = X_train[col].values.reshape(-1, 1)\n",
    "    X_test[col] = X_test[col].values.reshape(-1, 1)\n",
    "    scaler.fit( X_train[col].values.reshape(-1, 1))\n",
    "    X_test[col] = scaler.transform(X_test[col].values.reshape(-1, 1))\n",
    "  return X_test\n",
    "\n",
    "\n",
    "def Standard(col, X):\n",
    "    if 0 not in col:\n",
    "        # if no zero values, apply StandardScaler\n",
    "        scaler = StandardScaler()\n",
    "        scaler.fit(X)\n",
    "        col_trans = scaler.transform(col)\n",
    "        return col_trans\n",
    "    else:\n",
    "        print('column has 0 values, cannot apply standard scaling')\n",
    "        return col\n",
    "\n",
    "\n",
    "def Robust(col, X):\n",
    "    scaler = RobustScaler()\n",
    "    scaler.fit(X)\n",
    "    col_trans = scaler.transform(col)\n",
    "    return col_trans\n",
    "\n",
    "# does power transform on column automatically\n",
    "def apply_power_trans(col_trans):\n",
    "    if any(n <= 0 for n in col_trans):\n",
    "        # if there are negative or zero values, applies yeo-johnson transform\n",
    "        pt = PowerTransformer('yeo-johnson')\n",
    "        col_trans_pow = pt.fit_transform(col_trans)\n",
    "        # checks if column values are strictly positive\n",
    "    elif all(n > 0 for n in col_trans):\n",
    "        # if values are strictly positive, applies box-cox transform\n",
    "        pt = PowerTransformer('box-cox')\n",
    "        col_trans_pow = pt.fit_transform(col_trans)\n",
    "    return col_trans_pow\n",
    "\n",
    "# does scaling on dataset automatically\n",
    "def apply_scaling(df, columns, X_train):\n",
    "    df_scaled = pd.DataFrame(columns=df.columns)\n",
    "    for i in df.columns: \n",
    "        #checking if data is Gaussian\n",
    "        if i in columns:\n",
    "            if not check_distribution_shapiro(df[i]) and not check_distribution_dagostino(df[i]):\n",
    "                print(str(i) + ' does not have a Gaussian distribution and will be scaled')\n",
    "                #scaling data\n",
    "                df_scaled[i] = standardization_needed(df[i], X_train[i])\n",
    "            else: \n",
    "                df_scaled[i] = df[i]\n",
    "                print(str(i) + ' has a gaussian distribution')\n",
    "        else:\n",
    "            df_scaled[i] = df[i]\n",
    "    return df_scaled\n",
    "\n",
    "# does scaling on dataset according to user input\n",
    "def apply_scaling_manual(df, columns, X_train, scaling):\n",
    "    iterator = 0\n",
    "    if len(scaling) != len(columns):\n",
    "        print(\"Please specify scaling for all columns listed\")\n",
    "        return\n",
    "    else:\n",
    "        df_scaled = pd.DataFrame(columns=df.columns)\n",
    "        for i in df.columns:\n",
    "            if i in columns:\n",
    "             # checking if data is Gaussian\n",
    "                if not check_distribution_shapiro(df[i]) and not check_distribution_dagostino(df[i]):\n",
    "                    print(str(i) + ' does not have a Gaussian distribution and will be scaled')\n",
    "                    # scaling data\n",
    "                    df_scaled[i] = standardization_needed_manual(df[i], X_train[i], scaling[iterator])\n",
    "                    iterator = iterator + 1\n",
    "                else:\n",
    "                    df_scaled[i] = df[i]\n",
    "                    print(str(i) + ' has a gaussian distribution')\n",
    "            else:\n",
    "                df_scaled[i] = df[i]\n",
    "        return df_scaled\n",
    "\n",
    "\n",
    "def get_rare(y, method, extr_type, thresh, coef, control_pts):\n",
    "\n",
    "    # we will be getting the relevance function on all the data not just the training data because\n",
    "    # when we want to apply Lime on the 'rare' testing instances, the relevance function must map all possible demand\n",
    "    # values to a certain relevance. If it happens that some demand values are present only in the testing\n",
    "    # and not in the training data, we cannot detect rare values correctly. The way we compute\n",
    "    # rare values depends on the relevance\n",
    "\n",
    "    # param y: the target variable vector\n",
    "    # param method: 'extremes' or 'range'. Default is 'extremes'\n",
    "    # param extr_type: 'both', 'high', or 'low'\n",
    "    # param thresh: threshold. Default is 0.8\n",
    "    # param coef: parameter needed for method \"extremes\" to specify how far the wiskers extend to the most extreme data point in the boxplot. The default is 1.5.\n",
    "    # param control_pts: if method == 'range', then this is the relevance matrix provided by the user. Default is None\n",
    "\n",
    "    # return the indices of the rare values in the data\n",
    "\n",
    "    yrel = get_relevance_2(y, df=None, target_variable=None, method=method, extr_type=extr_type, control_pts=control_pts)\n",
    "\n",
    "    # get the the phi.control returned parameters that are used as input for computing the relevance function phi\n",
    "    # (function provided by R UBL's package: https://www.rdocumentation.org/packages/UBL/versions/0.0.6/topics/phi)\n",
    "    # (function provided by R UBL's package\n",
    "    # https://www.rdocumentation.org/packages/UBL/versions/0.0.6/topics/phi.control)\n",
    "    # we need those returned parameters for computing rare values\n",
    "\n",
    "    print('relevance method - phi function : {}'.format(method))\n",
    "\n",
    "    if control_pts is None:\n",
    "        # without relevance matrix\n",
    "        print('control.pts - phi function: {}'.format(control_pts))\n",
    "        print('without relevance matrix')\n",
    "        params = runit.get_relevance_params_extremes(y, rel_method=method, extr_type=extr_type, coef=coef)\n",
    "    else:\n",
    "        # with relevance matrix (provided by the user)\n",
    "        print('control.pts - phi function: {}'.format(control_pts))\n",
    "        print('with relevance matrix')\n",
    "        params = runit.get_relevance_params_range(y, rel_method=method, extr_type=extr_type, coef=coef,\n",
    "                                                  relevance_pts=control_pts)\n",
    "\n",
    "    # phi params\n",
    "    phi_params = params[0]\n",
    "    loss_params = params[1]\n",
    "\n",
    "    phi_params = dict(zip(phi_params.names, list(phi_params)))\n",
    "    loss_params = dict(zip(loss_params.names, list(loss_params)))\n",
    "\n",
    "    print('\\nCONTROL PTS')\n",
    "    print(phi_params['control.pts'])\n",
    "    print(\"for the whole dataset\")\n",
    "    rare_indices = get_rare_indices(y=y, y_rel=yrel, thresh=thresh, controlpts=phi_params['control.pts'])\n",
    "    # print('rare indices are: {}'.format(rare_indices))\n",
    "\n",
    "    return rare_indices, phi_params, loss_params, yrel\n",
    "\n",
    "\n",
    "def get_relevance_2(y, df, target_variable, method, extr_type, control_pts):\n",
    "\n",
    "    # gets the relevance values of the target variable vector\n",
    "    # param y: the target variable vector\n",
    "    # param df: if y in None, this must be passed. It is the data frame of interest\n",
    "    # param target_variable: if y is None, this must be passed. It is the name of the target variable\n",
    "    # param method: 'extremes' or 'range'\n",
    "    # param extr_type: 'both', 'high', or 'low'\n",
    "    # param control_pts: if method == 'range', will be a relevance matrix provided by the user\n",
    "    # return: the relevance values of the associated target variable\n",
    "\n",
    "    # get the target variable vector y\n",
    "    if y is None:\n",
    "        if df is None or target_variable is None:\n",
    "            raise ValueError('if y is None, neither df nor target_variable must be None')\n",
    "        y = df[target_variable]\n",
    "\n",
    "    # check that the passed parameters are in order\n",
    "    if method != 'range' and method != 'extremes':\n",
    "        raise ValueError('method must be \"range\" or \"extremes\", there is no method called \"%s\"' % method)\n",
    "    elif method == 'range' and control_pts is None:\n",
    "        raise ValueError('If method == \"range\", then control_pts must not be None')\n",
    "    elif method == 'extremes' and extr_type not in ['high', 'low', 'both']:\n",
    "        raise ValueError('extr_type must wither be \"high\", \"low\", or \"both\"')\n",
    "    else:\n",
    "        if control_pts is None:\n",
    "            print('getting yrel - Control pts is {}, method is {}'.format(control_pts, method))\n",
    "            y_rel = runit.get_yrel(y=np.array(y), meth=method, extr_type=extr_type)\n",
    "        else:\n",
    "            print('getting yrel - Control pts is not None, method is {}'.format(method))\n",
    "            y_rel = runit.get_yrel(y=np.array(y), meth=method, extr_type=extr_type, control_pts=control_pts)\n",
    "\n",
    "    return y_rel\n",
    "\n",
    "\n",
    "def get_rare_indices(y, y_rel, thresh, controlpts):\n",
    "    # get the indices of the rare values in the data\n",
    "    # param y: the target variable vector\n",
    "    # param y_rel: the target variable (y) relevance vector\n",
    "    # param thresh: the threshold of interest\n",
    "    # param controlpts: the phi.control (function provided by R UBL's package: https://www.rdocumentation.org/packages/UBL/versions/0.0.6/topics/phi.control)\n",
    "    # returned parameters that are used as input for computing the relevance function phi (function provided by R UBL's package: https://www.rdocumentation.org/packages/UBL/versions/0.0.6/topics/phi)\n",
    "    # return: the indices of the rare values in 'y'\n",
    "    \n",
    "\n",
    "    # references\n",
    "    # https://github.com/paobranco/SMOGN-LIDTA17/blob/8964a2327de19f6ca9e6f7055479ca863cd6b8a0/R_Code/ExpsDIBS.R#L41\n",
    "\n",
    "    # transform controlpts returned by R into a python list\n",
    "    controlpts = list(np.array(controlpts))\n",
    "    # print(controlpts)\n",
    "\n",
    "    # boolean variable indicating whether both low and high rare exist\n",
    "    both = [controlpts[i] for i in [1, 7]] == [1, 1]\n",
    "\n",
    "    # initialize rare cases to empty list (in case there are no rare cases at all)\n",
    "    rare_cases = []\n",
    "\n",
    "    if both:\n",
    "        # bothr = True\n",
    "        print('\\nWe have both low and high extremes')\n",
    "        rare_low = [i for i, e in enumerate(y_rel) if e > thresh and y[i] < controlpts[3]]\n",
    "        rare_high = [i for i, e in enumerate(y_rel) if e > thresh and y[i] > controlpts[3]]\n",
    "\n",
    "        # merge two lists (of low rare + high rare) together\n",
    "        rare_cases = rare_low + rare_high\n",
    "\n",
    "    else:\n",
    "        print('\\nWe dont have both', end=' ')\n",
    "        if controlpts[1] == 1:\n",
    "            print('We have only low rare')\n",
    "            # lowr = True\n",
    "            rare_cases = [i for i, e in enumerate(y_rel) if e > thresh and y[i] < controlpts[3]]\n",
    "        else:\n",
    "            print('We have only high rare')\n",
    "            # highr = True\n",
    "            rare_cases = [i for i, e in enumerate(y_rel) if e > thresh and y[i] > controlpts[3]]\n",
    "\n",
    "    total = len(rare_cases)\n",
    "\n",
    "    print('Total Number of rare cases: %d out of %d' % (total, len(y)))\n",
    "    print('Percentage of Rare Cases: %.2f%%\\n' % (total/len(y) * 100))\n",
    "\n",
    "    return rare_cases\n",
    "\n",
    "\n",
    "def round_oversampled_one_hot_encoded(df):\n",
    "\n",
    "    # round one hot encoded vectors of an oversampled dataset. We have fed the SMOGN/SMOTER/GN/RandUnder\n",
    "    # a data frame having one hot encoded values (0s and 1s). However, given that we are using Euclidean/Manhattan\n",
    "    # distances for oversampling, some noise is added to these making them 1.0003, 0.99, etc.\n",
    "    # Having this said, this function will round these values back again so they are\n",
    "    # perfect 0s or 1s. We could have used HEOM distance, but it expects \"nominal\" features\n",
    "    # as opposed to one hot encodings.\n",
    "    # param df: the over-sampled data frame\n",
    "    # return: the over-sampled data frame with one hot encodings rounded\n",
    "\n",
    "    for col in one_hot_encoded:\n",
    "        df.loc[df[col] < 0.5, col] = 0\n",
    "        df.loc[df[col] >=0.5, col] = 1\n",
    "    return df\n",
    "\n",
    "\n",
    "def count_abnormal(df):\n",
    "\n",
    "    # Due to Oversampling, SMOGN is adding noise to the one hot encoded vectors. This function counts how many of these\n",
    "    # are being done\n",
    "    # param df: the oversampled data frame\n",
    "    # return: statistics about the above\n",
    "\n",
    "    count = 0\n",
    "    for col in one_hot_encoded:\n",
    "        for i, row in df.iterrows():\n",
    "            if row[col] not in [0, 1]:\n",
    "                count += 1\n",
    "            else:\n",
    "                continue\n",
    "\n",
    "    print('number of noisy one hot encoded: {} out of {}'.format(count, len(df)))\n",
    "    print('percentage of noisy one hot encoded: %.3f' % (count / len(df) * 100))\n",
    "\n",
    "#calculates all error metrics needed\n",
    "def calculate_errors(actual, predicted, nb_columns):\n",
    "    n = len(actual)\n",
    "    r2score = r2_score(actual, predicted)\n",
    "    adjusted_r2 = 1 - ((1 - r2score) * (n - 1)) / (n - nb_columns - 1)\n",
    "    mase = mean_absolute_error(actual, predicted)\n",
    "    rms = sqrt(mean_squared_error(actual, predicted))\n",
    "    mse = mean_squared_error(actual, predicted)\n",
    "    re = (mse / np.mean(predicted)) * 100\n",
    "    pearson, pval = stats.pearsonr(np.array(actual).ravel(), np.array(predicted).ravel())\n",
    "    mae = np.mean(np.abs((actual - predicted) / actual)) * 100\n",
    "    spearman_corr, _ = spearmanr(actual, predicted)\n",
    "    distance_corr = distance.correlation(actual, predicted)\n",
    "    mape_score = np.asarray(np.abs(( np.array(actual) - np.array(predicted)) / np.array(actual)), dtype=np.float64).mean() * 100\n",
    "    return r2score, mase, rms, mse, re, pearson, pval, mae, adjusted_r2, spearman_corr, distance_corr, mape_score\n",
    "\n",
    "#get indices of folds in Stratified KFold CV\n",
    "def get_fold_indices(X,y,n_splits,rare_values):\n",
    "    rare_vec = [1 if i in rare_values else 0 for i in range(len(y))]\n",
    "    y = np.array(rare_vec)\n",
    "    splitter = StratifiedKFold(n_splits=n_splits, shuffle=False, random_state=123)\n",
    "    folds = list(splitter.split(X, y))\n",
    "    return folds\n",
    "    \n",
    "#get indices of folds in Stratified KFold CV\n",
    "def get_rare_idex(X,y,rare_values):\n",
    "    rare_vec = [1 if i in rare_values else 0 for i in range(len(y))]\n",
    "    y = np.array(rare_vec)\n",
    "    return y\n",
    "  \n",
    "#get grid of all hyper-parameters\n",
    "def get_param_grid(dicts):\n",
    "  return [dict(zip(dicts.keys(), p)) for p in it.product(*dicts.values())]\n",
    "\n",
    "def model_fit_predict_CV(X, y, split, params):\n",
    "\n",
    "    X_train, y_train = X.iloc[split[0],:], y.iloc[split[0], :]\n",
    "    X_valid, y_valid   = X.iloc[split[1],:], y.iloc[split[1], :]\n",
    "    \n",
    "    reg = _doFitBoostedTreeRegressor(X_train, y_train, X_train.columns, params)\n",
    "    y_pred = _doPredictBoostedTreeRegressor(X_valid, reg)\n",
    "    \n",
    "    df_test = X_valid\n",
    "\n",
    "    # combine y_test and y_pred in 1 dataset\n",
    "    df_test[y_test_name] = y_valid\n",
    "    df_test[y_test_pred_name] = y_pred\n",
    "    \n",
    "    accuracy, aic, bic, nmi, mape_score,distance_corr,spearman_corr,pearson_corr,mae_score,mse_score,rmse_score,adjusted_r2,r2_Score,f1,f2,f5,prec,recall = evaluate(df_test, actual=y_test_name, predicted=y_test_pred_name,\n",
    "             thresh=0.8, rel_method='extremes', extr_type='high',\n",
    "             coef=1.5, relevance_pts=None)\n",
    "             \n",
    "    return accuracy, aic, bic, nmi, mape_score,distance_corr,spearman_corr,pearson_corr,mae_score,mse_score,rmse_score,adjusted_r2,r2_Score,f1,f2,f5,prec,recall\n",
    "\n",
    "#make input data pipeline for tf model\n",
    "def make_input_fn(X, y, n_epochs=None, shuffle=False):\n",
    "    NUM_EXAMPLES = math.floor(len(y) / 2)\n",
    "\n",
    "    def input_fn():\n",
    "    \n",
    "        dataset = tf.data.Dataset.from_tensor_slices((dict(X), y))\n",
    "        #print(dataset)\n",
    "        if shuffle:\n",
    "          dataset = dataset.shuffle(NUM_EXAMPLES)\n",
    "        #For training, cycle thru dataset as many times as need (n_epochs=None).\n",
    "        dataset = dataset.repeat(n_epochs)\n",
    "        # In memory training doesn't use batching.\n",
    "        dataset = dataset.batch(NUM_EXAMPLES,  drop_remainder=False)\n",
    "        #print(dataset)\n",
    "        return dataset\n",
    "\n",
    "    return input_fn\n",
    "\n",
    "#make test input data pipeline for tf model\n",
    "def make_input_fn_test(X, n_epochs=None, shuffle=False):\n",
    "    NUM_EXAMPLES = math.floor(len(X) / 2)\n",
    "    def input_fn():\n",
    "        dataset = tf.data.Dataset.from_tensor_slices(dict(X))\n",
    "        if shuffle:\n",
    "            dataset = dataset.shuffle(NUM_EXAMPLES)\n",
    "        # For training, cycle thru dataset as many times as need (n_epochs=None).\n",
    "        dataset = dataset.repeat(n_epochs)\n",
    "        # In memory training doesn't use batching.\n",
    "        dataset = dataset.batch(NUM_EXAMPLES,drop_remainder=True)\n",
    "        #print(dataset)\n",
    "        return dataset\n",
    "\n",
    "    return input_fn\n",
    "\n",
    "#train the boosted tree\n",
    "def _doFitBoostedTreeRegressor(X, Y, columns, params):\n",
    "    # Define our feature columns\n",
    "    fc = tf.feature_column\n",
    "    feature_columns = []\n",
    "    NUMERIC_COLUMNS = columns\n",
    "\n",
    "    for feature_name in NUMERIC_COLUMNS:\n",
    "        feature_columns.append(tf.feature_column.numeric_column(feature_name, dtype=tf.float32))\n",
    "\n",
    "    # Creating the TF dataset\n",
    "    train_input_fn = make_input_fn(X, Y)\n",
    "    #print(train_input_fn, 'train input fn')\n",
    "\n",
    "    # Defining the estimator (BoostedTreeRegressor)\n",
    "    n_batches = 2\n",
    "    est = tf.estimator.BoostedTreesRegressor(feature_columns, n_batches_per_layer=n_batches, **params)\n",
    "    # Training the Model\n",
    "    est.train(train_input_fn, max_steps=100)\n",
    "    print(\"Done training for hyperparameter set\" + str(params))\n",
    "    return est\n",
    "\n",
    "#predict on test values\n",
    "def _doPredictBoostedTreeRegressor(X, reg):\n",
    "    origShape = X.shape\n",
    "    test_input_fn = make_input_fn_test(X)\n",
    "    outData = reg.predict(test_input_fn)\n",
    "    print(outData)\n",
    "\n",
    "    out = []\n",
    "    count = 1\n",
    "    while (count <= origShape[0]):\n",
    "        try:\n",
    "            out.append(float(next(outData)['predictions']))\n",
    "            count = count + 1\n",
    "        except:\n",
    "            break\n",
    "\n",
    "    out = np.array(out)\n",
    "\n",
    "    return out\n",
    "\n",
    "\n",
    "def rarify_data(df, df_train, df_test, target_variable, method, extr_type, thresh, coef, control_pts):\n",
    "\n",
    "    # get df_train and df_test\n",
    "    # param df_train: the training data frame\n",
    "    # param df_test: the testing data frame\n",
    "    # param target_variable: name of the target variable column\n",
    "    # return: df_train and df_test with equal class distribution between classes: rare and not rare\n",
    "\n",
    "    print(\"checking null values in dataset when applying rarify\")\n",
    "    print(df.isnull().values.any())\n",
    "\n",
    "    # get y, reset the index to avoid falsy retrievals by index later on\n",
    "    y = df[target_variable].reset_index(drop=True)\n",
    "    # get the indices of the rare values in the combined data frame\n",
    "    # note that the relevance returned is the relevance of the whole data frame not just the training\n",
    "    rare_values, phi_params, loss_params, yrel = get_rare(y, method, extr_type,thresh, coef, control_pts)\n",
    "\n",
    "    # dictionary mapping each value to its relevance\n",
    "    demandrel = {}\n",
    "    relvals = np.array(yrel)\n",
    "\n",
    "    for i, e in enumerate(y):\n",
    "        if e not in demandrel:\n",
    "            rel = relvals[i]\n",
    "            demandrel[e] = rel\n",
    "\n",
    "    # now we have the indices of the rare values, get their percentage\n",
    "\n",
    "    # percentage of rare values in the whole dataset\n",
    "    prare = len(rare_values)/len(df)\n",
    "    print('percentage of rare values in dataset before smogn: ' + str(prare*100) , file=open(output_path +\"rare_perc_results.txt\", \"a\"))\n",
    "    # number of rare values in the whole dataset\n",
    "    numrare = len(rare_values)\n",
    "    print('number of rare values in dataset before smogn: {}/{}'.format(numrare, len(df)), file=open(output_path +\"rare_perc_results.txt\", \"a\"))\n",
    "\n",
    "    # number of rare values that are be in each of the train and test\n",
    "    numraretrain = int(round(prare * len(df_train)))\n",
    "    numraretest = int(round(prare * len(df_test)))\n",
    "\n",
    "    print('number of rare in train: {}/{}'.format(numraretrain, len(df_train)))\n",
    "    print('==> {}%%'.format((numraretrain/len(df_train))*100))\n",
    "    print('number of rare in test: {}/{}'.format(numraretest, len(df_test)))\n",
    "    print('==> {}%%'.format((numraretest / len(df_test))*100))\n",
    "\n",
    "    rare_values = sorted(rare_values)\n",
    "\n",
    "    # rare indices partitioned for each of the train and test\n",
    "    rtrain = rare_values[:numraretrain]\n",
    "    rtest = rare_values[numraretrain:]\n",
    "\n",
    "    # get the relevance of each of the  dftrain and dftest\n",
    "    yreltrain = [demandrel[d] for d in df_train[target_variable]]\n",
    "    yreltest = [demandrel[d] for d in df_test[target_variable]]\n",
    "\n",
    "    if len(rtrain) != numraretrain:\n",
    "        raise ValueError('Incompatibility between the number of rare values that must be included in the '\n",
    "                         'training data for equal class distribution and the obtained number of rare')\n",
    "\n",
    "    if len(rtest) != numraretest:\n",
    "        raise ValueError('Incompatibility between the number of rare values that must be included in the '\n",
    "                         'testing data for equal class distribution and the obtained number of rare')\n",
    "\n",
    "    return rtrain, rtest, yreltrain, yreltest, phi_params['control.pts'], loss_params, demandrel\n",
    "\n",
    "#required error metrics\n",
    "def error_metrics(y_test, y_pred, ncols):\n",
    "    r2score, mase, rms, mse, re, pearson, pval, mae, adjusted_r2, spearman_corr, distance_corr, mape = calculate_errors(y_test, y_pred, ncols)\n",
    "    print(\"The range for the output variable is:\" + str(y_test.mean()))\n",
    "    print(\"r2score : \" + str(r2score))\n",
    "    print(\"adj r2score : \" + str(adjusted_r2))\n",
    "    print(\"mae : \" + str(mase))\n",
    "    print(\"rmse : \" + str(rms))\n",
    "    print(\"mse : \" + str(mse))\n",
    "    print(\"re : \" + str(re))\n",
    "    print(\"pearson : \" + str(pearson))\n",
    "    print(\"spearman : \" + str(spearman_corr))\n",
    "    print(\"distance : \" + str(distance_corr))\n",
    "    print(\"mape : \" + str(mape))\n",
    "\n",
    "#evaluate ub error metrics\n",
    "def evaluate(df, actual, predicted, thresh, rel_method='extremes', extr_type='high', coef=1.5, relevance_pts=None):\n",
    "    y = np.array(df[actual])\n",
    "    phi_params, loss_params, _ = get_phi_loss_params(y, rel_method, extr_type, coef, relevance_pts)\n",
    "\n",
    "    nb_columns = len(list(df.columns.values)) - 1\n",
    "\n",
    "    accuracy, aic, bic, nmi, mape_score,distance_corr,spearman_corr,pearson_corr,mae_score,mse_score,rmse_score,adjusted_r2,r2_Score,f1,f2,f5,prec,recall = get_stats(df[actual], df[predicted], nb_columns, thresh, phi_params, loss_params)\n",
    "    return accuracy, aic, bic, nmi, mape_score,distance_corr,spearman_corr,pearson_corr,mae_score,mse_score,rmse_score,adjusted_r2,r2_Score,f1,f2,f5,prec,recall\n",
    "\n",
    "\n",
    "\n",
    "def get_phi_loss_params(y, rel_method, extr_type='high', coef=1.5, relevance_pts=None):\n",
    "\n",
    "    # get the parameters of the relevance function\n",
    "    # param df: dataframe being used\n",
    "    # param target_variable: name of the target variable\n",
    "    # param rel_method: either 'extremes' or 'range'\n",
    "    # param extr_type: either 'high', 'low', or 'both' (defualt)\n",
    "    # param coef: default: 1.5\n",
    "    # param relevance_pts: the relevance matrix in case rel_method = 'range'\n",
    "    # return: phi parameters and loss parameters\n",
    "\n",
    "\n",
    "    if relevance_pts is None:\n",
    "        print('Will not use relevance matrix')\n",
    "        params = runit.get_relevance_params_extremes(y, rel_method=rel_method, extr_type=extr_type, coef=coef)\n",
    "    else:\n",
    "        print('Using supplied relevance matrix')\n",
    "        params = runit.get_relevance_params_range(y, rel_method=rel_method, extr_type=extr_type, coef=coef,\n",
    "                                                  relevance_pts=relevance_pts)\n",
    "\n",
    "    # phi params and loss params\n",
    "    phi_params = params[0]\n",
    "    loss_params = params[1]\n",
    "    relevance_values = params[2]\n",
    "\n",
    "    phi_params = dict(zip(phi_params.names, list(phi_params)))\n",
    "    loss_params = dict(zip(loss_params.names, list(loss_params)))\n",
    "\n",
    "    return phi_params, loss_params, relevance_values\n",
    "\n",
    "\n",
    "def get_stats(y_test, y_pred, nb_columns, thr_rel, phi_params, loss_params):\n",
    "\n",
    "# Function to compute regression error metrics between actual and predicted values +\n",
    "# correlation between both using different methods: Pearson, Spearman, and Distance\n",
    "# param y_test: the actual values. Example df['actual'] (the string inside is the name\n",
    "# of the actual column. Example: df['LE (mm)'], df['demand'], etc.)\n",
    "# param y_pred: the predicted vlaues. Example df['predicted']\n",
    "# param nb_columns: number of columns <<discarding the target variable column>>\n",
    "# return: R2, Adj-R2, RMSE, MSE, MAE, MAPE\n",
    "\n",
    "    def mean_absolute_percentage_error(y_true, y_pred):\n",
    "        y_true, y_pred = np.array(y_true), np.array(y_pred)\n",
    "        return np.mean(np.abs((y_true - y_pred) / y_true)) * 100\n",
    "\n",
    "    if not isinstance(y_test, list):\n",
    "        y_test = list(y_test)\n",
    "    if not isinstance(y_pred, list):\n",
    "        y_pred = list(y_pred)\n",
    "\n",
    "    n = len(y_test)\n",
    "\n",
    "    r2_Score = r2_score(y_test, y_pred)  # r-squared\n",
    "    adjusted_r2 = 1 - ((1 - r2_Score) * (n - 1)) / (n - nb_columns - 1)  # adjusted r-squared\n",
    "    rmse_score = np.sqrt(mean_squared_error(y_test, y_pred))  # RMSE\n",
    "    mse_score = mean_squared_error(y_test, y_pred)  # MSE\n",
    "    mae_score = mean_absolute_error(y_test, y_pred)  # MAE\n",
    "    #print(np.asarray(np.abs(( np.array(y_test) - np.array(y_pred)) / np.array(y_test)), dtype=np.float64))\n",
    "    mape_score = np.asarray(np.abs(( np.array(y_test) - np.array(y_pred)) / np.array(y_test)), dtype=np.float64).mean() * 100  # MAPE\n",
    "    accuracy = 100 - mape_score\n",
    "    aic = len(y_test) * np.log(mse_score)\n",
    "    bic = len(y_test) * np.log(mse_score)\n",
    "    nmi = normalized_mutual_info_score(y_test, y_pred)\n",
    "\n",
    "    trues = np.array(y_test)\n",
    "    preds = np.array(y_pred)\n",
    "\n",
    "    method = phi_params['method']\n",
    "    npts = phi_params['npts']\n",
    "    controlpts = phi_params['control.pts']\n",
    "    ymin = loss_params['ymin']\n",
    "    ymax = loss_params['ymax']\n",
    "    tloss = loss_params['tloss']\n",
    "    epsilon = loss_params['epsilon']\n",
    "\n",
    "    rmetrics = runit.eval_stats(trues, preds, thr_rel, method, npts, controlpts, ymin, ymax, tloss, epsilon)\n",
    "\n",
    "    # create a dictionary of the r metrics extracted above\n",
    "    rmetrics_dict = dict(zip(rmetrics.names, list(rmetrics)))\n",
    "\n",
    "    if isinstance(y_pred[0], np.ndarray):\n",
    "        y_pred_new = [x[0] for x in y_pred]\n",
    "        y_pred = y_pred_new\n",
    "    \n",
    "    pearson_corr, _ = pearsonr(y_test, y_pred)\n",
    "    spearman_corr, _ = spearmanr(y_test, y_pred)\n",
    "    distance_corr = distance.correlation(y_test, y_pred)\n",
    "\n",
    "    print('\\nUtility Based Metrics')\n",
    "    print('F1: %.5f' % rmetrics_dict['ubaF1'][0])\n",
    "    print('F2: %.5f' % rmetrics_dict['ubaF2'][0])\n",
    "    print('F05: %.5f' % rmetrics_dict['ubaF05'][0])\n",
    "    print('precision: %.5f' % rmetrics_dict['ubaprec'][0])\n",
    "    print('recall: %.5f' % rmetrics_dict['ubarec'][0])\n",
    "\n",
    "    print('\\nRegression Error Metrics')\n",
    "    print('R2: %.5f' % r2_Score)\n",
    "    print('Adj-R2: %.5f' % adjusted_r2)\n",
    "    print('RMSE: %.5f' % rmse_score)\n",
    "    print('MSE: %.5f' % mse_score)\n",
    "    print('MAE: %.5f' % mae_score)\n",
    "    print('MAPE: %.5f' % mape_score)\n",
    "    print('Accuracy: %.5f' % accuracy)\n",
    "    print('aic: %.5f' % aic)\n",
    "    print('bic: %.5f' % bic)\n",
    "    print('nmi: %.5f' % nmi)\n",
    "\n",
    "    print('\\nCorrelations')\n",
    "    print('Pearson: %.5f' % pearson_corr)\n",
    "    print('Spearman: %.5f' % spearman_corr)\n",
    "    print('Distance: %.5f' % distance_corr)\n",
    "    return accuracy, aic, bic, nmi, mape_score,distance_corr,spearman_corr,pearson_corr,mae_score,mse_score,rmse_score,adjusted_r2,r2_Score,rmetrics_dict['ubaF1'][0],rmetrics_dict['ubaF2'][0],rmetrics_dict['ubaF05'][0],rmetrics_dict['ubaprec'][0],rmetrics_dict['ubarec'][0]\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "def calculate_avg_error_metrics(mape_folds,  acc_folds, aic_folds, bic_folds, nmi_folds, d_f,sp_f, p_f, mae_f,mse_f, rmse_f, ar2_f, r2_f, f1_f, f2_f, f5_f,prec_f,recall_f,folds):\n",
    "    print('\\nUtility Based Metrics Across All', file=open(output_path +\"output_CV_results.txt\", \"a\"))\n",
    "    avg_f1 = f1_f / folds\n",
    "    print('F1: ' , avg_f1, file=open(output_path +\"output_CV_results.txt\", \"a\"))\n",
    "    avg_f2 =  f2_f / folds\n",
    "    print('F2: ' , avg_f2, file=open(output_path +\"output_CV_results.txt\", \"a\") )\n",
    "    avg_f5 = f5_f / folds\n",
    "    print('F05:' ,  avg_f5, file=open(output_path +\"output_CV_results.txt\", \"a\"))\n",
    "    avg_prec = prec_f / folds\n",
    "    print('precision: ', avg_prec  , file=open(output_path +\"output_CV_results.txt\", \"a\"))\n",
    "    avg_recall = recall_f / folds\n",
    "    print('recall:' , avg_recall , file=open(output_path +\"output_CV_results.txt\", \"a\"))\n",
    "\n",
    "    print('\\nRegression Error Metrics Across All', file=open(output_path +\"output_CV_results.txt\", \"a\"))\n",
    "    avg_r2 = r2_f/folds\n",
    "    print('R2:' , avg_r2, file=open(output_path +\"output_CV_results.txt\", \"a\"))\n",
    "    avg_ar2 =  ar2_f/folds\n",
    "    print('Adj-R2:' , avg_ar2, file=open(output_path +\"output_CV_results.txt\", \"a\"))\n",
    "    avg_rmse = rmse_f/folds\n",
    "    print('RMSE:' , avg_rmse , file=open(output_path +\"output_CV_results.txt\", \"a\"))\n",
    "    avg_mse = mse_f/folds\n",
    "    print('MSE:' , avg_mse , file=open(output_path +\"output_CV_results.txt\", \"a\"))\n",
    "    avg_mae =  mae_f/folds\n",
    "    print('MAE:' , avg_mae, file=open(output_path +\"output_CV_results.txt\", \"a\"))\n",
    "    avg_mape = mape_folds/folds\n",
    "    print('MAPE:' , avg_mape , file=open(output_path +\"output_CV_results.txt\", \"a\"))\n",
    "    avg_acc = acc_folds/folds\n",
    "    print('Accuracy:' , avg_acc , file=open(output_path +\"output_CV_results.txt\", \"a\"))\n",
    "    avg_aic = aic_folds/folds\n",
    "    print('AIC:' , avg_aic , file=open(output_path +\"output_CV_results.txt\", \"a\"))\n",
    "    avg_bic = bic_folds/folds\n",
    "    print('BIC:' , avg_bic , file=open(output_path +\"output_CV_results.txt\", \"a\"))\n",
    "    avg_nmi = nmi_folds/folds\n",
    "    print('NMI:' , avg_nmi , file=open(output_path +\"output_CV_results.txt\", \"a\"))\n",
    "\n",
    "\n",
    "    print('\\nCorrelations Across All', file=open(output_path +\"output_CV_results.txt\", \"a\"))\n",
    "    avg_pearson =  p_f/folds\n",
    "    print('Pearson:' , avg_pearson, file=open(output_path +\"output_CV_results.txt\", \"a\"))\n",
    "    avg_spearman = sp_f/folds\n",
    "    print('Spearman:' , avg_spearman , file=open(output_path +\"output_CV_results.txt\", \"a\"))\n",
    "    avg_dist =  d_f/folds\n",
    "    print('Distance:' , avg_dist, file=open(output_path +\"output_CV_results.txt\", \"a\"))\n",
    "    return avg_f1, avg_f2, avg_f5, avg_prec, avg_recall, avg_r2, avg_ar2, avg_rmse, avg_mse, avg_mae, avg_mape, avg_acc, avg_aic, avg_bic, avg_nmi, avg_pearson, avg_spearman, avg_dist\n",
    "\n",
    "def get_relevance_oversampling(smogned, target_variable, targetrel):\n",
    "\n",
    "    # gets the relevance values of an oversampled data frame\n",
    "    # param smogned: the oversampled data frame\n",
    "    # param target_variable: name of the target variable column\n",
    "    # param targetrel: dictionary mapping each target variable value to a relevance value\n",
    "    # return: the relevance of the oversampled data frame\n",
    "\n",
    "    yrelafter = []\n",
    "    distances = []\n",
    "    for val in smogned[target_variable]:\n",
    "        if val in targetrel:\n",
    "            yrelafter.append(targetrel[val])\n",
    "        else:\n",
    "            nearest = min(sorted(list(targetrel.keys())), key=lambda x: abs(x - val))\n",
    "            distances.append(abs(nearest - val))\n",
    "            yrelafter.append(targetrel[nearest])\n",
    "\n",
    "    return yrelafter, distances\n",
    "  \n",
    "def get_formula(target_variable):\n",
    "\n",
    "    # gets the formula for passing it to R functions. Example: target_variable ~ col1 + col2 ...\n",
    "    # param target_variable: the name of the target variable\n",
    "    # return: R's formula as follows: target_variable ~ other[0] + other[1] + other[2] + other[3] + ...\n",
    "\n",
    "    formula = runit.create_formula(target_variable)\n",
    "    return formula\n",
    "    \n",
    "def apply_smogn(df_train, smogn, target_variable, phi_params, thr_rel, Cperc, k, repl, dist, p, pert, plotdensity=False ):\n",
    "\n",
    "    #method that applies SMOGN Algorithm to the current data frame\n",
    "    # print('getting back values from oversampled R data frame')\n",
    "    # print('before smogn')\n",
    "    #print(pandas2ri.py2ri(df_train).head(), \"this is py2ri\")\n",
    "    if smogn:\n",
    "        smogned = runit.WFDIBS(\n",
    "            fmla=get_formula(target_variable),\n",
    "            dat= pandas2ri.py2ri(df_train),\n",
    "            #dat=df_train,\n",
    "            method=phi_params['method'][0],\n",
    "            npts=phi_params['npts'][0],\n",
    "            controlpts=phi_params['control.pts'],\n",
    "            thrrel=thr_rel,\n",
    "            Cperc=Cperc,\n",
    "            k=k,\n",
    "            repl=repl,\n",
    "            dist=dist,\n",
    "            p=p, \n",
    "            pert=pert)\n",
    "\n",
    "        # print('after smogn')\n",
    "        # print('before pandas2ri')\n",
    "         #convert the oversampled R Data.Frame back to a pandas data frame\n",
    "        smogned = pandas2ri.ri2py_dataframe(smogned)\n",
    "        # print('after pandas2ri')\n",
    "\n",
    "        if plotdensity:\n",
    "            # density plot after smooting\n",
    "            plot_density(smogned,target_variable,output_folder + 'plots/', 'density_after_smogn', 'Density Plot')\n",
    "\n",
    "        X_train = np.array(smogned.loc[:, smogned.columns != target_variable])\n",
    "        y_train = np.array(smogned.loc[:, target_variable])\n",
    "\n",
    "        return X_train, y_train\n",
    "  \n",
    "def write_to_txt(filename, content):\n",
    "  text_file = open(output_path + filename, \"w\")\n",
    "  text_file.write(content)\n",
    "  text_file.close()\n",
    "  \n",
    "def export_scores(scores, columnName):\n",
    "  file_name = output_path + \"score_sheet_final.csv\"\n",
    "  if not os.path.exists(file_name):\n",
    "      print(\"path does not exist\")\n",
    "      df = pd.DataFrame(list())\n",
    "      df.to_csv(file_name, index=False)\n",
    "  else:\n",
    "      print(\"path exists\")\n",
    "      df = pd.read_csv(file_name, delimiter=',')\n",
    "\n",
    "  df[\"Error Metrics\"] = scores.keys()\n",
    "  df[columnName] = scores.values()\n",
    "  df.to_csv(file_name, index=False)\n",
    "  return df\n",
    "  \n",
    "\n",
    "def calculate_std_error_metrics(acc_folds_2 , aic_folds_2 , bic_folds_2 , nmi_folds_2 , mape_folds_2 , dist_folds_2 ,spearman_folds_2, pearson_folds_2, mae_folds_2,mse_folds_2, rmse_folds_2, ar2_folds_2, r2_folds_2, f1_folds_2, f2_folds_2, f5_folds_2,prec_folds_2,rec_folds_2,folds):\n",
    "    print('\\nUtility Based Metrics Across All', file=open(output_path +\"output_CV_results.txt\", \"a\"))\n",
    "    std_f1 = np.std(f1_folds_2) \n",
    "    print('F1: ' , std_f1, file=open(output_path +\"output_CV_results.txt\", \"a\"))\n",
    "    std_f2 =  np.std(f2_folds_2)\n",
    "    print('F2: ' , std_f2, file=open(output_path +\"output_CV_results.txt\", \"a\") )\n",
    "    std_f5 = np.std(f5_folds_2)\n",
    "    print('F05:' ,  std_f5, file=open(output_path +\"output_CV_results.txt\", \"a\"))\n",
    "    std_prec = np.std(prec_folds_2) \n",
    "    print('precision: ', std_prec  , file=open(output_path +\"output_CV_results.txt\", \"a\"))\n",
    "    std_recall = np.std(rec_folds_2 )\n",
    "    print('recall:' , std_recall , file=open(output_path +\"output_CV_results.txt\", \"a\"))\n",
    "\n",
    "    print('\\nRegression Error Metrics Across All', file=open(output_path +\"output_CV_results.txt\", \"a\"))\n",
    "    std_r2 = np.std(r2_folds_2)\n",
    "    print('R2:' , std_r2, file=open(output_path +\"output_CV_results.txt\", \"a\"))\n",
    "    std_ar2 =  np.std(ar2_folds_2)\n",
    "    print('Adj-R2:' , std_ar2, file=open(output_path +\"output_CV_results.txt\", \"a\"))\n",
    "    std_rmse = np.std(rmse_folds_2)\n",
    "    print('RMSE:' , std_rmse , file=open(output_path +\"output_CV_results.txt\", \"a\"))\n",
    "    std_mse = np.std(mse_folds_2)\n",
    "    print('MSE:' , std_mse , file=open(output_path +\"output_CV_results.txt\", \"a\"))\n",
    "    std_mae =  np.std(mae_folds_2)\n",
    "    print('MAE:' , std_mae, file=open(output_path +\"output_CV_results.txt\", \"a\"))\n",
    "    std_mape = np.std(mape_folds_2)\n",
    "    print('MAPE:' , std_mape , file=open(output_path +\"output_CV_results.txt\", \"a\"))\n",
    "    std_acc = np.std(acc_folds_2)\n",
    "    print('Accuracy:' , std_acc , file=open(output_path +\"output_CV_results.txt\", \"a\"))\n",
    "    std_aic = np.std(aic_folds_2)\n",
    "    print('AIC:' , std_aic , file=open(output_path +\"output_CV_results.txt\", \"a\"))\n",
    "    std_bic = np.std(bic_folds_2)\n",
    "    print('BIC:' , std_bic , file=open(output_path +\"output_CV_results.txt\", \"a\"))\n",
    "    std_nmi = np.std(nmi_folds_2)\n",
    "    print('NMI:' , std_nmi , file=open(output_path +\"output_CV_results.txt\", \"a\"))\n",
    "\n",
    "\n",
    "    print('\\nCorrelations Across All', file=open(output_path +\"output_CV_results.txt\", \"a\"))\n",
    "    std_pearson =  np.nanstd(pearson_folds_2)\n",
    "    print('Pearson:' , std_pearson, file=open(output_path +\"output_CV_results.txt\", \"a\"))\n",
    "    std_spearman = np.nanstd(spearman_folds_2)\n",
    "    print('Spearman:' , std_spearman , file=open(output_path +\"output_CV_results.txt\", \"a\"))\n",
    "    std_dist =  np.std(dist_folds_2)\n",
    "    print('Distance:' , std_dist, file=open(output_path +\"output_CV_results.txt\", \"a\"))\n",
    "    return std_f1, std_f2, std_f5, std_prec, std_recall, std_r2, std_ar2, std_rmse, std_mse, std_mae, std_mape, std_acc, std_aic, std_bic, std_nmi, std_pearson, std_spearman, std_dist\n",
    "\n",
    "\n",
    "def plot_actual_vs_predicted(df, predicted_variable):\n",
    "  plt.plot(list(range(1, len(df) + 1)), df[y_test_name], color='b', label='actual')\n",
    "  plt.plot(list(range(1, len(df) + 1)), df[predicted_variable], color='r', label='predicted')\n",
    "  plt.legend(loc='best')\n",
    "  plt.suptitle('actual vs. predicted')\n",
    "  plt.savefig(output_path + 'actual_vs_predicted')\n",
    "  plt.close()\n",
    "    \n",
    "def plot_actual_vs_predicted_scatter_bisector(df, predicted_variable):\n",
    "  fig, ax = plt.subplots()\n",
    "  ax.scatter(df[y_test_name], df[predicted_variable], c='black')\n",
    "  lims = [\n",
    "      np.min([ax.get_xlim(), ax.get_ylim()]),  # min of both axes\n",
    "      np.max([ax.get_xlim(), ax.get_ylim()]),  # max of both axes\n",
    "  ]\n",
    "  ax.plot(lims, lims, 'k-', alpha=0.75, zorder=0)\n",
    "  ax.set_aspect('equal')\n",
    "  ax.set_xlim(lims)\n",
    "  ax.set_ylim(lims)\n",
    "  plt.suptitle('actual vs. predicted forecasts')\n",
    "  plt.savefig(output_path + 'actual_vs_predicted_scatter_plot')\n",
    "  plt.close()\n",
    "\n",
    "\n",
    "def plot_relevance(y, yrel, target_variable, output_folder, fig_name):\n",
    "    reldict = {}\n",
    "    y = y[target_variable]\n",
    "    for i, e in enumerate(y):\n",
    "        if e not in reldict:\n",
    "            reldict[e] = yrel[i]\n",
    "\n",
    "    reldict = dict(collections.OrderedDict(sorted(reldict.items())))\n",
    "    plt.plot(list(reldict.keys()), list(reldict.values()))\n",
    "    plt.xlabel(target_variable)\n",
    "    plt.ylabel('relevance')\n",
    "\n",
    "    plt.savefig(output_folder + fig_name)\n",
    "    plt.close()\n",
    "\n",
    "def plot_target_variable(df, df_resampled, output_column, output_folder, fig_name):\n",
    "    y = df[output_column]\n",
    "    y_resamp = df_resampled[output_column]\n",
    "    plt.plot(list(range(len(y))), sorted(y), label = \"original\")\n",
    "    plt.plot(list(range(len(y_resamp))), sorted(y_resamp), label = \"resampled\")\n",
    "    plt.xlabel('Index')\n",
    "    plt.ylabel(target_variable)\n",
    "    plt.legend()\n",
    "    plt.savefig(output_folder + fig_name)\n",
    "    plt.close()\n",
    "  \n",
    "def get_relevance():\n",
    "    ctrl = phi_params['control.pts']\n",
    "    if rel_method[0] == 'extremes' and relevance_pts[0] is None:\n",
    "        rell = np.array([\n",
    "            [ctrl[0], ctrl[1], ctrl[2]],\n",
    "            [ctrl[3], ctrl[4], ctrl[5]],\n",
    "            [ctrl[6], ctrl[7], ctrl[8]]\n",
    "        ])\n",
    "    else:\n",
    "        rell = relevance_pts[0]\n",
    "\n",
    "    return rell\n",
    "\n",
    "## Generate lags for all input features, re-generate even if some exist so that order will not be shuffled after nan dropping\n",
    "def generate_lags_for(df, column, lags_count):\n",
    "        for i in range(lags_count):\n",
    "            lag_name = column + \"-\" + str(i + 1)\n",
    "            df[lag_name] = df[column].shift(i + 1)\n",
    "        return df\n",
    "\n",
    "def generate_lags(df, lagsForColumns):\n",
    "    '''This function generates the lags for the list of columns'''\n",
    "    for k in range(len(lagsForColumns)):\n",
    "        col = lagsForColumns[k]\n",
    "        if col in df.columns:\n",
    "            df = generate_lags_for(df, col, 5)\n",
    "    return df\n",
    "\n",
    "\n",
    "def split_train_test_valid(df, TRAIN_RATIO, TEST_RATIO):\n",
    "    X_train = pd.DataFrame()\n",
    "    X_test = pd.DataFrame()\n",
    "    Y_train = pd.DataFrame()\n",
    "    Y_test = pd.DataFrame()\n",
    "    unique_sites = df[\"Site\"].unique()\n",
    "    print(\"Number of sites:\", len(unique_sites))\n",
    "\n",
    "    for site in unique_sites:\n",
    "        df_site = df[df[\"Site\"] == site]\n",
    "        X = df_site\n",
    "        train_index = int(X.shape[0] * TRAIN_RATIO)\n",
    "        test_index = int(X.shape[0] * (TRAIN_RATIO + TEST_RATIO))\n",
    "\n",
    "        X_train = X_train.append(X[:train_index], ignore_index = True)\n",
    "        X_test = X_test.append(X[train_index:], ignore_index = True)\n",
    "        Y_train = Y_train.append(X[:train_index], ignore_index = True)\n",
    "        Y_test = Y_test.append(X[train_index:], ignore_index = True)\n",
    "\n",
    "    Y_train = Y_train[[output_column]]\n",
    "    Y_test = Y_test[[output_column]]\n",
    "   \n",
    "    X_train = X_train.drop([output_column], axis = 1)\n",
    "    X_test = X_test.drop([output_column], axis = 1)\n",
    "   \n",
    "    return X_train, X_test, Y_train, Y_test\n",
    "\n",
    "\n",
    "########################################################################################################################\n",
    "                                            #Establish a connection to R library\n",
    "########################################################################################################################\n",
    "rpy2.robjects.numpy2ri.activate()\n",
    "runit = robjects.r\n",
    "runit['source']('smogn.R')\n",
    "\n",
    "########################################################################################################################\n",
    "                                            #Read and Preprocess Dataset\n",
    "########################################################################################################################\n",
    "\n",
    "#read csv\n",
    "df = pd.read_csv(input_path, delimiter=',')\n",
    "\n",
    "# drop NaN values\n",
    "df.dropna(inplace=True)\n",
    "print(df)\n",
    "\n",
    "# filter sites if ameri or euro\n",
    "#df = df[df[\"Site Id\"].str.startswith('US-')]\n",
    "#df = df[~df[\"Site Id\"].str.startswith('US-')]\n",
    "\n",
    "#set output variable between 1 and 15 only\n",
    "df = df[df[output_column].between(1, 15)]\n",
    "df.rename(columns_rename, inplace=True)\n",
    "\n",
    "#drop desired columns, rename, and drop the nans\n",
    "df = df.drop(columnsToDrop, axis = 1)\n",
    "df.dropna(inplace=True)\n",
    "\n",
    "#generate lags for columns\n",
    "lagsForColumns = [\"WS\", \"RH\", \"TA\", \"Eeflux_LST\", \"Eeflux_Albedo\", \"Eeflux_NDVI\", \"SW_IN\"]\n",
    "df = generate_lags(df, lagsForColumns)\n",
    "\n",
    "#drop nan for the first 5 rows of the generated lags only 5 rows will be removed in here\n",
    "df.isnull().mean() * 10\n",
    "df.dropna(inplace=True)\n",
    "print(df.shape)\n",
    "print(\"checking null values in the whole dataset\")\n",
    "print(df.isnull().values.any())\n",
    "print(df.columns)\n",
    "#df[output_column] = df['ET_bowen_corr_mm'] / df['ETo']\n",
    "#df = df.drop(['ET_bowen_corr_mm', 'ETo'], axis=1)\n",
    "df.dropna(inplace=True)\n",
    "\n",
    "########################################################################################################################\n",
    "                                            #Train and Test\n",
    "########################################################################################################################\n",
    "#split into train and test according to special split\n",
    "X_train, X_test, y_train, y_test = split_train_test_valid(df, 0.8, 0.2)\n",
    "\n",
    "#test with Site and Date\n",
    "df_test_all = X_test\n",
    "df_test_all[y_test_name] = y_test\n",
    "df_test_all.to_csv(output_path + \"test_dataset_full.csv\")\n",
    "\n",
    "#dropping site id after filtering the sites\n",
    "columnToDrop = \"Site\"\n",
    "X_train.drop([columnToDrop, 'Date'], axis = 1, inplace=True)\n",
    "X_test.drop([columnToDrop, 'Date'], axis = 1, inplace=True)\n",
    "\n",
    "#defining the test dataset\n",
    "df_test = X_test\n",
    "df_test[y_test_name] = y_test\n",
    "\n",
    "#defining train dataset\n",
    "df_train = X_train\n",
    "df_train[output_column] = y_train\n",
    "size_original = df_train.shape\n",
    "\n",
    "#resetting indexes \n",
    "df_train.reset_index(drop=True)\n",
    "df_test.reset_index(drop=True)\n",
    "\n",
    "#checking if null values exist here\n",
    "print(\"checking null values in train\")\n",
    "print(df_train.isnull().values.any())\n",
    "print(\"checking null values in test\")\n",
    "print(df_test.isnull().values.any())\n",
    "df_train.to_csv(output_path + \"df_train_before_rarify.csv\")\n",
    "\n",
    "\n",
    "if utility_based:\n",
    "    #retrieve indexes of rare values to utilize in stratified folds\n",
    "    df.dropna(inplace=True)\n",
    "    rtrain, rtest, yreltrain, yreltest, phi_params, loss_params, targetrel = rarify_data(df, df_train, df_test, output_column, rel_method,extr_type, thr_rel,coef, relevance_pts)\n",
    "    df_train.to_csv(output_path + \"df_train_after_rarify.csv\")\n",
    "    #plotting relevance values in both train and test target variables\n",
    "    yreltrain = np.array(yreltrain).reshape(len(yreltrain), 1)\n",
    "    plot_relevance(y_train, yreltrain, output_column, output_path, \"relevance_values_train_data\")\n",
    "    plot_relevance(y_test, yreltest, output_column, output_path, \"relevance_values_test_data\")\n",
    "    df_rel = df_train \n",
    "    df_rel[\"y_rel\"] = yreltrain\n",
    "    df_rel.to_csv(output_path+\"yrel_df.csv\")\n",
    "    X_train = X_train.drop([\"y_rel\"], axis=1)\n",
    "\n",
    "#Ensuring target variable column is dropped\n",
    "X_train = X_train.drop([output_column], axis=1)\n",
    "X_test = X_test.drop([output_column], axis=1)\n",
    "\n",
    "#showing columns used in training and size of X_train\n",
    "cols = X_train.columns\n",
    "print(\"cols in X train after rarify are: \")\n",
    "print(X_train.columns)\n",
    "print(\"size of Xtrain is : \")\n",
    "print(len(X_train))\n",
    "\n",
    "\n",
    "########################################################################################################################\n",
    "                                        #Scaling\n",
    "########################################################################################################################\n",
    "\n",
    "#if scaling: \n",
    "# if automatic:\n",
    "#   #type desired col names in X to be scaled\n",
    "#   all_columns = ['WS', 'WS-1', 'WS-2', 'WS-3', 'WS-4',\n",
    "#      'WS-5', 'RH', 'RH-1', 'RH-2', 'RH-3', 'RH-4', 'RH-5', 'TA', 'TA-1',\n",
    "#      'TA-2', 'TA-3', 'TA-4', 'TA-5', 'LE_bowen_corr_mm',\n",
    "#      'Eeflux_LST', 'Eeflux_LST-1', 'Eeflux_LST-2', 'Eeflux_LST-3',\n",
    "#      'Eeflux_LST-4', 'Eeflux_LST-5', 'Eeflux_NDVI', 'Eeflux_NDVI-1',\n",
    "#      'Eeflux_NDVI-2', 'Eeflux_NDVI-3', 'Eeflux_NDVI-4', 'Eeflux_NDVI-5',\n",
    "#      'Eeflux_Albedo', 'Eeflux_Albedo_1', 'Eeflux_Albedo-2',\n",
    "#      'Eeflux_Albedo-3', 'Eeflux_Albedo-4', 'Eeflux_Albedo-5']\n",
    "#   #standardize dataset\n",
    "#   X_train = apply_scaling(X_train, all_columns, X_train)\n",
    "#   X_test = apply_scaling(X_test, all_columns, X_train)\n",
    "# else: \n",
    "#   df_scaled_manual = apply_scaling_manual(df, all_columns, X_train, scaling)\n",
    "\n",
    "#all_columns = ['WS', 'WS-1', 'WS-2', 'WS-3', 'WS-4',\n",
    "#  'WS-5', 'RH', 'RH-1', 'RH-2', 'RH-3', 'RH-4', 'RH-5', 'TA', 'TA-1',\n",
    "#  'TA-2', 'TA-3', 'TA-4', 'TA-5', 'SW_IN', 'LE_bowen_corr_mm',\n",
    "#  'Eeflux_LST', 'Eeflux_LST-1', 'Eeflux_LST-2', 'Eeflux_LST-3',\n",
    "#  'Eeflux_LST-4', 'Eeflux_LST-5', 'Eeflux_NDVI', 'Eeflux_NDVI-1',\n",
    "#  'Eeflux_NDVI-2', 'Eeflux_NDVI-3', 'Eeflux_NDVI-4', 'Eeflux_NDVI-5',\n",
    "#  'Eeflux_Albedo', 'Eeflux_Albedo_1', 'Eeflux_Albedo-2',\n",
    "#  'Eeflux_Albedo-3', 'Eeflux_Albedo-4', 'Eeflux_Albedo-5', 'SW_IN-1',\n",
    "#  'SW_IN-2', 'SW_IN-3', 'SW_IN-4', 'SW_IN-5']\n",
    "#X_train = Min_Max_auto(X_train, X_train, all_columns)\n",
    "#X_test =  Min_Max_auto(X_train, X_test, all_columns)\n",
    "#print(X_train)\n",
    "    \n",
    "# y_label = get_rare_idex(X_train,y_train,rtrain)\n",
    "# y_train_label = np.array(y_train)\n",
    "# for i in range(20):\n",
    "#     if y_label[i] == 1:\n",
    "#         print(y_label[i])\n",
    "#         print(y_train_label[i])\n",
    "\n",
    "\n",
    "\n",
    "########################################################################################################################\n",
    "                                        #Random Search\n",
    "########################################################################################################################\n",
    "# define set of hyper-parameters\n",
    "# params = {\n",
    "# 'n_trees': [50, 150, 200, 250, 300, 350],\n",
    "# 'max_depth': [1, 3, 5, 7, 9],\n",
    "# 'learning_rate' :  [0.0001, 0.001, 0.01, 0.1, 0.2, 0.3],\n",
    "# 'l1_regularization': [0, 0.001, 0.01, 0.1, 0.2, 0.3],\n",
    "# 'l2_regularization' : [0, 0.001,  0.01, 0.1, 0.2, 0.3]\n",
    "# }\n",
    "\n",
    "# # set of hyper-parameters but with tree complexity and pruning\n",
    "# params_with_complexity = {\n",
    "# 'n_trees': [50, 150, 200, 250, 300],\n",
    "# 'max_depth': [1, 3, 5, 7, 9],\n",
    "# 'learning_rate' :  [0.0001, 0.001, 0.01, 0.1, 0.2, 0.3],\n",
    "# 'l1_regularization': [0.01, 0.1, 0.2, 0.3], \n",
    "# 'l2_regularization' : [0.01, 0.1, 0.2, 0.3],\n",
    "# 'tree_complexity' : [1,2,3],\n",
    "# 'pruning_mode' : ['pre', 'post']\n",
    "# }\n",
    "\n",
    "if random_search:\n",
    "    # do random search\n",
    "    rng = np.random.RandomState(0)\n",
    "    #specify random parameter number\n",
    "\n",
    "    # getting list of hyper-parameters\n",
    "    param_list = list(ParameterSampler(params_with_complexity, n_iter=n_params, random_state=rng))\n",
    "\n",
    "    mape_list = []\n",
    "    #looping over hyper-parameters\n",
    "    for param in param_list:\n",
    "        #fitting regressors\n",
    "        start = time.time()\n",
    "        reg = _doFitBoostedTreeRegressor(X_train, y_train, X_train.columns, param)\n",
    "        y_pred = _doPredictBoostedTreeRegressor(X_test, reg)\n",
    "        end = time.time()\n",
    "        print(\"The time taken to train and predict is \" + str(end - start) + \" seconds\")\n",
    "        df_test[y_test_name_pred] = y_pred\n",
    "        if utility_based:\n",
    "            mape = evaluate(df_test, actual=y_test_name, predicted=y_test_name_pred, thresh=0.8, rel_method='extremes', extr_type='high',coef=1.5, relevance_pts=None)\n",
    "            mape_list.append(mape)\n",
    "        else:\n",
    "            error_metrics(y_test, y_pred)\n",
    "\n",
    "    print(\"The best mape is \" + str(min(mape_list)))\n",
    "    index = mape_list.index(min(mape_list))\n",
    "    print(\"The best hyper-params are \" + str(param_list[index]))\n",
    "\n",
    "########################################################################################################################\n",
    "                                          #Grid Creation\n",
    "########################################################################################################################\n",
    "\n",
    "#The best hyper-params in random search were the following:\n",
    "#params_best = {'n_trees': 100, 'max_depth': 5, 'learning_rate': 0.1, 'l1_regularization': 0, 'l2_regularization': 0}\n",
    "\n",
    "#we shall do gridsearch around these hyper-params:\n",
    "params_grid = {\n",
    "'n_trees': [100],\n",
    "'max_depth': [7,6],\n",
    "'learning_rate' :  [0.1]\n",
    "}\n",
    "\n",
    "#create dict of hyper-params\n",
    "if grid_search:\n",
    "    grid = get_param_grid(params_grid)\n",
    "    print(\"We will be trying \" + str(len(grid)) +  \" hyper-params\" )\n",
    "\n",
    "########################################################################################################################\n",
    "                                        #Grid Search + CV\n",
    "########################################################################################################################\n",
    "if grid_search:\n",
    "    if train_batches:\n",
    "        mape_all, f1_all,f2_all,f5_all,prec_all,rec_all,r2_all,ar2_all,rmse_all,mse_all,mae_all,acc_all,aic_all,bic_all,nmi_all,pearson_all,spearman_all,dist_all = ([] for i in range(18))\n",
    "        n_params = batch_size / (repetitions * folds)\n",
    "        total_iter = len(grid) * folds * repetitions\n",
    "        grid_start = ( batch_num - 1 )* n_params\n",
    "        grid_end = batch_num * n_params\n",
    "        grid_needed = grid[int(grid_start):int(grid_end)]\n",
    "    else:\n",
    "        mape_all, f1_all,f2_all,f5_all,prec_all,rec_all,r2_all,ar2_all,rmse_all,mse_all,mae_all,acc_all,aic_all,bic_all,nmi_all,pearson_all,spearman_all,dist_all = ([] for i in range(18))\n",
    "        grid_needed = grid\n",
    "\n",
    "    for param in grid_needed:\n",
    "        acc_rep=aic_rep=bic_rep=nmi_rep=mape_rep= d_rep=sp_rep= p_rep= mae_rep=mse_rep= rmse_rep= ar2_rep= r2_rep= f1_rep= f2_rep= f5_rep=prec_rep=recall_rep=0\n",
    "        for rep in range(repetitions):\n",
    "            fold_indx = get_fold_indices(X_train,y_train,folds,rtrain)\n",
    "            print(\"Calculated stratified fold indices and will start training\")\n",
    "            acc_folds = aic_folds = bic_folds = nmi_folds = mape_folds = d_f=sp_f= p_f= mae_f=mse_f= rmse_f= ar2_f= r2_f= f1_f= f2_f= f5_f=prec_f=recall_f = 0\n",
    "            acc_folds_2 , aic_folds_2 , bic_folds_2 , nmi_folds_2 , mape_folds_2 , dist_folds_2 ,spearman_folds_2, pearson_folds_2, mae_folds_2,mse_folds_2, rmse_folds_2, ar2_folds_2, r2_folds_2, f1_folds_2, f2_folds_2, f5_folds_2,prec_folds_2,rec_folds_2 = ([] for i in range(18)) \n",
    "            \n",
    "            for fold in range(folds):\n",
    "                print( \" *************************Results for FOLD number \" + str(fold) + \"***************************** \" )\n",
    "                print(\"Columns used in X_train in CV are: \")  \n",
    "                print(X_train.columns)\n",
    "                accuracy, aic, bic, nmi, mape_score,distance_corr,spearman_corr,pearson_corr,mae_score,mse_score,rmse_score,adjusted_r2,r2_Score,f1,f2,f5,prec,recall = model_fit_predict_CV(X_train,y_train,fold_indx[fold], param)\n",
    "                acc_folds += accuracy \n",
    "                aic_folds += aic\n",
    "                bic_folds += bic\n",
    "                nmi_folds += nmi\n",
    "                mape_folds += mape_score\n",
    "                d_f += distance_corr\n",
    "                sp_f += spearman_corr\n",
    "                p_f += pearson_corr\n",
    "                mae_f += mae_score\n",
    "                mse_f += mse_score\n",
    "                rmse_f += rmse_score\n",
    "                ar2_f += adjusted_r2\n",
    "                r2_f += r2_Score\n",
    "                f1_f += f1\n",
    "                f2_f += f2 \n",
    "                f5_f += f5 \n",
    "                prec_f += prec\n",
    "                recall_f += recall \n",
    "                \n",
    "                acc_folds_2.append(accuracy)\n",
    "                aic_folds_2.append(aic)\n",
    "                bic_folds_2.append(bic)\n",
    "                nmi_folds_2.append(nmi)\n",
    "                mape_folds_2.append(mape_score)\n",
    "                dist_folds_2.append(distance_corr)\n",
    "                spearman_folds_2.append(spearman_corr)\n",
    "                pearson_folds_2.append(pearson_corr)\n",
    "                mae_folds_2.append(mae_score)\n",
    "                mse_folds_2.append(mse_score)\n",
    "                rmse_folds_2.append(rmse_score)\n",
    "                ar2_folds_2.append(adjusted_r2)\n",
    "                r2_folds_2.append(r2_Score)\n",
    "                f1_folds_2.append(f1)\n",
    "                f2_folds_2.append(f2)\n",
    "                f5_folds_2.append(f5)\n",
    "                prec_folds_2.append(prec)\n",
    "                rec_folds_2.append(recall)\n",
    "                \n",
    "            print(\"For param \" + str(param) , file=open(output_path +\"output_CV_results.txt\", \"a\") )\n",
    "            print( \" *************************FOLDS Average***************************** \", file=open(output_path +\"output_CV_results.txt\", \"a\") )\n",
    "            avg_f1, avg_f2, avg_f5, avg_prec, avg_recall, avg_r2, avg_ar2, avg_rmse, avg_mse, avg_mae, avg_mape, avg_accuracy, avg_aic, avg_bic, avg_nmi, avg_pearson, avg_spearman, avg_dist = calculate_avg_error_metrics(mape_folds, acc_folds, aic_folds, bic_folds, nmi_folds,  d_f,sp_f, p_f, mae_f,mse_f, rmse_f, ar2_f, r2_f, f1_f, f2_f, f5_f,prec_f,recall_f,folds)\n",
    "                    \n",
    "            acc_rep += avg_accuracy \n",
    "            aic_rep += avg_aic\n",
    "            bic_rep += avg_bic\n",
    "            nmi_rep += avg_nmi\n",
    "            mape_rep += avg_mape\n",
    "            d_rep += avg_dist\n",
    "            sp_rep += avg_spearman\n",
    "            p_rep += avg_pearson\n",
    "            mae_rep += avg_mae\n",
    "            mse_rep += avg_mse\n",
    "            rmse_rep += avg_rmse\n",
    "            ar2_rep += avg_ar2\n",
    "            r2_rep += avg_r2\n",
    "            f1_rep += avg_f1\n",
    "            f2_rep += avg_f2 \n",
    "            f5_rep += avg_f5 \n",
    "            prec_rep += avg_prec\n",
    "            recall_rep += avg_recall\n",
    "        \n",
    "        print( \"************************REPETITIONS Average*******************************\", file=open(output_path +\"output_CV_results.txt\", \"a\") )\n",
    "        avg_f1, avg_f2, avg_f5, avg_prec, avg_recall, avg_r2, avg_ar2, avg_rmse, avg_mse, avg_mae, avg_mape, avg_accuracy, avg_aic, avg_bic, avg_nmi, avg_pearson, avg_spearman, avg_dist = calculate_avg_error_metrics(mape_rep,acc_rep, aic_rep, bic_rep, nmi_rep, d_rep,sp_rep, p_rep, mae_rep,mse_rep, rmse_rep,ar2_rep, r2_rep, f1_rep, f2_rep, f5_rep,prec_rep,recall_rep, repetitions)\n",
    "        std_f1, std_f2, std_f5, std_prec, std_recall, std_r2, std_ar2, std_rmse, std_mse, std_mae, std_mape, std_acc, std_aic, std_bic, std_nmi, std_pearson, std_spearman, std_dist = calculate_std_error_metrics(acc_folds_2 , aic_folds_2 , bic_folds_2 , nmi_folds_2 , mape_folds_2 , dist_folds_2 ,spearman_folds_2, pearson_folds_2, mae_folds_2,mse_folds_2, rmse_folds_2, ar2_folds_2, r2_folds_2, f1_folds_2, f2_folds_2, f5_folds_2,prec_folds_2,rec_folds_2,folds)       \n",
    "                   \n",
    "        print(\"average mape\", avg_mape)\n",
    "        mape_all.append(avg_mape)\n",
    "        f1_all.append(avg_f1)\n",
    "        f2_all.append(avg_f2)\n",
    "        f5_all.append(avg_f5)\n",
    "        prec_all.append(avg_prec)\n",
    "        rec_all.append(avg_recall)\n",
    "        r2_all.append(avg_r2)\n",
    "        ar2_all.append(avg_ar2)\n",
    "        rmse_all.append(avg_rmse)\n",
    "        mse_all.append(avg_mse)\n",
    "        mae_all.append(avg_mae)\n",
    "        acc_all.append(avg_accuracy)\n",
    "        aic_all.append(avg_aic)\n",
    "        bic_all.append(avg_bic)\n",
    "        nmi_all.append(avg_nmi)\n",
    "        pearson_all.append(avg_pearson)\n",
    "        spearman_all.append(avg_spearman)\n",
    "        dist_all.append(avg_dist)\n",
    "        \n",
    "        scores_dict = OrderedDict()\n",
    "        scores_dict[\"mean target\"] = average_target_variable  \n",
    "        scores_dict[\"F1\"] = std_f1\n",
    "        scores_dict[\"F2\"] =  std_f2\n",
    "        scores_dict[\"F05\"] = std_f5\n",
    "        scores_dict[\"Precision\"] = std_prec\n",
    "        scores_dict[\"Recall\"] = std_recall\n",
    "        scores_dict[\"R2\"] = std_r2\n",
    "        scores_dict[\"Adjusted R2\"] = std_ar2\n",
    "        scores_dict[\"RMSE\"] = std_rmse\n",
    "        scores_dict[\"MSE\"] = std_mse\n",
    "        scores_dict[\"MAE\"] = std_mae\n",
    "        scores_dict[\"MAPE\"] = std_mape\n",
    "        scores_dict[\"Accuracy\"] = std_acc\n",
    "        scores_dict[\"Pearson C.C.\"] = std_pearson\n",
    "        scores_dict[\"Spearman C.C.\"] = std_spearman\n",
    "        scores_dict[\"Spatial Distance\"] =  std_dist\n",
    "        scores_dict[\"NMI\"] = std_nmi\n",
    "        scores_dict[\"AIC\"] = std_aic\n",
    "        scores_dict[\"BIC\"] = std_bic\n",
    "        scores_dict[\"Data Size train\"] = '-'\n",
    "        scores_dict[\"Data Size test\"] = '-'\n",
    "        scores_dict[\"Training Time (seconds)\"] = '-'\n",
    "        scores_dict[\"Testing Time (seconds)\"] = '-'\n",
    "        export_scores(scores_dict, \"Validation Scores standard deviation\")\n",
    "\n",
    "        \n",
    "\n",
    "    print(\"The best mape is \" + str(min(mape_all)))\n",
    "    index = mape_all.index(min(mape_all))\n",
    "    print(index)\n",
    "    print(mape_all) \n",
    "    print(len(grid_needed))\n",
    "    print(\"The best hyper-params are \" + str(grid_needed[index]))\n",
    "\n",
    "    scores_dict = OrderedDict()\n",
    "    scores_dict[\"mean target\"] = average_target_variable  \n",
    "    scores_dict[\"F1\"] = f1_all[index]\n",
    "    scores_dict[\"F2\"] =  f2_all[index]\n",
    "    scores_dict[\"F05\"] = f5_all[index]\n",
    "    scores_dict[\"Precision\"] = prec_all[index]\n",
    "    scores_dict[\"Recall\"] = rec_all[index]\n",
    "    scores_dict[\"R2\"] = r2_all[index]\n",
    "    scores_dict[\"Adjusted R2\"] = ar2_all[index]\n",
    "    scores_dict[\"RMSE\"] = rmse_all[index]\n",
    "    scores_dict[\"MSE\"] = mse_all[index]\n",
    "    scores_dict[\"MAE\"] = mae_all[index]\n",
    "    scores_dict[\"MAPE\"] = mape_all[index]\n",
    "    scores_dict[\"Accuracy\"] = acc_all[index]\n",
    "    scores_dict[\"Pearson C.C.\"] = pearson_all[index]\n",
    "    scores_dict[\"Spearman C.C.\"] = spearman_all[index]\n",
    "    scores_dict[\"Spatial Distance\"] =  dist_all[index]\n",
    "    scores_dict[\"NMI\"] = nmi_all[index]\n",
    "    scores_dict[\"AIC\"] = aic_all[index]\n",
    "    scores_dict[\"BIC\"] = bic_all[index]\n",
    "    scores_dict[\"Data Size train\"] = '-'\n",
    "    scores_dict[\"Data Size test\"] = '-'\n",
    "    scores_dict[\"Training Time (seconds)\"] = '-'\n",
    "    scores_dict[\"Testing Time (seconds)\"] = '-'\n",
    "    export_scores(scores_dict, \"Validation Scores\")\n",
    "\n",
    "    write_to_txt('winning-hyperparams.txt', str(grid_needed[index]))\n",
    "    winning_hyper = grid_needed[index]\n",
    "\n",
    "else:\n",
    "    #winning_hyper = {'n_trees': 100, 'max_depth': 7, 'learning_rate': 0.1}\n",
    "    winning_hyper = params_grid[model_name]\n",
    "#######################################################################################################################\n",
    "                                        #Applying Smogn\n",
    "#######################################################################################################################\n",
    "\n",
    "if smogn:\n",
    "    #redefine my train data\n",
    "    df_train = X_train\n",
    "    df_train[target_variable] = y_train\n",
    "    df_train.to_csv(output_path + \"df_train_before_smogn_1.csv\")\n",
    "    #confirm if rel method matches rel points\n",
    "    if rel_method == 'range' and relevance_pts is None:\n",
    "        raise ValueError('You have set rel_method = range. You must provide relevance_pts as a matrix. Currently, it is None')\n",
    "    #get phi loss params\n",
    "    y_train_a = np.array(df_train[target_variable])\n",
    "    phi_params, loss_params, relevance_values = get_phi_loss_params(y_train_a, rel_method, extr_type, coef,relevance_pts)\n",
    "    df_train.to_csv(output_path + \"df_train_before_smogn_after_phi.csv\")\n",
    "    #apply smogn on the df_train data \n",
    "    X_train,y_train = apply_smogn(df_train, smogn, target_variable, phi_params, thr_rel, Cperc, k, repl, dist, p, pert, plotdensity=False)\n",
    "\n",
    "    #define X_train as df \n",
    "    X_train = pd.DataFrame(X_train, columns= cols)\n",
    "\n",
    "    print(\"cols in X train after smogn are :\" )\n",
    "    print(X_train.columns)\n",
    "    \n",
    "    #define a smogned df_train\n",
    "    df_train_smogned = X_train\n",
    "    df_train_smogned[output_column] = y_train\n",
    "    df_train_smogned.to_csv(output_path + 'df_train_smogned.csv')\n",
    "########################################################################################################################\n",
    "                                         #Fix one-hot encoded errors \n",
    "########################################################################################################################\n",
    "X_train.to_csv(output_path + 'X_train_b4_1_hot.csv')\n",
    "if smogn:\n",
    "  if one_hot_encoded:\n",
    "      count_abnormal(X_train)\n",
    "      print(\"fixing one hot encoded cols\")\n",
    "      X_train = round_oversampled_one_hot_encoded(X_train) \n",
    "  \n",
    "  else:\n",
    "      print(\"there are no onehot encoded cols to be accounted for\")\n",
    "\n",
    "\n",
    "########################################################################################################################\n",
    "                                         #Reporting Rarity Metrics \n",
    "########################################################################################################################\n",
    "\n",
    "if smogn:\n",
    "    print(\"The size of the original data is \" + str(size_original))\n",
    "    print(\"The size of the oversampled data is \" + str(df_train_smogned.shape))\n",
    "    yrelafter, distances = get_relevance_oversampling(df_train_smogned, output_column, targetrel)\n",
    "    roversampled = get_rare_indices(df_train_smogned[output_column], yrelafter, thr_rel, phi_params['control.pts'])\n",
    "    rare_train_after = (len(roversampled)/len(df_train_smogned)) * 100\n",
    "    print(\"The percentage of rare values in dataset after smogn are \"  + str(rare_train_after))\n",
    "\n",
    "########################################################################################################################\n",
    "                                        #Final Training\n",
    "########################################################################################################################\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "if output_column in X_train:\n",
    "    X_train = X_train.drop([output_column], axis=1)\n",
    "\n",
    "print(\"Training model on the best hyper-params \" + str(winning_hyper) )\n",
    "print( \" *************************Final Results on all Folds***************************** \" )\n",
    "print(\"Columns used in X_train in final training are: \")\n",
    "print(X_train.columns)\n",
    "\n",
    "#fitting regressor\n",
    "df_test.to_csv(output_path + 'train_final_dataset.csv')\n",
    "reg = _doFitBoostedTreeRegressor(X_train, y_train, X_train.columns, winning_hyper)\n",
    "y_pred = _doPredictBoostedTreeRegressor(X_test, reg)\n",
    "end = time.time()\n",
    "print(\"The time taken to train and predict is \" + str(end - start) + \" seconds\")\n",
    "\n",
    "#printing average target variable\n",
    "print(\"The average target variable is \" + str(y_test.mean()))\n",
    "df_test = X_test\n",
    "df_test[y_test_name] = y_test\n",
    "\n",
    "# combine y_test and y_pred in 1 dataset\n",
    "df_test[y_test_pred_name] = y_pred\n",
    "df_test.to_csv(output_path + 'test_dataset.csv')\n",
    "\n",
    "#plotting actual vs predicted\n",
    "plot_actual_vs_predicted(df_test, y_test_pred_name)\n",
    "plot_actual_vs_predicted_scatter_bisector(df_test, y_test_pred_name)\n",
    "\n",
    "#plotting target variable is smogn is used\n",
    "if smogn:\n",
    "    print(df_train.columns, df_train_smogned.columns)\n",
    "    plot_target_variable(df_train, df_train_smogned, output_column, output_path, 'target_variable')\n",
    "\n",
    "#reporting error metrics\n",
    "if utility_based:\n",
    "    accuracy, aic, bic, nmi, mape_score,distance_corr,spearman_corr,pearson_corr,mae_score,mse_score,rmse_score,adjusted_r2,r2_Score,f1,f2,f5,prec,recall = evaluate(df_test, actual=y_test_name, predicted=y_test_pred_name,\n",
    "            thresh=0.8, rel_method='extremes', extr_type='high',\n",
    "            coef=1.5, relevance_pts=None)\n",
    "else:\n",
    "    nb_columns = len(list(df_test.columns.values)) - 1\n",
    "    error_metrics(np.array(y_test), np.array(y_pred), nb_columns )\n",
    "    \n",
    "    \n",
    "scores_dict = OrderedDict()\n",
    "scores_dict[\"mean target\"] = average_target_variable  \n",
    "scores_dict[\"F1\"] = f1\n",
    "scores_dict[\"F2\"] =  f2\n",
    "scores_dict[\"F05\"] = f5\n",
    "scores_dict[\"Precision\"] = prec\n",
    "scores_dict[\"Recall\"] = recall\n",
    "scores_dict[\"R2\"] = r2_Score\n",
    "scores_dict[\"Adjusted R2\"] = adjusted_r2\n",
    "scores_dict[\"RMSE\"] = rmse_score\n",
    "scores_dict[\"MSE\"] = mse_score\n",
    "scores_dict[\"MAE\"] = mae_score\n",
    "scores_dict[\"MAPE\"] = mape_score\n",
    "scores_dict[\"Accuracy\"] = accuracy\n",
    "scores_dict[\"Pearson C.C.\"] = pearson_corr\n",
    "scores_dict[\"Spearman C.C.\"] = spearman_corr\n",
    "scores_dict[\"Spatial Distance\"] =  distance_corr\n",
    "scores_dict[\"NMI\"] = nmi\n",
    "scores_dict[\"AIC\"] = aic\n",
    "scores_dict[\"BIC\"] = bic\n",
    "scores_dict[\"Data Size train\"] = len(df_train)\n",
    "scores_dict[\"Data Size test\"] = len(df_test)\n",
    "scores_dict[\"Training Time (seconds)\"] = training_time\n",
    "scores_dict[\"Testing Time (seconds)\"] = testing_time\n",
    "export_scores(scores_dict, \"Testing Scores\")\n",
    "\n",
    "#saving error metrics in file\n",
    "with open(output_path + 'winning-model-scores.txt', 'a') as the_file:\n",
    "     the_file.write('\\nUtility Based Metrics'+'\\n')\n",
    "     the_file.write('F1: %.5f' % f1 + '\\n')\n",
    "     the_file.write('F2: %.5f' % f2+'\\n')\n",
    "     the_file.write('F05: %.5f' % f5+'\\n')\n",
    "     the_file.write('precision: %.5f' %prec+'\\n')\n",
    "     the_file.write('recall: %.5f' % recall+'\\n')\n",
    "\n",
    "     the_file.write('\\nRegression Error Metrics'+'\\n')\n",
    "     the_file.write('R2: %.5f' % r2_Score+'\\n')\n",
    "     the_file.write('Adj-R2: %.5f' % adjusted_r2+'\\n')\n",
    "     the_file.write('RMSE: %.5f' % rmse_score+'\\n')\n",
    "     the_file.write('MSE: %.5f' % mse_score+'\\n')\n",
    "     the_file.write('MAE: %.5f' % mae_score+'\\n')\n",
    "     the_file.write('MAPE: %.5f' % mape_score+'\\n')\n",
    "\n",
    "     the_file.write('\\nCorrelations'+'\\n')\n",
    "     the_file.write('Pearson: %.5f' % pearson_corr+'\\n')\n",
    "     the_file.write('Spearman: %.5f' % spearman_corr+'\\n')\n",
    "     the_file.write('Distance: %.5f' % distance_corr+'\\n')\n",
    "\n",
    "   \n",
    "   \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
