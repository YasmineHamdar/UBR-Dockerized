{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using supplied relevance matrix\n",
      "\n",
      "Utility Based Metrics\n",
      "F1: 0.79518\n",
      "F2: 0.80594\n",
      "F05: 0.78471\n",
      "precision: 0.77788\n",
      "recall: 0.81328\n",
      "\n",
      "Regression Error Metrics\n",
      "R2: -1.15716\n",
      "Adj-R2: -4.63258\n",
      "RMSE: 3.02911\n",
      "MSE: 9.17552\n",
      "MAE: 2.11303\n",
      "MAPE: 56.70533\n",
      "Accuracy: 43.29467\n",
      "aic: 210.57120\n",
      "bic: 210.57120\n",
      "nmi: 0.89147\n",
      "\n",
      "Correlations\n",
      "Pearson: 0.26641\n",
      "Spearman: 0.27604\n",
      "Distance: 0.73359\n",
      "TO EXCEL\n",
      "3.806324413789474\n",
      "0.7951821078007248\n",
      "0.8059403451553143\n",
      "0.7847073034615862\n",
      "0.7778760802231185\n",
      "0.8132756988367532\n",
      "-1.157159975250167\n",
      "-4.63258437981988\n",
      "3.0291119774417083\n",
      "9.175519371880815\n",
      "2.1130309381684205\n",
      "56.705329907124636\n",
      "43.294670092875364\n",
      "210.57120496465657\n",
      "210.57120496465657\n",
      "0.8914699840889503\n",
      "0.2664075411028962\n",
      "0.2760414205202841\n",
      "0.7335924588971039\n",
      "Using supplied relevance matrix\n",
      "\n",
      "Utility Based Metrics\n",
      "F1: 0.00001\n",
      "F2: 0.00001\n",
      "F05: 0.00001\n",
      "precision: 0.00001\n",
      "recall: 0.00001\n",
      "\n",
      "Regression Error Metrics\n",
      "R2: 0.66021\n",
      "Adj-R2: 0.11277\n",
      "RMSE: 0.45671\n",
      "MSE: 0.20858\n",
      "MAE: 0.25173\n",
      "MAPE: 29.01079\n",
      "Accuracy: 70.98921\n",
      "aic: -148.90449\n",
      "bic: -148.90449\n",
      "nmi: 1.00000\n",
      "\n",
      "Correlations\n",
      "Pearson: 0.83156\n",
      "Spearman: 0.90167\n",
      "Distance: 0.16844\n",
      "TO EXCEL\n",
      "0.8335768773579597\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "0.660209099043765\n",
      "0.11276820305871971\n",
      "0.4567094612456307\n",
      "0.20858353199127427\n",
      "0.25173149771559133\n",
      "29.01079230056248\n",
      "70.98920769943751\n",
      "-148.90449006774185\n",
      "-148.90449006774185\n",
      "1.0\n",
      "0.8315555808506709\n",
      "0.9016657334826427\n",
      "0.16844441914932917\n",
      "Using supplied relevance matrix\n",
      "\n",
      "Utility Based Metrics\n",
      "F1: 0.91811\n",
      "F2: 0.92347\n",
      "F05: 0.91281\n",
      "precision: 0.90932\n",
      "recall: 0.92708\n",
      "\n",
      "Regression Error Metrics\n",
      "R2: 0.76438\n",
      "Adj-R2: 0.38477\n",
      "RMSE: 1.38923\n",
      "MSE: 1.92996\n",
      "MAE: 1.00117\n",
      "MAPE: 142.62210\n",
      "Accuracy: -42.62210\n",
      "aic: 62.46222\n",
      "bic: 62.46222\n",
      "nmi: 1.00000\n",
      "\n",
      "Correlations\n",
      "Pearson: 0.88326\n",
      "Spearman: 0.85146\n",
      "Distance: 0.11674\n",
      "TO EXCEL\n",
      "-0.9922422440421053\n",
      "0.9181115816168569\n",
      "0.9234704436148412\n",
      "0.9128145553119111\n",
      "0.9093170310968458\n",
      "0.9270779082346718\n",
      "0.7643802049261619\n",
      "0.38477053508497816\n",
      "1.3892284839108051\n",
      "1.9299557805091143\n",
      "1.0011742371789474\n",
      "142.62209807415476\n",
      "-42.62209807415476\n",
      "62.46222364509591\n",
      "62.46222364509591\n",
      "1.0\n",
      "0.8832647966126291\n",
      "0.8514557670772677\n",
      "0.11673520338737131\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "pd.set_option('use_inf_as_na', True)\n",
    "import numpy as np\n",
    "from scipy.spatial import distance\n",
    "import rpy2.robjects as robjects\n",
    "from rpy2.robjects import pandas2ri\n",
    "import rpy2\n",
    "import rpy2.robjects.packages as rpackages\n",
    "import rpy2.robjects as ro\n",
    "pandas2ri.activate()\n",
    "import rpy2.robjects.numpy2ri\n",
    "rpy2.robjects.numpy2ri.activate()\n",
    "from rpy2.robjects.conversion import localconverter\n",
    "import warnings\n",
    "from sklearn.ensemble import BaggingRegressor\n",
    "from sklearn.ensemble import AdaBoostRegressor\n",
    "from sklearn.ensemble import ExtraTreesRegressor\n",
    "from rpy2.rinterface import RRuntimeWarning\n",
    "warnings.filterwarnings(\"ignore\", category=RRuntimeWarning)\n",
    "pandas2ri.activate()\n",
    "from matplotlib import pyplot\n",
    "from collections import Counter\n",
    "from sklearn.model_selection import StratifiedKFold, KFold\n",
    "from numpy import mean\n",
    "from numpy import std\n",
    "import xgboost as xgb\n",
    "from pandas import read_csv\n",
    "from sklearn.preprocessing import RobustScaler, OneHotEncoder, MinMaxScaler, PowerTransformer, StandardScaler\n",
    "from scipy.stats import normaltest\n",
    "from sklearn.model_selection import ParameterSampler\n",
    "from numpy.random import randn\n",
    "from sklearn.ensemble import GradientBoostingRegressor, RandomForestRegressor\n",
    "from scipy.stats import shapiro\n",
    "from sklearn.model_selection import train_test_split\n",
    "from numpy import *\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "from scipy.stats.stats import pearsonr, spearmanr\n",
    "from scipy import stats\n",
    "import tensorflow as tf\n",
    "import multiprocessing as mp\n",
    "import time\n",
    "from sklearn.metrics.cluster import normalized_mutual_info_score\n",
    "import os\n",
    "from sklearn.svm import SVR\n",
    "import collections\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import OrderedDict\n",
    "import category_encoders as ce\n",
    "import itertools as it\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)\n",
    "import re\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "#specify if you wish to apply over sampling by smogn\n",
    "smogn = False\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def get_rare(y, method, extr_type, thresh, coef, control_pts):\n",
    "\n",
    "    # we will be getting the relevance function on all the data not just the training data because\n",
    "    # when we want to apply Lime on the 'rare' testing instances, the relevance function must map all possible demand\n",
    "    # values to a certain relevance. If it happens that some demand values are present only in the testing\n",
    "    # and not in the training data, we cannot detect rare values correctly. The way we compute\n",
    "    # rare values depends on the relevance\n",
    "\n",
    "    # param y: the target variable vector\n",
    "    # param method: 'extremes' or 'range'. Default is 'extremes'\n",
    "    # param extr_type: 'both', 'high', or 'low'\n",
    "    # param thresh: threshold. Default is 0.8\n",
    "    # param coef: parameter needed for method \"extremes\" to specify how far the wiskers extend to the most extreme data point in the boxplot. The default is 1.5.\n",
    "    # param control_pts: if method == 'range', then this is the relevance matrix provided by the user. Default is None\n",
    "\n",
    "    # return the indices of the rare values in the data\n",
    "\n",
    "    yrel = get_relevance_2(y, df=None, target_variable=None, method=method, extr_type=extr_type, control_pts=control_pts)\n",
    "\n",
    "    # get the the phi.control returned parameters that are used as input for computing the relevance function phi\n",
    "    # (function provided by R UBL's package: https://www.rdocumentation.org/packages/UBL/versions/0.0.6/topics/phi)\n",
    "    # (function provided by R UBL's package\n",
    "    # https://www.rdocumentation.org/packages/UBL/versions/0.0.6/topics/phi.control)\n",
    "    # we need those returned parameters for computing rare values\n",
    "\n",
    "    print('relevance method - phi function : {}'.format(method))\n",
    "\n",
    "    if control_pts is None:\n",
    "        # without relevance matrix\n",
    "        print('control.pts - phi function: {}'.format(control_pts))\n",
    "        print('without relevance matrix')\n",
    "        params = runit.get_relevance_params_extremes(y, rel_method=method, extr_type=extr_type, coef=coef)\n",
    "    else:\n",
    "        # with relevance matrix (provided by the user)\n",
    "        print('control.pts - phi function: {}'.format(control_pts))\n",
    "        print('with relevance matrix')\n",
    "        params = runit.get_relevance_params_range(y, rel_method=method, extr_type=extr_type, coef=coef,\n",
    "                                                  relevance_pts=control_pts)\n",
    "\n",
    "    # phi params\n",
    "    phi_params = params[0]\n",
    "    loss_params = params[1]\n",
    "\n",
    "    phi_params = dict(zip(phi_params.names, list(phi_params)))\n",
    "    loss_params = dict(zip(loss_params.names, list(loss_params)))\n",
    "\n",
    "    print('\\nCONTROL PTS')\n",
    "    print(phi_params['control.pts'])\n",
    "    print(\"for the whole dataset\")\n",
    "    rare_indices = get_rare_indices(y=y, y_rel=yrel, thresh=thresh, controlpts=phi_params['control.pts'])\n",
    "    # print('rare indices are: {}'.format(rare_indices))\n",
    "\n",
    "    return rare_indices, phi_params, loss_params, yrel\n",
    "\n",
    "\n",
    "def get_relevance_2(y, df, target_variable, method, extr_type, control_pts):\n",
    "\n",
    "    # gets the relevance values of the target variable vector\n",
    "    # param y: the target variable vector\n",
    "    # param df: if y in None, this must be passed. It is the data frame of interest\n",
    "    # param target_variable: if y is None, this must be passed. It is the name of the target variable\n",
    "    # param method: 'extremes' or 'range'\n",
    "    # param extr_type: 'both', 'high', or 'low'\n",
    "    # param control_pts: if method == 'range', will be a relevance matrix provided by the user\n",
    "    # return: the relevance values of the associated target variable\n",
    "\n",
    "    # get the target variable vector y\n",
    "    if y is None:\n",
    "        if df is None or target_variable is None:\n",
    "            raise ValueError('if y is None, neither df nor target_variable must be None')\n",
    "        y = df[target_variable]\n",
    "\n",
    "    # check that the passed parameters are in order\n",
    "    if method != 'range' and method != 'extremes':\n",
    "        raise ValueError('method must be \"range\" or \"extremes\", there is no method called \"%s\"' % method)\n",
    "    elif method == 'range' and control_pts is None:\n",
    "        raise ValueError('If method == \"range\", then control_pts must not be None')\n",
    "    elif method == 'extremes' and extr_type not in ['high', 'low', 'both']:\n",
    "        raise ValueError('extr_type must wither be \"high\", \"low\", or \"both\"')\n",
    "    else:\n",
    "        if control_pts is None:\n",
    "            print('getting yrel - Control pts is {}, method is {}'.format(control_pts, method))\n",
    "            y_rel = runit.get_yrel(y=np.array(y), meth=method, extr_type=extr_type)\n",
    "        else:\n",
    "            print('getting yrel - Control pts is not None, method is {}'.format(method))\n",
    "            y_rel = runit.get_yrel(y=np.array(y), meth=method, extr_type=extr_type, control_pts=control_pts)\n",
    "\n",
    "    return y_rel\n",
    "\n",
    "\n",
    "def get_rare_indices(y, y_rel, thresh, controlpts):\n",
    "    # get the indices of the rare values in the data\n",
    "    # param y: the target variable vector\n",
    "    # param y_rel: the target variable (y) relevance vector\n",
    "    # param thresh: the threshold of interest\n",
    "    # param controlpts: the phi.control (function provided by R UBL's package: https://www.rdocumentation.org/packages/UBL/versions/0.0.6/topics/phi.control)\n",
    "    # returned parameters that are used as input for computing the relevance function phi (function provided by R UBL's package: https://www.rdocumentation.org/packages/UBL/versions/0.0.6/topics/phi)\n",
    "    # return: the indices of the rare values in 'y'\n",
    "    \n",
    "\n",
    "    # references\n",
    "    # https://github.com/paobranco/SMOGN-LIDTA17/blob/8964a2327de19f6ca9e6f7055479ca863cd6b8a0/R_Code/ExpsDIBS.R#L41\n",
    "\n",
    "    # transform controlpts returned by R into a python list\n",
    "    controlpts = list(np.array(controlpts))\n",
    "    # print(controlpts)\n",
    "\n",
    "    # boolean variable indicating whether both low and high rare exist\n",
    "    both = [controlpts[i] for i in [1, 7]] == [1, 1]\n",
    "\n",
    "    # initialize rare cases to empty list (in case there are no rare cases at all)\n",
    "    rare_cases = []\n",
    "\n",
    "    if both:\n",
    "        # bothr = True\n",
    "        print('\\nWe have both low and high extremes')\n",
    "        rare_low = [i for i, e in enumerate(y_rel) if e > thresh and y[i] < controlpts[3]]\n",
    "        rare_high = [i for i, e in enumerate(y_rel) if e > thresh and y[i] > controlpts[3]]\n",
    "\n",
    "        # merge two lists (of low rare + high rare) together\n",
    "        rare_cases = rare_low + rare_high\n",
    "\n",
    "    else:\n",
    "        print('\\nWe dont have both', end=' ')\n",
    "        if controlpts[1] == 1:\n",
    "            print('We have only low rare')\n",
    "            # lowr = True\n",
    "            rare_cases = [i for i, e in enumerate(y_rel) if e > thresh and y[i] < controlpts[3]]\n",
    "        else:\n",
    "            print('We have only high rare')\n",
    "            # highr = True\n",
    "            rare_cases = [i for i, e in enumerate(y_rel) if e > thresh and y[i] > controlpts[3]]\n",
    "\n",
    "    total = len(rare_cases)\n",
    "\n",
    "    print('Total Number of rare cases: %d out of %d' % (total, len(y)), file=open(output_path +\"smogn_stats.txt\", \"a\"))\n",
    "    print('Percentage of Rare Cases: %.2f%%\\n' % (total/len(y) * 100), file=open(output_path +\"smogn_stats.txt\", \"a\"))\n",
    "\n",
    "    return rare_cases\n",
    "\n",
    "\n",
    "def round_oversampled_one_hot_encoded(df):\n",
    "\n",
    "    # round one hot encoded vectors of an oversampled dataset. We have fed the SMOGN/SMOTER/GN/RandUnder\n",
    "    # a data frame having one hot encoded values (0s and 1s). However, given that we are using Euclidean/Manhattan\n",
    "    # distances for oversampling, some noise is added to these making them 1.0003, 0.99, etc.\n",
    "    # Having this said, this function will round these values back again so they are\n",
    "    # perfect 0s or 1s. We could have used HEOM distance, but it expects \"nominal\" features\n",
    "    # as opposed to one hot encodings.\n",
    "    # param df: the over-sampled data frame\n",
    "    # return: the over-sampled data frame with one hot encodings rounded\n",
    "\n",
    "    for col in one_hot_encoded:\n",
    "        df.loc[df[col] < 0.5, col] = 0\n",
    "        df.loc[df[col] >=0.5, col] = 1\n",
    "    return df\n",
    "\n",
    "\n",
    "def count_abnormal(df):\n",
    "\n",
    "    # Due to Oversampling, SMOGN is adding noise to the one hot encoded vectors. This function counts how many of these\n",
    "    # are being done\n",
    "    # param df: the oversampled data frame\n",
    "    # return: statistics about the above\n",
    "\n",
    "    count = 0\n",
    "    for col in one_hot_encoded:\n",
    "        for i, row in df.iterrows():\n",
    "            if row[col] not in [0, 1]:\n",
    "                count += 1\n",
    "            else:\n",
    "                continue\n",
    "\n",
    "    print('number of noisy one hot encoded: {} out of {}'.format(count, len(df)))\n",
    "    print('percentage of noisy one hot encoded: %.3f' % (count / len(df) * 100))\n",
    "\n",
    "#calculates all error metrics needed\n",
    "def calculate_errors(actual, predicted, nb_columns):\n",
    "    n = len(actual)\n",
    "    r2score = r2_score(actual, predicted)\n",
    "    adjusted_r2 = 1 - ((1 - r2score) * (n - 1)) / (n - nb_columns - 1)\n",
    "    mase = mean_absolute_error(actual, predicted)\n",
    "    rms = sqrt(mean_squared_error(actual, predicted))\n",
    "    mse = mean_squared_error(actual, predicted)\n",
    "    re = (mse / np.mean(predicted)) * 100\n",
    "    pearson, pval = stats.pearsonr(np.array(actual).ravel(), np.array(predicted).ravel())\n",
    "    mae = np.mean(np.abs((actual - predicted) / actual)) * 100\n",
    "    spearman_corr, _ = spearmanr(actual, predicted)\n",
    "    distance_corr = distance.correlation(actual, predicted)\n",
    "    mape_score = np.asarray(np.abs(( np.array(actual) - np.array(predicted)) / np.array(actual)), dtype=np.float64).mean() * 100\n",
    "    return r2score, mase, rms, mse, re, pearson, pval, mae, adjusted_r2, spearman_corr, distance_corr, mape_score\n",
    "\n",
    "#get indices of folds in Stratified KFold CV\n",
    "def get_fold_indices(X,y,n_splits,rare_values):\n",
    "    rare_vec = [1 if i in rare_values else 0 for i in range(len(y))]\n",
    "    y = np.array(rare_vec)\n",
    "    splitter = StratifiedKFold(n_splits=n_splits, shuffle=False, random_state=123)\n",
    "    folds = list(splitter.split(X, y))\n",
    "    return folds\n",
    "    \n",
    "#get indices of folds in Stratified KFold CV\n",
    "def get_rare_idex(X,y,rare_values):\n",
    "    rare_vec = [1 if i in rare_values else 0 for i in range(len(y))]\n",
    "    y = np.array(rare_vec)\n",
    "    return y\n",
    "  \n",
    "#get grid of all hyper-parameters\n",
    "def get_param_grid(dicts):\n",
    "  return [dict(zip(dicts.keys(), p)) for p in it.product(*dicts.values())]\n",
    "\n",
    "def model_fit_predict_CV(X, y, split, parameter, model_name):\n",
    "\n",
    "    X_train, y_train = X.iloc[split[0],:], y.iloc[split[0], :]\n",
    "    X_valid, y_valid   = X.iloc[split[1],:], y.iloc[split[1], :]\n",
    "    #removed param\n",
    "    model = xgb.XGBRegressor(**parameter)\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_valid)\n",
    "\n",
    "    df_test = X_valid\n",
    "\n",
    "    # combine y_test and y_pred in 1 dataset\n",
    "    df_test[y_test_name] = y_valid\n",
    "    df_test[y_test_pred_name] = y_pred\n",
    "    \n",
    "    accuracy, aic, bic, nmi, mape_score,distance_corr,spearman_corr,pearson_corr,mae_score,mse_score,rmse_score,adjusted_r2,r2_Score,f1,f2,f5,prec,recall = evaluate(df_test, actual=y_test_name, predicted=y_test_pred_name,\n",
    "             thresh=0.8, rel_method='extremes', extr_type='high',\n",
    "             coef=1.5, relevance_pts=None)\n",
    "             \n",
    "    return accuracy, aic, bic, nmi, mape_score,distance_corr,spearman_corr,pearson_corr,mae_score,mse_score,rmse_score,adjusted_r2,r2_Score,f1,f2,f5,prec,recall\n",
    "\n",
    "#make input data pipeline for tf model\n",
    "def make_input_fn(X, y, n_epochs=None, shuffle=False):\n",
    "    NUM_EXAMPLES = math.floor(len(y) / 2)\n",
    "\n",
    "    def input_fn():\n",
    "    \n",
    "        dataset = tf.data.Dataset.from_tensor_slices((dict(X), y))\n",
    "        #print(dataset)\n",
    "        if shuffle:\n",
    "          dataset = dataset.shuffle(NUM_EXAMPLES)\n",
    "        #For training, cycle thru dataset as many times as need (n_epochs=None).\n",
    "        dataset = dataset.repeat(n_epochs)\n",
    "        # In memory training doesn't use batching.\n",
    "        dataset = dataset.batch(NUM_EXAMPLES,  drop_remainder=False)\n",
    "        #print(dataset)\n",
    "        return dataset\n",
    "\n",
    "    return input_fn\n",
    "\n",
    "#make test input data pipeline for tf model\n",
    "def make_input_fn_test(X, n_epochs=None, shuffle=False):\n",
    "    NUM_EXAMPLES = math.floor(len(X) / 2)\n",
    "    def input_fn():\n",
    "        dataset = tf.data.Dataset.from_tensor_slices(dict(X))\n",
    "        if shuffle:\n",
    "            dataset = dataset.shuffle(NUM_EXAMPLES)\n",
    "        # For training, cycle thru dataset as many times as need (n_epochs=None).\n",
    "        dataset = dataset.repeat(n_epochs)\n",
    "        # In memory training doesn't use batching.\n",
    "        dataset = dataset.batch(NUM_EXAMPLES,drop_remainder=True)\n",
    "        #print(dataset)\n",
    "        return dataset\n",
    "\n",
    "    return input_fn\n",
    "\n",
    "\n",
    "\n",
    "def rarify_data(df, df_train, df_test, target_variable, method, extr_type, thresh, coef, control_pts):\n",
    "\n",
    "    # get df_train and df_test\n",
    "    # param df_train: the training data frame\n",
    "    # param df_test: the testing data frame\n",
    "    # param target_variable: name of the target variable column\n",
    "    # return: df_train and df_test with equal class distribution between classes: rare and not rare\n",
    "\n",
    "    print(\"checking null values in dataset when applying rarify\")\n",
    "    print(df.isnull().values.any())\n",
    "\n",
    "    # get y, reset the index to avoid falsy retrievals by index later on\n",
    "    y = df[target_variable].reset_index(drop=True)\n",
    "    # get the indices of the rare values in the combined data frame\n",
    "    # note that the relevance returned is the relevance of the whole data frame not just the training\n",
    "    rare_values, phi_params, loss_params, yrel = get_rare(y, method, extr_type,thresh, coef, control_pts)\n",
    "\n",
    "    # dictionary mapping each value to its relevance\n",
    "    demandrel = {}\n",
    "    relvals = np.array(yrel)\n",
    "\n",
    "    for i, e in enumerate(y):\n",
    "        if e not in demandrel:\n",
    "            rel = relvals[i]\n",
    "            demandrel[e] = rel\n",
    "\n",
    "    # now we have the indices of the rare values, get their percentage\n",
    "\n",
    "    # percentage of rare values in the whole dataset\n",
    "    prare = len(rare_values)/len(df)\n",
    "    print('percentage of rare values in dataset before smogn: ' + str(prare*100) , file=open(output_path +\"rare_perc_results.txt\", \"a\"))\n",
    "    # number of rare values in the whole dataset\n",
    "    numrare = len(rare_values)\n",
    "    print('number of rare values in dataset before smogn: {}/{}'.format(numrare, len(df)), file=open(output_path +\"rare_perc_results.txt\", \"a\"))\n",
    "\n",
    "    # number of rare values that are be in each of the train and test\n",
    "    numraretrain = int(round(prare * len(df_train)))\n",
    "    numraretest = int(round(prare * len(df_test)))\n",
    "\n",
    "    print('number of rare in train: {}/{}'.format(numraretrain, len(df_train)), file=open(output_path +\"smogn_stats.txt\", \"a\"))\n",
    "    print('==> {}%%'.format((numraretrain/len(df_train))*100), file=open(output_path +\"smogn_stats.txt\", \"a\"))\n",
    "    print('number of rare in test: {}/{}'.format(numraretest, len(df_test)), file=open(output_path +\"smogn_stats.txt\", \"a\"))\n",
    "    print('==> {}%%'.format((numraretest / len(df_test))*100), file=open(output_path +\"smogn_stats.txt\", \"a\"))\n",
    "\n",
    "    rare_values = sorted(rare_values)\n",
    "\n",
    "    # rare indices partitioned for each of the train and test\n",
    "    rtrain = rare_values[:numraretrain]\n",
    "    rtest = rare_values[numraretrain:]\n",
    "\n",
    "    # get the relevance of each of the  dftrain and dftest\n",
    "    yreltrain = [demandrel[d] for d in df_train[target_variable]]\n",
    "    yreltest = [demandrel[d] for d in df_test[target_variable]]\n",
    "\n",
    "    if len(rtrain) != numraretrain:\n",
    "        raise ValueError('Incompatibility between the number of rare values that must be included in the '\n",
    "                         'training data for equal class distribution and the obtained number of rare')\n",
    "\n",
    "    if len(rtest) != numraretest:\n",
    "        raise ValueError('Incompatibility between the number of rare values that must be included in the '\n",
    "                         'testing data for equal class distribution and the obtained number of rare')\n",
    "\n",
    "    return rtrain, rtest, yreltrain, yreltest, phi_params['control.pts'], loss_params, demandrel\n",
    "\n",
    "#required error metrics\n",
    "def error_metrics(y_test, y_pred, ncols):\n",
    "    r2score, mase, rms, mse, re, pearson, pval, mae, adjusted_r2, spearman_corr, distance_corr, mape = calculate_errors(y_test, y_pred, ncols)\n",
    "    print(\"The range for the output variable is:\" + str(y_test.mean()))\n",
    "    print(\"r2score : \" + str(r2score))\n",
    "    print(\"adj r2score : \" + str(adjusted_r2))\n",
    "    print(\"mae : \" + str(mase))\n",
    "    print(\"rmse : \" + str(rms))\n",
    "    print(\"mse : \" + str(mse))\n",
    "    print(\"re : \" + str(re))\n",
    "    print(\"pearson : \" + str(pearson))\n",
    "    print(\"spearman : \" + str(spearman_corr))\n",
    "    print(\"distance : \" + str(distance_corr))\n",
    "    print(\"mape : \" + str(mape))\n",
    "\n",
    "#evaluate ub error metrics\n",
    "def evaluate(df, actual, predicted, thresh, rel_method='extremes', extr_type='high', coef=1.5, relevance_pts=None):\n",
    "    y = np.array(df[actual])\n",
    "    phi_params, loss_params, _ = get_phi_loss_params(y, rel_method, extr_type, coef, relevance_pts)\n",
    "\n",
    "    nb_columns = len(list(df.columns.values)) - 1\n",
    "\n",
    "    accuracy, aic, bic, nmi, mape_score,distance_corr,spearman_corr,pearson_corr,mae_score,mse_score,rmse_score,adjusted_r2,r2_Score,f1,f2,f5,prec,recall = get_stats(df[actual], df[predicted], nb_columns, thresh, phi_params, loss_params)\n",
    "    return accuracy, aic, bic, nmi, mape_score,distance_corr,spearman_corr,pearson_corr,mae_score,mse_score,rmse_score,adjusted_r2,r2_Score,f1,f2,f5,prec,recall\n",
    "\n",
    "\n",
    "def get_phi_loss_params(y, rel_method, extr_type='high', coef=1.5, relevance_pts=None):\n",
    "\n",
    "    # get the parameters of the relevance function\n",
    "    # param df: dataframe being used\n",
    "    # param target_variable: name of the target variable\n",
    "    # param rel_method: either 'extremes' or 'range'\n",
    "    # param extr_type: either 'high', 'low', or 'both' (defualt)\n",
    "    # param coef: default: 1.5\n",
    "    # param relevance_pts: the relevance matrix in case rel_method = 'range'\n",
    "    # return: phi parameters and loss parameters\n",
    "\n",
    "\n",
    "    if relevance_pts is None:\n",
    "        print('Will not use relevance matrix')\n",
    "        params = runit.get_relevance_params_extremes(y, rel_method=rel_method, extr_type=extr_type, coef=coef)\n",
    "    else:\n",
    "        print('Using supplied relevance matrix')\n",
    "        params = runit.get_relevance_params_range(y, rel_method=rel_method, extr_type=extr_type, coef=coef,\n",
    "                                                  relevance_pts=relevance_pts)\n",
    "\n",
    "    # phi params and loss params\n",
    "    phi_params = params[0]\n",
    "    loss_params = params[1]\n",
    "    relevance_values = params[2]\n",
    "\n",
    "    phi_params = dict(zip(phi_params.names, list(phi_params)))\n",
    "    loss_params = dict(zip(loss_params.names, list(loss_params)))\n",
    "\n",
    "    return phi_params, loss_params, relevance_values\n",
    "\n",
    "\n",
    "def get_stats(y_test, y_pred, nb_columns, thr_rel, phi_params, loss_params):\n",
    "\n",
    "    # Function to compute regression error metrics between actual and predicted values +\n",
    "    # correlation between both using different methods: Pearson, Spearman, and Distance\n",
    "    # param y_test: the actual values. Example df['actual'] (the string inside is the name\n",
    "    # of the actual column. Example: df['LE (mm)'], df['demand'], etc.)\n",
    "    # param y_pred: the predicted vlaues. Example df['predicted']\n",
    "    # param nb_columns: number of columns <<discarding the target variable column>>\n",
    "    # return: R2, Adj-R2, RMSE, MSE, MAE, MAPE\n",
    "\n",
    "    def mean_absolute_percentage_error(y_true, y_pred):\n",
    "        y_true, y_pred = np.array(y_true), np.array(y_pred)\n",
    "        return np.mean(np.abs((y_true - y_pred) / y_true)) * 100\n",
    "\n",
    "    if not isinstance(y_test, list):\n",
    "        y_test = list(y_test)\n",
    "    if not isinstance(y_pred, list):\n",
    "        y_pred = list(y_pred)\n",
    "\n",
    "    n = len(y_test)\n",
    "\n",
    "    r2_Score = r2_score(y_test, y_pred)  # r-squared\n",
    "    adjusted_r2 = 1 - ((1 - r2_Score) * (n - 1)) / (n - nb_columns - 1)  # adjusted r-squared\n",
    "    rmse_score = np.sqrt(mean_squared_error(y_test, y_pred))  # RMSE\n",
    "    mse_score = mean_squared_error(y_test, y_pred)  # MSE\n",
    "    mae_score = mean_absolute_error(y_test, y_pred)  # MAE\n",
    "    #print(np.asarray(np.abs(( np.array(y_test) - np.array(y_pred)) / np.array(y_test)), dtype=np.float64))\n",
    "    mape_score = np.asarray(np.abs(( np.array(y_test) - np.array(y_pred)) / np.array(y_test)), dtype=np.float64).mean() * 100  # MAPE\n",
    "    accuracy = 100 - mape_score\n",
    "    aic = len(y_test) * np.log(mse_score)\n",
    "    bic = len(y_test) * np.log(mse_score)\n",
    "    nmi = normalized_mutual_info_score(y_test, y_pred)\n",
    "\n",
    "    trues = np.array(y_test)\n",
    "    preds = np.array(y_pred)\n",
    "\n",
    "    method = phi_params['method']\n",
    "    npts = phi_params['npts']\n",
    "    controlpts = phi_params['control.pts']\n",
    "    ymin = loss_params['ymin']\n",
    "    ymax = loss_params['ymax']\n",
    "    tloss = loss_params['tloss']\n",
    "    epsilon = loss_params['epsilon']\n",
    "\n",
    "    rmetrics = runit.eval_stats(trues, preds, thr_rel, method, npts, controlpts, ymin, ymax, tloss, epsilon)\n",
    "\n",
    "    # create a dictionary of the r metrics extracted above\n",
    "    rmetrics_dict = dict(zip(rmetrics.names, list(rmetrics)))\n",
    "\n",
    "    if isinstance(y_pred[0], np.ndarray):\n",
    "        y_pred_new = [x[0] for x in y_pred]\n",
    "        y_pred = y_pred_new\n",
    "    \n",
    "    pearson_corr, _ = pearsonr(y_test, y_pred)\n",
    "    spearman_corr, _ = spearmanr(y_test, y_pred)\n",
    "    distance_corr = distance.correlation(y_test, y_pred)\n",
    "\n",
    "    print('\\nUtility Based Metrics')\n",
    "    print('F1: %.5f' % rmetrics_dict['ubaF1'][0])\n",
    "    print('F2: %.5f' % rmetrics_dict['ubaF2'][0])\n",
    "    print('F05: %.5f' % rmetrics_dict['ubaF05'][0])\n",
    "    print('precision: %.5f' % rmetrics_dict['ubaprec'][0])\n",
    "    print('recall: %.5f' % rmetrics_dict['ubarec'][0])\n",
    "\n",
    "    print('\\nRegression Error Metrics')\n",
    "    print('R2: %.5f' % r2_Score)\n",
    "    print('Adj-R2: %.5f' % adjusted_r2)\n",
    "    print('RMSE: %.5f' % rmse_score)\n",
    "    print('MSE: %.5f' % mse_score)\n",
    "    print('MAE: %.5f' % mae_score)\n",
    "    print('MAPE: %.5f' % mape_score)\n",
    "    print('Accuracy: %.5f' % accuracy)\n",
    "    print('aic: %.5f' % aic)\n",
    "    print('bic: %.5f' % bic)\n",
    "    print('nmi: %.5f' % nmi)\n",
    "\n",
    "    print('\\nCorrelations')\n",
    "    print('Pearson: %.5f' % pearson_corr)\n",
    "    print('Spearman: %.5f' % spearman_corr)\n",
    "    print('Distance: %.5f' % distance_corr)\n",
    "    \n",
    "    print('TO EXCEL')\n",
    "    print(np.average(y_test))\n",
    "    print(rmetrics_dict['ubaF1'][0])\n",
    "    print(rmetrics_dict['ubaF2'][0])\n",
    "    print(rmetrics_dict['ubaF05'][0])\n",
    "    print(rmetrics_dict['ubaprec'][0])\n",
    "    print(rmetrics_dict['ubarec'][0])\n",
    "    print(r2_Score)\n",
    "    print(adjusted_r2)\n",
    "    print(rmse_score)\n",
    "    print(mse_score)\n",
    "    print(mae_score)\n",
    "    print(mape_score)\n",
    "    print(accuracy)\n",
    "    print(aic)\n",
    "    print(bic)\n",
    "    print(nmi)\n",
    "\n",
    "    print(pearson_corr)\n",
    "    print(spearman_corr)\n",
    "    print(distance_corr)\n",
    "    return accuracy, aic, bic, nmi, mape_score,distance_corr,spearman_corr,pearson_corr,mae_score,mse_score,rmse_score,adjusted_r2,r2_Score,rmetrics_dict['ubaF1'][0],rmetrics_dict['ubaF2'][0],rmetrics_dict['ubaF05'][0],rmetrics_dict['ubaprec'][0],rmetrics_dict['ubarec'][0]\n",
    "    \n",
    "    \n",
    "def calculate_avg_error_metrics(mape_folds,  acc_folds, aic_folds, bic_folds, nmi_folds, d_f,sp_f, p_f, mae_f,mse_f, rmse_f, ar2_f, r2_f, f1_f, f2_f, f5_f,prec_f,recall_f,folds):\n",
    "  print('\\nUtility Based Metrics Across All', file=open(output_path +\"output_CV_results.txt\", \"a\"))\n",
    "  avg_f1 = f1_f / folds\n",
    "  print('F1: ' , avg_f1, file=open(output_path +\"output_CV_results.txt\", \"a\"))\n",
    "  avg_f2 =  f2_f / folds\n",
    "  print('F2: ' , avg_f2, file=open(output_path +\"output_CV_results.txt\", \"a\") )\n",
    "  avg_f5 = f5_f / folds\n",
    "  print('F05:' ,  avg_f5, file=open(output_path +\"output_CV_results.txt\", \"a\"))\n",
    "  avg_prec = prec_f / folds\n",
    "  print('precision: ', avg_prec  , file=open(output_path +\"output_CV_results.txt\", \"a\"))\n",
    "  avg_recall = recall_f / folds\n",
    "  print('recall:' , avg_recall , file=open(output_path +\"output_CV_results.txt\", \"a\"))\n",
    "  \n",
    "  print('\\nRegression Error Metrics Across All', file=open(output_path +\"output_CV_results.txt\", \"a\"))\n",
    "  avg_r2 = r2_f/folds\n",
    "  print('R2:' , avg_r2, file=open(output_path +\"output_CV_results.txt\", \"a\"))\n",
    "  avg_ar2 =  ar2_f/folds\n",
    "  print('Adj-R2:' , avg_ar2, file=open(output_path +\"output_CV_results.txt\", \"a\"))\n",
    "  avg_rmse = rmse_f/folds\n",
    "  print('RMSE:' , avg_rmse , file=open(output_path +\"output_CV_results.txt\", \"a\"))\n",
    "  avg_mse = mse_f/folds\n",
    "  print('MSE:' , avg_mse , file=open(output_path +\"output_CV_results.txt\", \"a\"))\n",
    "  avg_mae =  mae_f/folds\n",
    "  print('MAE:' , avg_mae, file=open(output_path +\"output_CV_results.txt\", \"a\"))\n",
    "  avg_mape = mape_folds/folds\n",
    "  print('MAPE:' , avg_mape , file=open(output_path +\"output_CV_results.txt\", \"a\"))\n",
    "  avg_acc = acc_folds/folds\n",
    "  print('Accuracy:' , avg_acc , file=open(output_path +\"output_CV_results.txt\", \"a\"))\n",
    "  avg_aic = aic_folds/folds\n",
    "  print('AIC:' , avg_aic , file=open(output_path +\"output_CV_results.txt\", \"a\"))\n",
    "  avg_bic = bic_folds/folds\n",
    "  print('BIC:' , avg_bic , file=open(output_path +\"output_CV_results.txt\", \"a\"))\n",
    "  avg_nmi = nmi_folds/folds\n",
    "  print('NMI:' , avg_nmi , file=open(output_path +\"output_CV_results.txt\", \"a\"))\n",
    "\n",
    "\n",
    "  print('\\nCorrelations Across All', file=open(output_path +\"output_CV_results.txt\", \"a\"))\n",
    "  avg_pearson =  p_f/folds\n",
    "  print('Pearson:' , avg_pearson, file=open(output_path +\"output_CV_results.txt\", \"a\"))\n",
    "  avg_spearman = sp_f/folds\n",
    "  print('Spearman:' , avg_spearman , file=open(output_path +\"output_CV_results.txt\", \"a\"))\n",
    "  avg_dist =  d_f/folds\n",
    "  print('Distance:' , avg_dist, file=open(output_path +\"output_CV_results.txt\", \"a\"))\n",
    "  return avg_f1, avg_f2, avg_f5, avg_prec, avg_recall, avg_r2, avg_ar2, avg_rmse, avg_mse, avg_mae, avg_mape, avg_acc, avg_aic, avg_bic, avg_nmi, avg_pearson, avg_spearman, avg_dist\n",
    "  \n",
    "def get_relevance_oversampling(smogned, target_variable, targetrel):\n",
    "\n",
    "    # gets the relevance values of an oversampled data frame\n",
    "    # param smogned: the oversampled data frame\n",
    "    # param target_variable: name of the target variable column\n",
    "    # param targetrel: dictionary mapping each target variable value to a relevance value\n",
    "    # return: the relevance of the oversampled data frame\n",
    "\n",
    "    yrelafter = []\n",
    "    distances = []\n",
    "    for val in smogned[target_variable]:\n",
    "        if val in targetrel:\n",
    "            yrelafter.append(targetrel[val])\n",
    "        else:\n",
    "            nearest = min(sorted(list(targetrel.keys())), key=lambda x: abs(x - val))\n",
    "            distances.append(abs(nearest - val))\n",
    "            yrelafter.append(targetrel[nearest])\n",
    "\n",
    "    return yrelafter, distances\n",
    "  \n",
    "def get_formula(target_variable):\n",
    "\n",
    "    # gets the formula for passing it to R functions. Example: target_variable ~ col1 + col2 ...\n",
    "    # param target_variable: the name of the target variable\n",
    "    # return: R's formula as follows: target_variable ~ other[0] + other[1] + other[2] + other[3] + ...\n",
    "\n",
    "    formula = runit.create_formula(target_variable)\n",
    "    return formula\n",
    "    \n",
    "def apply_smogn(df_train, smogn, target_variable, phi_params, thr_rel, Cperc, k, repl, dist, p, pert, plotdensity=False ):\n",
    "\n",
    "    #method that applies SMOGN Algorithm to the current data frame\n",
    "    # print('getting back values from oversampled R data frame')\n",
    "    # print('before smogn')\n",
    "    #print(pandas2ri.py2ri(df_train).head(), \"this is py2ri\")\n",
    "    if smogn:\n",
    "        smogned = runit.WFDIBS(\n",
    "            fmla=get_formula(target_variable),\n",
    "            dat= pandas2ri.py2ri(df_train),\n",
    "            #dat=df_train,\n",
    "            method=phi_params['method'][0],\n",
    "            npts=phi_params['npts'][0],\n",
    "            controlpts=phi_params['control.pts'],\n",
    "            thrrel=thr_rel,\n",
    "            Cperc=Cperc,\n",
    "            k=k,\n",
    "            repl=repl,\n",
    "            dist=dist,\n",
    "            p=p, \n",
    "            pert=pert)\n",
    "\n",
    "        # print('after smogn')\n",
    "        # print('before pandas2ri')\n",
    "         #convert the oversampled R Data.Frame back to a pandas data frame\n",
    "        smogned = pandas2ri.ri2py_dataframe(smogned)\n",
    "        # print('after pandas2ri')\n",
    "\n",
    "        if plotdensity:\n",
    "            # density plot after smooting\n",
    "            plot_density(smogned,target_variable,output_folder + 'plots/', 'density_after_smogn', 'Density Plot')\n",
    "\n",
    "        X_train = np.array(smogned.loc[:, smogned.columns != target_variable])\n",
    "        y_train = np.array(smogned.loc[:, target_variable])\n",
    "\n",
    "        return X_train, y_train\n",
    "  \n",
    "def write_to_txt(filename, content):\n",
    "  text_file = open(output_path + filename, \"w\")\n",
    "  text_file.write(content)\n",
    "  text_file.close()\n",
    "  \n",
    "def plot_actual_vs_predicted(df, predicted_variable):\n",
    "  plt.plot(list(range(1, len(df) + 1)), df[y_test_name], color='b', label='actual')\n",
    "  plt.plot(list(range(1, len(df) + 1)), df[predicted_variable], color='r', label='predicted')\n",
    "  plt.legend(loc='best')\n",
    "  plt.suptitle('actual vs. predicted')\n",
    "  plt.savefig(output_path + 'actual_vs_predicted')\n",
    "  plt.close()\n",
    "    \n",
    "def plot_actual_vs_predicted_scatter_bisector(df, predicted_variable):\n",
    "  fig, ax = plt.subplots()\n",
    "  ax.scatter(df[y_test_name], df[predicted_variable], c='black')\n",
    "  lims = [\n",
    "      np.min([ax.get_xlim(), ax.get_ylim()]),  # min of both axes\n",
    "      np.max([ax.get_xlim(), ax.get_ylim()]),  # max of both axes\n",
    "  ]\n",
    "  ax.plot(lims, lims, 'k-', alpha=0.75, zorder=0)\n",
    "  ax.set_aspect('equal')\n",
    "  ax.set_xlim(lims)\n",
    "  ax.set_ylim(lims)\n",
    "  plt.suptitle('actual vs. predicted forecasts')\n",
    "  plt.savefig(output_path + 'actual_vs_predicted_scatter_plot')\n",
    "  plt.close()\n",
    "\n",
    "\n",
    "def plot_relevance(y, yrel, target_variable, output_folder, fig_name):\n",
    "    reldict = {}\n",
    "    y = y[target_variable]\n",
    "    for i, e in enumerate(y):\n",
    "        if e not in reldict:\n",
    "            reldict[e] = yrel[i]\n",
    "\n",
    "    reldict = dict(collections.OrderedDict(sorted(reldict.items())))\n",
    "    plt.plot(list(reldict.keys()), list(reldict.values()))\n",
    "    plt.xlabel(target_variable)\n",
    "    plt.ylabel('relevance')\n",
    "\n",
    "    plt.savefig(output_folder + fig_name)\n",
    "    plt.close()\n",
    "\n",
    "def plot_target_variable(df, df_resampled, output_column, output_folder, fig_name):\n",
    "    y = df[output_column]\n",
    "    y_resamp = df_resampled[output_column]\n",
    "    plt.plot(list(range(len(y))), sorted(y), label = \"original\")\n",
    "    plt.plot(list(range(len(y_resamp))), sorted(y_resamp), label = \"resampled\")\n",
    "    plt.xlabel('Index')\n",
    "    plt.ylabel(target_variable)\n",
    "    plt.legend()\n",
    "    plt.savefig(output_folder + fig_name)\n",
    "    plt.close()\n",
    "  \n",
    "def get_relevance():\n",
    "    ctrl = phi_params['control.pts']\n",
    "    if rel_method[0] == 'extremes' and relevance_pts[0] is None:\n",
    "        rell = np.array([\n",
    "            [ctrl[0], ctrl[1], ctrl[2]],\n",
    "            [ctrl[3], ctrl[4], ctrl[5]],\n",
    "            [ctrl[6], ctrl[7], ctrl[8]]\n",
    "        ])\n",
    "    else:\n",
    "        rell = relevance_pts[0]\n",
    "\n",
    "    return rell\n",
    "\n",
    "## Generate lags for all input features, re-generate even if some exist so that order will not be shuffled after nan dropping\n",
    "def generate_lags_for(df, column, lags_count):\n",
    "        for i in range(lags_count):\n",
    "            lag_name = column + \"-\" + str(i + 1)\n",
    "            df[lag_name] = df[column].shift(i + 1)\n",
    "        return df\n",
    "\n",
    "def generate_lags(df, lagsForColumns):\n",
    "    '''This function generates the lags for the list of columns'''\n",
    "    for k in range(len(lagsForColumns)):\n",
    "        col = lagsForColumns[k]\n",
    "        if col in df.columns:\n",
    "            df = generate_lags_for(df, col, 5)\n",
    "    return df\n",
    "\n",
    "\n",
    "def split_train_test_valid(df, TRAIN_RATIO, TEST_RATIO):\n",
    "    X_train = pd.DataFrame()\n",
    "    X_test = pd.DataFrame()\n",
    "    Y_train = pd.DataFrame()\n",
    "    Y_test = pd.DataFrame()\n",
    "    unique_sites = df[\"Site\"].unique()\n",
    "    print(\"Number of sites:\", len(unique_sites))\n",
    "\n",
    "    for site in unique_sites:\n",
    "        df_site = df[df[\"Site\"] == site]\n",
    "        X = df_site\n",
    "        train_index = int(X.shape[0] * TRAIN_RATIO)\n",
    "        test_index = int(X.shape[0] * (TRAIN_RATIO + TEST_RATIO))\n",
    "\n",
    "        X_train = X_train.append(X[:train_index], ignore_index = True)\n",
    "        X_test = X_test.append(X[train_index:], ignore_index = True)\n",
    "        Y_train = Y_train.append(X[:train_index], ignore_index = True)\n",
    "        Y_test = Y_test.append(X[train_index:], ignore_index = True)\n",
    "\n",
    "    Y_train = Y_train[[output_column]]\n",
    "    Y_test = Y_test[[output_column]]\n",
    "   \n",
    "    X_train = X_train.drop([output_column], axis = 1)\n",
    "    X_test = X_test.drop([output_column], axis = 1)\n",
    "   \n",
    "    return X_train, X_test, Y_train, Y_test\n",
    "\n",
    "def binary_encode_column(df, columnToEncode):\n",
    "    encoder = ce.BinaryEncoder(cols=[columnToEncode])\n",
    "    df_encoder = encoder.fit_transform(df[columnToEncode])\n",
    "    df = pd.concat([df, df_encoder], axis=1)\n",
    "    return df\n",
    "\n",
    "def export_scores(scores, columnName):\n",
    "    file_name = output_path + \"score_sheet_final.csv\"\n",
    "    if not os.path.exists(file_name):\n",
    "        print(\"path does not exist\")\n",
    "        df = pd.DataFrame(list())\n",
    "        df.to_csv(file_name, index=False)\n",
    "    else:\n",
    "        print(\"path exists\")\n",
    "        df = pd.read_csv(file_name, delimiter=',')\n",
    "\n",
    "    df[\"Error Metrics\"] = scores.keys()\n",
    "    df[columnName] = scores.values()\n",
    "    df.to_csv(file_name, index=False)\n",
    "    return df\n",
    "\n",
    "\n",
    "########################################################################################################################\n",
    "                                            #Establish a connection to R library\n",
    "########################################################################################################################\n",
    "rpy2.robjects.numpy2ri.activate()\n",
    "runit = robjects.r\n",
    "runit['source']('smogn.R')\n",
    "\n",
    "\n",
    "########################################################################################################################\n",
    "                                            #Read and Preprocess Dataset\n",
    "########################################################################################################################\n",
    "\n",
    "#smogn relate hyper-params\n",
    "target_variable = \"LE_bowen_corr_mm\"\n",
    "rel_method='range'\n",
    "extr_type='high'\n",
    "coef=1.5\n",
    "\n",
    "rell = np.array([\n",
    "    [1, 0 , 0],\n",
    "    [4, 0 , 0],\n",
    "    [15, 1 , 0],\n",
    "])\n",
    "#rell = None\n",
    "relevance_pts=rell\n",
    "rel=\"auto\"\n",
    "thr_rel=0.1\n",
    "Cperc=np.array([1,1.2])\n",
    "k=5\n",
    "repl=False\n",
    "dist=\"Manhattan\"\n",
    "p=2\n",
    "pert=0.1\n",
    "input_path = \"/apps/data/output_eeflux.csv\"\n",
    "\n",
    "#read csv\n",
    "df_test = pd.read_csv(input_path, delimiter=',')\n",
    "\n",
    "df_test.loc[(df_test['Eeflux_ET'] < 0.3 ), 'Eeflux_ET'] = 0.3\n",
    "df_test.loc[(df_test['Eeflux_ET'] > 15), 'Eeflux_ET'] = 15\n",
    "\n",
    "df_test['resid1'] = df_test['LE_bowen_corr_mm']\n",
    "df_test['resid2'] =  df_test['Eeflux_ET']\n",
    "\n",
    "df_test['resid3'] = df_test['Eeflux_ET'] / df_test['LE_bowen_corr_mm']\n",
    "df_test['resid4'] =  df_test['Eeflux_ET'] / df_test['LE_bowen_corr_pred']\n",
    "\n",
    "df_test['resid5'] = df_test['Eeflux_ET'] - df_test['LE_bowen_corr_mm']\n",
    "df_test['resid6'] = df_test['Eeflux_ET'] - df_test['LE_bowen_corr_pred']\n",
    "\n",
    "accuracy, aic, bic, nmi, mape_score,distance_corr,spearman_corr,pearson_corr,mae_score,mse_score,rmse_score,adjusted_r2,r2_Score,f1,f2,f5,prec,recall = evaluate(df_test, actual='resid1', predicted='resid2',\n",
    "        thresh=0.1, rel_method=rel_method, extr_type='high',\n",
    "        coef=1.5, relevance_pts=rell)\n",
    "\n",
    "\n",
    "accuracy, aic, bic, nmi, mape_score,distance_corr,spearman_corr,pearson_corr,mae_score,mse_score,rmse_score,adjusted_r2,r2_Score,f1,f2,f5,prec,recall = evaluate(df_test, actual='resid3', predicted='resid4',\n",
    "        thresh=0.1, rel_method=rel_method, extr_type='high',\n",
    "        coef=1.5, relevance_pts=rell)\n",
    "\n",
    "accuracy, aic, bic, nmi, mape_score,distance_corr,spearman_corr,pearson_corr,mae_score,mse_score,rmse_score,adjusted_r2,r2_Score,f1,f2,f5,prec,recall = evaluate(df_test, actual='resid5', predicted='resid6',\n",
    "        thresh=0.1, rel_method=rel_method, extr_type='high',\n",
    "        coef=1.5, relevance_pts=rell)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
